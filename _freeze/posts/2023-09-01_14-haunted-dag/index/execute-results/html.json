{
  "hash": "c039c6e2b8ee2091c247a4447ece63c9",
  "result": {
    "markdown": "---\ntitle: \"Haunted DAGSs\"\norder: 15\ndate: 2023-09-01\ntwitter-card: true\nopen-graph: true\nbibliography: references.bib\n---\n\n\n::: callout-note\n## Listening\n\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/2IfmHgY7WKdyRIdiKQVBep?utm_source=generator\" width=\"100%\" height=\"152\" frameBorder=\"0\" allowfullscreen allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\">\n\n</iframe>\n:::\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(ggforce)\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(dagitty)\nlibrary(ggdag)\n\nsource(here::here(\"_defaults.r\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023-9-1)\n```\n:::\n\n\n## Newsworthiness & Trustworthiness\n\nThe first example in the book is about [Berkson's Paradox](https://en.wikipedia.org/wiki/Berkson%27s_paradox), which I believe is a kind of selection bias. The question is \"Why do so many research results that are newsworthy seem unreliable?\" The idea being that funding (or whatever, maybe \"editorial decisions to publish a paper\") is based jointly on its trustworthiness and its newsworthiness.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  trustworthiness = rnorm(200),\n  newsworthiness = rnorm(200),\n  score = trustworthiness + newsworthiness,\n  score_percentile = ecdf(score)(score)\n) ->\n  research\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nresearch |> \n  filter(\n    score_percentile >= 0.9\n  )->\n  selected\n\nresearch |> \n  ggplot(\n    aes(\n      newsworthiness,\n      trustworthiness\n    )\n  )+\n    geom_point()+\n    geom_mark_hull(\n      aes(\n        filter = score_percentile >= 0.9,\n        color = \"selected\"\n      ),\n      fill = \"grey\"\n    )+\n    stat_smooth(\n      aes(\n        color = \"all\"\n      ),\n      method = lm\n    )+\n    stat_smooth(\n      data = selected,\n      aes(\n        color = \"selected\"\n      ),\n      method = lm\n    )+\n    coord_fixed()+\n    theme(\n      aspect.ratio = 1\n    )+\n    labs(\n      color = NULL\n    )\n```\n\n::: {.cell-output-display}\n![The selection effect on the (non)correation between newsworthiness and trustworthiness](index_files/figure-html/fig-selection-1.png){#fig-selection width=80%}\n:::\n:::\n\n\n## Multicollinearity\n\n> In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction. You will just be frustrated trying to understand it.\n\nWhen I was starting to get into advanced statistical modelling during my PhD (some time around 2010?) everyone suddenly learned about multicollinearity and got freaked out about it, so this was genuinely new info to me. See also @vanhove2021, *Collinearity isn't a disease that needs curing*.\n\nThe illustration from the book was about the relationship between total body height and leg length.\n\n\n::: {.cell filename='leg-heigh simulation'}\n\n```{.r .cell-code}\ntibble(\n  n = 100,\n  # height in inches\n  height = rnorm(n, mean = 70, sd = 3),\n  # legs as a proportion of height\n  leg_prop = runif(n, 0.4, 0.5),\n  left_leg = height * leg_prop,\n  right_leg = left_leg + \n    rnorm(n, sd = 0.02)\n)->\n  height_legs\n```\n:::\n\n\nThe simulated right leg length is going to be, max, ±0.06 (^1^⁄~16~^th^ inch) the left leg.\n\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nheight_legs |> \n  ggplot(aes(left_leg, height))+\n    geom_point()+\n    theme(aspect.ratio = 1)->\n  lh\n\nheight_legs |> \n  ggplot(aes(right_leg, height))+\n    geom_point()+\n    theme(aspect.ratio = 1)->\n  rh\n\nheight_legs |> \n  ggplot(aes(left_leg, right_leg))+\n    geom_point()+\n    theme(aspect.ratio = 1)->\n  lr\n\nlh + rh +lr\n```\n\n::: {.cell-output-display}\n![data relationships](index_files/figure-html/fig-leg-height-1.png){#fig-leg-height width=960}\n:::\n:::\n\n\nA model with only left or right leg is going to be fine.\n\n\n::: {.cell filename='left leg model'}\n\n```{.r .cell-code}\nbrm(\n  height ~ left_leg,\n  data = height_legs,\n  prior = c(\n    prior(normal(70, 100), class = Intercept),\n    prior(normal(0, 10), class = b)\n  ),\n  backend = \"cmdstanr\",\n  file = \"height_left\"\n)->\n  height_left_mod\n```\n:::\n\n\nThe right leg model:\n\n\n::: {.cell filename='right leg model'}\n\n```{.r .cell-code}\nbrm(\n  height ~ right_leg,\n  data = height_legs,\n  prior = c(\n    prior(normal(70, 100), class = Intercept),\n    prior(normal(0, 10), class = b)\n  ),\n  backend = \"cmdstanr\",\n  file = \"height_right\"\n)->\n  height_right_mod\n```\n:::\n\n\nI'm going to get a little fancy to get the parameter estimates from both models all at once.\n\n\n::: {.cell filename='getting parameter estimates'}\n\n```{.r .cell-code}\nlist(\n  left = height_left_mod,\n  right = height_right_mod\n) |> \n  map(\n    ~ gather_draws(\n      .x, \n      `.*_leg`,\n      regex = T\n    )\n  ) |> \n  list_rbind(\n    names_to = \"model\"\n  )->\n  rl_params\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nrl_params |> \n  ggplot(\n    aes(\n      .value, \n      .variable,\n    )\n  )+\n    stat_halfeye(\n      aes(\n        fill = model\n      )\n    )+\n  expand_limits(\n    x = 0\n  )+\n  ylim(\n    \"b_right_leg\",\n    \"b_left_leg\"\n  )\n```\n\n::: {.cell-output-display}\n![Estimated leg parameter](index_files/figure-html/fig-rl-mods-1.png){#fig-rl-mods width=576}\n:::\n:::\n\n\nBut if we include *both* left and right leg, the estimate of each one's parameter gets weird.\n\n\n::: {.cell filename='both leg model'}\n\n```{.r .cell-code}\nbrm(\n  height ~ left_leg + right_leg,\n  data = height_legs,\n  prior = c(\n    prior(normal(70, 100), class = Intercept),\n    prior(normal(0, 10), class = b)\n  ),\n  backend = \"cmdstanr\",\n  file = \"height_both\"\n)->\n  height_both_mod\n```\n:::\n\n::: {.cell filename='getting parameter estimates'}\n\n```{.r .cell-code}\nlist(\n  left = height_left_mod,\n  right = height_right_mod,\n  both = height_both_mod\n) |> \n  map(\n    ~ gather_draws(\n      .x, \n      `.*_leg`,\n      regex = T\n    )\n  ) |> \n  list_rbind(\n    names_to = \"model\"\n  )->\n  all_params\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nall_params |> \n  mutate(\n    model = model |>  \n      fct_relevel(\"both\", after = Inf)\n  ) |> \n  ggplot(\n    aes(\n      .value, \n      .variable,\n    )\n  )+\n    stat_pointinterval(\n      aes(\n        color = model\n      ),\n      position = \"dodge\"\n    )+\n  expand_limits(\n    x = 0\n  )+\n  ylim(\n    \"b_right_leg\",\n    \"b_left_leg\"\n  )\n```\n\n::: {.cell-output-display}\n![Oops, multicollinear!](index_files/figure-html/fig-both-mod-1.png){#fig-both-mod width=576}\n:::\n:::\n\n\nThis was the kind of outcome that I was taught to do things like residualize, but McElreath says that while the model we tried to write out was:\n\n$$\n\\text{height} = \\text{Intercept} + \\beta_1\\text{left} + \\beta_2\\text{right}\n$$\n\nbecause the right leg and the left leg are basically the same, we have something more like\n\n$$\n\\text{height} = \\text{Intercept} + (\\beta_1 + \\beta_2)\\text{leg}\n$$\n\nAnd because there's nothing else in the model to *specify* what the value of $\\beta_1$ and $\\beta_2$ are, they're all over the place. But crucially, they ought to add up to a similar value to what we got for just `left_leg` and `right_leg` in the first two models. They *also* should be negatively correlated, so when one is large and positive, the other should be large and negative, so they cancel out to around the values we got before.\n\n\n::: {.cell filename='the multicollinear estimates'}\n\n```{.r .cell-code}\nheight_both_mod |> \n  spread_draws(\n    `.*_leg`,\n    regex = T\n  )->\n  leg_ests\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nleg_ests |> \n  ggplot(\n    aes(\n      b_left_leg,\n      b_right_leg\n    )\n  )+\n    geom_point(\n      alpha = 0.1\n    )+\n    coord_fixed()+\n    theme(\n      aspect.ratio = 1\n    )\n```\n\n::: {.cell-output-display}\n![Correlated leg estimates](index_files/figure-html/fig-est-cor-1.png){#fig-est-cor width=60%}\n:::\n:::\n\n\nWe can add each parameter together and compare it to the original two models.\n\n\n::: {.cell filename='adding together multicollinear estimates'}\n\n```{.r .cell-code}\nleg_ests |> \n  mutate(\n    .variable = \"b_leg\",\n    .value = b_left_leg + b_right_leg,\n    model = \"both\"\n  ) |> \n  bind_rows(\n    rl_params\n  ) |> \n  mutate(\n    model = fct_relevel(\n      model,\n      \"both\",\n      after = Inf\n    )\n  )->\n  rl_comp\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nrl_comp |> \n  ggplot(\n    aes(\n      .value,\n      .variable\n    )\n  )+\n    stat_halfeye(\n      aes(\n        fill = model\n      )\n    ) +\n  expand_limits(\n    x = 0\n  ) +\n  ylim(\n    \"b_leg\",\n    \"b_right_leg\",\n    \"b_left_leg\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=576}\n:::\n:::\n\n\n### \"The model will work fine for prediction.\"\n\nJust to hammer home the point that the *predictive* value of the multicollinear model, we can compare its posterior predictive checks to the left and right leg models.\n\n\n::: {.cell filename='posterior predictive checks' crop='true'}\n\n```{.r .cell-code}\npp_check(height_left_mod)+\n  labs(title = \"model: left\")->\n  left_pp\n\npp_check(height_right_mod)+\n  labs(title = \"model: right\") ->\n  right_pp\n\npp_check(height_both_mod)+\n  labs(title = \"model: both\") ->\n  both_pp\n\nleft_pp + right_pp + both_pp + plot_layout(guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![Posterior predictive checks](index_files/figure-html/fig-ppcheck-1.png){#fig-ppcheck width=960}\n:::\n:::\n\n\nWe can also compare their predictions for new leg length.\n\n\n::: {.cell filename='getting predicted values'}\n\n```{.r .cell-code}\ntibble(\n  left_leg = 32,\n  right_leg = left_leg+0.01\n)->\n  newleg\n\nlist(\n  left = height_left_mod,\n  right = height_right_mod,\n  both = height_both_mod\n) |> \n  map(\n    ~ predictions(\n      .x, \n      newdata = newleg\n    ) |> \n      posterior_draws()\n  ) |> \n  list_rbind(\n    names_to = \"model\"\n  )->\n  all_pred\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nall_pred |> \n  ggplot(\n    aes(\n      draw,\n      model\n    )\n  )+\n    labs(\n      x = \"predicted height\"\n    )+\n    stat_halfeye()\n```\n\n::: {.cell-output-display}\n![Predicted heights](index_files/figure-html/fig-predictions-1.png){#fig-predictions width=80%}\n:::\n:::\n\n\n### So what to do?\n\nThe upshot of McElreath's recommendation for what to do about all this multicollinearity is \"have a bad time.\" There's no generic answer. Maybe there's an acceptable way to specify the model depending on the DAG, but also maybe some questions aren't well put, like \"what are the individual contribution of the left leg and the right leg to total height?\"\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}