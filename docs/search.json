[
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html",
    "title": "Predictive Distributions",
    "section": "",
    "text": "Listening\nOne last post to work through different predictive distributions given 6W and 3W."
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#loading",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#loading",
    "title": "Predictive Distributions",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(here)\nlibrary(ggblend)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#prior-predictive",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#prior-predictive",
    "title": "Predictive Distributions",
    "section": "Prior predictive",
    "text": "Prior predictive\nThe prior was a Uniform distribution between 0 and 1, or \\(\\mathcal{U}(0,1)\\). The outcome will also be uniform, but I‚Äôll go through all the steps for completeness.\n\ntibble(\n  p = runif(1e4)\n) -&gt;\n  prior_samp\n\nI‚Äôll make use of vectorization of rbinom() to generate possible samples with probabilities in prior_samp$p.\n\nset.seed(2023)\nprior_samp |&gt; \n  mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n1  count(pred_binom) |&gt;\n  mutate(prob = n/sum(n)) -&gt;\n  prior_pred\n\n\n1\n\nI‚Äôm going straight from generated samples to summarising for the distribution.\n\n\n\n\n\nprior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n1    theme_no_y()\n\n\n1\n\nI‚Äôve moved theme_no_y() into _defaults.R which I source at the top."
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#maximum-likelihood",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#maximum-likelihood",
    "title": "Predictive Distributions",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe maximum likelihood probability of W is \\(\\frac{6}{9}=0.6\\overline{6}\\). We can get predicted values for that probability from binomial distribution.\n\ntibble(\n  pred_binom = 0:9,\n  dens = dbinom(\n    pred_binom, \n    size = 9, \n    prob = 6/9\n  ),\n  prob = dens\n) -&gt; \n  ml_pred\n\nml_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#posterior-prediction",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#posterior-prediction",
    "title": "Predictive Distributions",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\nFor the posterior prediction, first we sample probabilities from the beta distribution\n\\[\np \\sim \\text{Beta}(W+1, L+1)\n\\]\n(a.k.a. dbeta())\nThen for each \\(p\\), we generate predictions from the binomial distribution.\n\\[\nY|p \\sim \\text{Bin}(9,p)\n\\]\nThis chapter has been doing grid sampling, but I‚Äôll just use the rbeta() function for simplicity.\n\ntibble(\n  p = rbeta(1e4, 6+1, 3+1)\n) -&gt;\n  posterior_samp\n\nThen, it‚Äôs the same operation as the prior predictive distribution.\n\nposterior_samp |&gt; \n   mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n  count(pred_binom) |&gt; \n  mutate(prob = n/sum(n)) -&gt;\n  posterior_pred\n\n\nposterior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col() +\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    ) +\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#comparisons",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#comparisons",
    "title": "Predictive Distributions",
    "section": "Comparisons",
    "text": "Comparisons\nNow, I want to compare the posterior predictive distribution to the maximum likelihood and the prior. This is one concept I had, which has the prior and the ML values as thinner bars within the posterior predictive bars.\n\nposterior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    geom_segment(\n      data = ml_pred,\n      aes(\n        x = pred_binom-0.1,\n        xend = pred_binom-0.1,\n        yend = 0,\n        color = \"maximum likelihood\"\n      ),\n      linewidth = 2\n    )+\n    geom_segment(\n      data = prior_pred,\n      aes(\n        x = pred_binom+0.1,\n        xend = pred_binom+0.1,\n        yend = 0,\n        color = \"prior\"\n      ),\n      linewidth = 2\n    )+  \n    scale_x_continuous(\n      breaks = seq(1, 9, length = 2)\n    )+\n    labs(\n      color = NULL,\n      x = \"predicted W\"\n    )+\n    theme_no_y()+\n    theme(\n      legend.position = \"top\"\n    )\n\n\n\n\nOverplotting them is also a possibility, and a good occasion to test out {ggblend}. I had some issues getting this to work with the available graphic devices & quarto.\n\n```{r}\n#| dev: \"png\"\n#| dev-args:\n#|   - type: \"cairo\"\nbind_rows(\n  posterior_pred |&gt; \n    mutate(pred = \"posterior\"),\n  ml_pred |&gt; \n    mutate(pred = \"maximum likelihood\"),\n  prior_pred |&gt; \n    mutate(pred = \"prior\")\n) |&gt; \n  ggplot(aes(pred_binom, prob, fill = pred))+\n    geom_area(position = \"identity\", alpha = 0.5) * \n      (blend(\"lighten\") + blend(\"darken\")) +\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n    theme_no_y()+\n    theme(\n      legend.position = \"top\"\n    )\n```"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#visualizing-the-posterior-prediction-process",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#visualizing-the-posterior-prediction-process",
    "title": "Predictive Distributions",
    "section": "Visualizing the posterior prediction process",
    "text": "Visualizing the posterior prediction process\n\nposterior_samp |&gt; \n  slice(1:4) |&gt; \n  mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n  arrange(\n    p\n  ) -&gt;\n  example_samp\n\n\nexample_samp |&gt; \n  mutate(\n    dens = dbeta(p, 6+1, 3+1),\n    n = row_number()\n  )-&gt;\n  example_samp\n\ntibble(\n  p = seq(0, 1, length = 100),\n  dens = dbeta(p, 6+1, 3+1)\n) |&gt; \n  ggplot(aes(p, dens))+\n    geom_area(fill = \"grey\") +\n    geom_segment(\n      data = example_samp,\n      aes(\n        xend = p,\n        yend = 0,\n        color = factor(n)\n      ),\n      linewidth = 1.5,\n      lineend = \"round\",\n      show.legend = F\n    )+\n    theme_no_y()+\n    labs(\n      title = \"Posterior\"\n    )-&gt;\n  posterior_plot\n\nThis involved messing around with {rlang} data masking that I still don‚Äôt quite follow. Both the p and pred arguments had to be (just once!) prefixed with !!, but not n.\n\nsample_plot_fun &lt;- function(p, pred, n){\n  this_scale &lt;- as.vector(khroma::color(\"bright\")(4))\n  tibble(\n    ws = 0:9,\n    dens = dbinom(ws, size = 9, prob = !!p)\n  ) |&gt; \n    ggplot(aes(ws, dens))+\n      geom_col(\n        aes(fill = ws == !!pred)\n      )+\n      scale_fill_manual(\n        values = c(\"grey\", this_scale[n]),\n        guide = \"none\"\n      ) +\n      scale_x_continuous(\n        breaks = seq(1,9, by = 2)\n      )+\n      labs(\n        x = \"prediction\",\n        title = str_glue(\"p = {round(p, digits = 2)}\")\n      ) +\n      theme_no_y()+\n      theme(\n        plot.title = element_text(size = 10)\n      )\n}\n\n\nexample_samp |&gt; \n  rowwise() |&gt; \n  mutate(\n    plot = list(\n      sample_plot_fun(p, pred_binom, n)\n    )\n  )-&gt;\n  samp_plots\n\n\nlibrary(patchwork)\n\nGoing to use some patchwork and purr::reduce() fanciness.\n\nposterior_plot/\n(samp_plots |&gt; \n  ungroup() |&gt; \n  pull(plot) |&gt; \n  reduce(.f = `+`) +\n  plot_layout(nrow = 1))\n\n\n\n\nPretty pleased with this!"
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html",
    "href": "posts/2023-05-15_04-sampling/index.html",
    "title": "Starting Sampling",
    "section": "",
    "text": "Listening\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(here)\nlibrary(gt)\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#classic-base-rate-issues",
    "href": "posts/2023-05-15_04-sampling/index.html#classic-base-rate-issues",
    "title": "Starting Sampling",
    "section": "Classic Base Rate Issues",
    "text": "Classic Base Rate Issues\n\n\n\n\nflowchart TD\n  p[\"population&lt;br&gt;10,000 individuals\"] --&gt; |0.01| v[\"üßõ‚Äç‚ôÇÔ∏è&lt;br&gt;100 individuals\"]\n  p--&gt; |0.99| h[\"üë®&lt;br&gt;9,900 individuals\"]\n  \n  v --&gt; |0.95| vpos[\"üßõ‚Äç‚ôÇÔ∏è‚ûï&lt;br&gt;95 individuals\"]\n  v --&gt; |0.05| vneg[\"üßõ‚Äç‚ôÇÔ∏è‚ûñTest Negative&lt;br&gt;5 individuals\"]\n  \n  h --&gt; |0.01| hpos[\"üë®‚ûï&lt;br&gt;99 individuals\"]\n  h --&gt; |0.99| hneg[\"üë®‚ûñTest Negative&lt;br&gt;9,801 individuals\"]\n  \n  vpos --o pos[\"(üßõ‚Äç‚ôÇÔ∏è,üë®)‚ûï&lt;br&gt;194\"]\n  hpos --o pos\n  \n  vneg --o neg[\"(üßõ‚Äç‚ôÇÔ∏è, üë®)‚ûñ&lt;br&gt;9806\"]\n  hneg --o neg\n\n\n\n\n\nPlot of the base rate vs P(vampire | positive test)\n\ntibble(\n  # might as well get logarithmic\n  base_rate = 10^(seq(-3, -1, length = 20)),\n  vamp_and_pos = base_rate * 0.95,\n  vamp_and_neg = base_rate * 0.05,\n  human_and_pos = (1-base_rate) * 0.01,\n  human_and_neg = (1-base_rate) * 0.99,\n  p_vamp_pos = vamp_and_pos/(vamp_and_pos + human_and_pos), \n  p_hum_neg = human_and_neg/(vamp_and_neg + human_and_neg)\n) -&gt; test_metrics\n\n\ntest_metrics |&gt; \n  ggplot(aes(base_rate, p_vamp_pos))+\n    geom_point(color = \"steelblue\", \n               size = 3)+\n    geom_line(color = \"steelblue\",\n              linewidth = 1)+\n    scale_x_log10()+\n    ylim(0,1)+\n    labs(x = \"P(vampire)\",\n         y = \"P(vampire | positive)\",\n         subtitle = \"P(positive | vampire) = 0.95\\nP(positive | human) = 0.01\",\n         title = \"Positive Predictive Value\") +\n    theme(plot.subtitle = element_text(size = 12))\n\n\n\n\nFigure¬†1: Probability someone is a vampire, given that they tested positive, relative to the base rate of being a vampire\n\n\n\n\n\ntest_metrics |&gt; \n  ggplot(aes(base_rate, p_hum_neg))+\n    geom_point(color = \"steelblue\", \n               size = 3)+\n    geom_line(color = \"steelblue\",\n              linewidth = 1)+\n    scale_x_log10()+\n    labs(x = \"P(vampire)\",\n         y = \"P(human | negative)\",\n         subtitle = \"P(positive | vampire) = 0.95\\nP(positive | human) = 0.01\",\n         title = \"Negative Predictive Value\") +\n    theme(plot.subtitle = element_text(size = 12))\n\n\n\n\nFigure¬†2: Probability of being a human given a negative test, relative to the base rate of being a vampire."
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#tibble-grid-sampling",
    "href": "posts/2023-05-15_04-sampling/index.html#tibble-grid-sampling",
    "title": "Starting Sampling",
    "section": "Tibble grid sampling",
    "text": "Tibble grid sampling\nEstimating posterior density from grid sampling.\n\ngrid &lt;- tibble(\n  # The grid\n  prob = seq(0.0001, 0.9999, length = 5000), \n  \n  # the prior\n  prior_unstd = exp(-abs(prob - .5) / .25),\n  prior_std = prior_unstd/sum(prior_unstd),\n  \n  # the data\n  data = dbinom(6, size = 9, prob = prob),\n  \n  # the posterior\n  posterior_unstd = prior_std * data,\n  posterior = posterior_unstd / sum(posterior_unstd)\n)\n\n\ngrid |&gt; \n  ggplot(aes(prob, prior_std))+\n    geom_line()+\n    labs(y = \"prior density\",\n         title = \"Prior\") -&gt; \n  prior_plot\n\ngrid |&gt; \n  ggplot(aes(prob, data))+\n    geom_line()+\n    labs(y = \"data density\",\n         title = \"Data\") -&gt; \n  data_plot\n\ngrid |&gt; \n  ggplot(aes(prob, posterior))+\n    geom_line() +\n    labs(y = \"posterior density\",\n         title = \"Posterior\") -&gt; \n  posterior_plot\n\nprior_plot | data_plot | posterior_plot\n\n\n\n\nFigure¬†3: Prior, Data, Posterior\n\n\n\n\nSampling from the posterior, using sample_n().\n\ngrid |&gt; \n  sample_n(size = 1e4, \n           replace = T,\n           weight = posterior)-&gt;\n  posterior_samples\n\n\nhead(posterior_samples)\n\n# A tibble: 6 √ó 6\n   prob prior_unstd prior_std  data posterior_unstd posterior\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 0.703       0.444  0.000205 0.266       0.0000546  0.000419\n2 0.694       0.461  0.000213 0.269       0.0000574  0.000441\n3 0.670       0.507  0.000234 0.273       0.0000640  0.000491\n4 0.559       0.789  0.000365 0.220       0.0000803  0.000616\n5 0.620       0.619  0.000286 0.262       0.0000749  0.000575\n6 0.632       0.591  0.000273 0.267       0.0000729  0.000559\n\n\nI‚Äôm going to mess around with finessing the visualizations here.\n\nrenv::install(\"tidybayes\")\n\n\nlibrary(tidybayes)\n\n\nposterior_samples |&gt; \n  pull(prob) |&gt; \n  density() |&gt; \n  tidy() |&gt; \n  rename(prob = x, density = y) -&gt;\n  posterior_dens\n\nposterior_dens |&gt; \n  ggplot(aes(prob, density/max(density)))+\n    geom_area(fill = \"grey60\")+\n    geom_line(aes(y = posterior/max(posterior)),\n              linetype = 2,\n              data = grid)+\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†4: Comparison of the kernel density estimate vs the actual posterior distribution.\n\n\n\n\n\nposterior_samples |&gt; \n  median_hdci(prob, .width = c(0.5, 0.95)) -&gt;\n  intervals\nintervals |&gt; \n  gt() |&gt; \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n    \n  \n  \n    0.59\n0.48\n0.65\n0.50\nmedian\nhdci\n    0.59\n0.37\n0.85\n0.95\nmedian\nhdci\n  \n  \n  \n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(aes(fill = after_stat(pdf)), \n              fill_type = \"gradient\")+\n    scale_y_continuous(expand = expansion(mult = c(0,0)))+\n    khroma::scale_fill_batlow() +\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†5: Posterior density, colored according to the probability density function.\n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(x &gt;= 0.5)),\n      fill_type = \"gradient\"\n    ) +\n    scale_fill_manual(\n      values = c(\n        \"grey70\",\n        \"steelblue\"\n      )\n    )+\n   scale_y_continuous(expand = expansion(mult = c(0,0)))+\n   theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†6: The posterior density colored according to a critical value (0.5)\n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_halfeye(\n      aes(fill = after_stat(level)),\n      fill_type = \"gradient\",\n      point_interval = \"median_hdi\"\n    ) +\n   scale_y_continuous(expand = expansion(mult = c(0.05,0)))+\n   scale_fill_manual(\n     values = c(\"steelblue\", \"steelblue4\")\n   )+\n   theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†7: Posterior samples, colored by their highest density interval levels.\n\n\n\n\nCan I get the plot into a {gt} table? I thin I‚Äôll need to map over the widths? I‚Äôm going off of this gt help page: https://gt.rstudio.com/reference/ggplot_image.html. Let me get the plot right first.\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(level)),\n      .width = 0.66,\n      fill_type = \"gradient\",\n      point_interval = \"median_hdci\"\n    ) +\n    stat_slab(\n      fill = NA,\n      color = \"black\"\n    )+\n    scale_x_continuous(\n      limits = c(0,1),\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_y_continuous(\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_fill_manual(values = \"steelblue\", \n                      guide = \"none\")+\n    theme_void()\n\n\n\n\nFigure¬†8: Table figure experimentation\n\n\n\n\n\nmake_table_plot &lt;- function(.width, data) {\n  ggplot(data, aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(level)),\n      .width = .width,\n      point_interval = \"median_hdci\"\n    ) +\n    stat_slab(\n      fill = NA,\n      color = \"black\"\n    )+\n    scale_x_continuous(\n      limits = c(0,1),\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_y_continuous(\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_fill_manual(values = \"steelblue\", \n                      guide = \"none\")+\n    theme_void()\n}\n\nMap that function over the intervals table I made before.\n\nintervals |&gt; \n  mutate(\n    ggplot = map(.width, ~make_table_plot(.x, posterior_samples)),\n    \n    ## adding an empty column\n    dist = NA\n  ) -&gt; to_tibble\n\nto_tibble |&gt; \n  select(-ggplot) |&gt; \n  gt() |&gt; \n  text_transform(\n    locations = cells_body(columns = dist),\n    fn = \\(x) map(to_tibble$ggplot, ggplot_image, aspect_ratio = 2)\n  )\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n      dist\n    \n  \n  \n    0.587\n0.4815000\n0.6503000\n0.50\nmedian\nhdci\n\n    0.587\n0.3695874\n0.8489874\n0.95\nmedian\nhdci\n\n  \n  \n  \n\n\n\n\nI‚Äôd like more control over how the image appears in the table. Looks like I‚Äôll have to ggsave, and then embed.\n\nmake_custom_table_plot &lt;- function(p){\n  filename &lt;- tempfile(fileext = \".png\")\n  ggsave(plot = p, \n         filename = filename, \n         device = ragg::agg_png, \n         res = 100, \n         width =1.5,\n         height = 0.75)\n  local_image(filename=filename)\n}\n\n\nto_tibble |&gt; \n  select(-ggplot) |&gt; \n  gt() |&gt; \n  text_transform(\n    locations = cells_body(columns = vars(dist)),\n    fn = \\(x) map(to_tibble$ggplot, make_custom_table_plot)\n  )\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n      dist\n    \n  \n  \n    0.587\n0.4815000\n0.6503000\n0.50\nmedian\nhdci\n\n    0.587\n0.3695874\n0.8489874\n0.95\nmedian\nhdci\n\n  \n  \n  \n\n\n\n\nThere we go!\nTurns out this local_image() thing doesn‚Äôt play nice with conversion to pdf (üòï)."
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#brms",
    "href": "posts/2023-05-15_04-sampling/index.html#brms",
    "title": "Starting Sampling",
    "section": "BRMS",
    "text": "BRMS\n\nlibrary(brms)\n\n\ntibble(\n  water = 6,\n  samples = 9\n)-&gt; \n  water_to_model\n\n\nwater_form &lt;- bf(\n   water | trials(samples) ~ 1,\n   family = binomial(link = \"identity\")\n)\n\n\nbrm(\n  water | trials(samples) ~ 1,\n  data = water_to_model,\n  family = binomial(link = \"identity\"),\n  prior(beta(1, 1), class = Intercept, ub = 1, lb = 0),\n  file_refit = \"on_change\",\n  file = \"water_fit.rds\"\n) -&gt;\n  water_model\n\n\nwater_model\n\n Family: binomial \n  Links: mu = identity \nFormula: water | trials(samples) ~ 1 \n   Data: water_to_model (Number of observations: 1) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.64      0.14     0.35     0.88 1.00     1598     1945\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nlibrary(gtsummary)\n\n\nwater_model |&gt; \n  gtsummary::tbl_regression(intercept = T)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n    \n  \n  \n    (Intercept)\n0.64\n0.35, 0.88\n  \n  \n  \n    \n      1 CI = Credible Interval\n    \n  \n\n\n\n\nLet‚Äôs do this again.\n\nwater_model |&gt; \n  get_variables()\n\n[1] \"b_Intercept\"   \"lprior\"        \"lp__\"          \"accept_stat__\"\n[5] \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"  \n[9] \"energy__\"     \n\n\n\nwater_model |&gt; \n  gather_draws(b_Intercept)-&gt;\n  model_draws\n\nmodel_draws |&gt; \n  head() |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\nmodel_draws |&gt; \n  ggplot(aes(.value, .variable)) + \n    stat_halfeye(\n      point_interval = median_hdi,\n      aes(fill = after_stat(level)),\n      fill_type = \"gradient\"\n    ) +\n    xlim(0,1)+\n    scale_fill_manual(\n      values = c(\"steelblue4\", \"steelblue\"),\n    )"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html",
    "href": "posts/2023-06-06_08-linear-models-2/index.html",
    "title": "Linear models, part 2",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#loading",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#loading",
    "title": "Linear models, part 2",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(ggblend)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#finessing-the-model-diagram",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#finessing-the-model-diagram",
    "title": "Linear models, part 2",
    "section": "Finessing the model diagram",
    "text": "Finessing the model diagram\nI think I‚Äôve improved on the model diagram from the last post. Some things I‚Äôm still struggling with:\n\nFormatting the node text. Can‚Äôt seem to get either html nor markdown formatting to work.\nThe background color on edge labels is absent, making the ‚Äú~‚Äù illegible.\n\n\nflowchart TD\n  subgraph exp2[\"Exp\"]\n    one2[\"1\"]\n  end\n  exp2 -.-&gt; sigma4\n  \n  subgraph normal4[\"normal\"]\n    mu4[\"Œº=0\"]\n    sigma4[\"œÉ\"]\n  end\n  normal4 -.-&gt; Gamma[Œ≥j]\n  Gamma --&gt; gamma1\n  \n  subgraph normal3[\"normal\"]\n    mu3[\"Œº=0\"]\n    sigma3[\"œÉ=10\"]\n  end\n  normal3 -.-&gt; beta1\n  \n  subgraph normal2[\"normal\"]\n    mu2[\"Œº=0\"]\n    sigma2[\"œÉ=10\"]\n  end\n  normal2 -.-&gt; beta0\n  \n  subgraph sum1[\"+\"]\n    beta0[\"Œ≤‚ÇÄ\"]\n    subgraph mult1[\"√ó\"]\n      beta1[\"Œ≤‚ÇÅ\"]\n      x[\"x·µ¢\"]\n    end\n    gamma1[\"Œ≥j[i]\"]\n  end\n  \n  sum1 --&gt; mu1\n  \n  subgraph exp1[\"Exp\"]\n    one1[1]\n  end\n  exp1 -.-&gt;|\"~\"| sigma1\n  \n  subgraph normal1[\"normal\"]\n    mu1[\"Œº·µ¢\"]\n    sigma1[\"œÉ\"]\n  end\n  \n  normal1 -.-&gt;|\"~\"| y[\"y·µ¢\"]\n\n\n\nflowchart TD\n  subgraph exp2[\"Exp\"]\n    one2[\"1\"]\n  end\n  exp2 -.-&gt; sigma4\n  \n  subgraph normal4[\"normal\"]\n    mu4[\"Œº=0\"]\n    sigma4[\"œÉ\"]\n  end\n  normal4 -.-&gt; Gamma[Œ≥j]\n  Gamma --&gt; gamma1\n  \n  subgraph normal3[\"normal\"]\n    mu3[\"Œº=0\"]\n    sigma3[\"œÉ=10\"]\n  end\n  normal3 -.-&gt; beta1\n  \n  subgraph normal2[\"normal\"]\n    mu2[\"Œº=0\"]\n    sigma2[\"œÉ=10\"]\n  end\n  normal2 -.-&gt; beta0\n  \n  subgraph sum1[\"+\"]\n    beta0[\"Œ≤‚ÇÄ\"]\n    subgraph mult1[\"√ó\"]\n      beta1[\"Œ≤‚ÇÅ\"]\n      x[\"x·µ¢\"]\n    end\n    gamma1[\"Œ≥j[i]\"]\n  end\n  \n  sum1 --&gt; mu1\n  \n  subgraph exp1[\"Exp\"]\n    one1[1]\n  end\n  exp1 -.-&gt;|\"~\"| sigma1\n  \n  subgraph normal1[\"normal\"]\n    mu1[\"Œº·µ¢\"]\n    sigma1[\"œÉ\"]\n  end\n  \n  normal1 -.-&gt;|\"~\"| y[\"y·µ¢\"]\n\n\n\n\n\nHere‚Äôs the global water model. I‚Äôll replace the \\(\\mathcal{U}(0,1)\\) prior with the equivalent beta distribution, just for the consistency of going beta -&gt; binomial/bernoulli. I‚Äôll also notate the beta distribution with mean and precision.\n\\[\n\\mathcal{U}(0,1) = \\text{Beta}(a=1, b=1) = \\text{Beta}(\\mu=0.5, \\phi=2)\n\\]\nbecause\n\\[\na = \\mu\\phi\n\\]\n\\[\nb = (1-\\mu)\\phi\n\\]\n\n\n\n\nflowchart TD\n\nsubgraph beta1[\"beta\"]\n  mu[\"Œº=0.5\"]\n  phi[\"œï=2\"]\nend\nbeta1 -.-&gt; p\n\nsubgraph binomial1[\"binomial\"]\n  N\n  p\nend\n\nbinomial1 -.-&gt; W"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#height-data",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#height-data",
    "title": "Linear models, part 2",
    "section": "Height data",
    "text": "Height data\n{cmdstanr} is a dependency for {rmcelreath/rethinking}, and I don‚Äôt want to deal with that right now, so I‚Äôm just going to read the data from github.\n\nread_delim(\n  \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\", \n  delim = \";\"\n) -&gt;\n  Howell1\n\n\nlibrary(gt)\nlibrary(gtsummary)\n\n{gtsummary} has a summary table function that‚Äôs pretty ok. Not sure how to incorporate histograms into it like rethinking::precis().\n\nHowell1 |&gt; \n  tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 5441\n    \n  \n  \n    height\n149 (125, 157)\n    weight\n40 (22, 47)\n    age\n27 (12, 43)\n    male\n257 (47%)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nI‚Äôll get histograms with some pivoting and ggdist::stat_slab(density=\"histogram\")\n\nHowell1 |&gt; \n  mutate(row = row_number()) |&gt; \n  pivot_longer(\n    -row,\n    names_to = \"variable\",\n    values_to = \"value\"\n    ) |&gt; \n  ggplot(aes(value))+\n    stat_slab(\n      normalize = \"panels\", \n      density = \"histogram\"\n    )+\n    facet_wrap(\n      ~variable, \n      scales = \"free\"\n    )+\n    theme_no_y()\n\n\n\n\nHeight has a pretty long leftward tail because children are included in the data.\n\nHowell1 |&gt; \n  ggplot(aes(height, factor(male)))+\n    stat_slab()\n\n\n\n\n\nHowell1 |&gt; \n  ggplot(aes(age, height, color = factor(male)))+\n    geom_point()+\n    stat_smooth(method = \"gam\", formula = y ~ s(x, bs = 'cs'))\n\n\n\n\n\nAside, experimenting with {marginaleffects}\nThe Rethinking book just cuts the age at 18, but the trend for men and women in the figure above looks like it‚Äôs still increasing until at least 25. I‚Äôll mess around with marginaleffects::slopes() to see when the growth trend really stops.\n\nlibrary(mgcv)\nlibrary(marginaleffects)\n\nmgcv::gam() doesn‚Äôt like it when the s(by=‚Ä¶) argument isn‚Äôt a factor, so preparing for modelling.\n\nHowell1 |&gt; \n  mutate(male = factor(male)) -&gt;\n  height_to_mod\n\n\nmod &lt;- gam(height ~ male + s(age, by = male), data = height_to_mod)\n\nI‚Äôd have to double check the documentation for how to specify which variable you want the slope across, but I know how to do it with a new dataframe, so I‚Äôll just do that and filter. I set eps to 1, which I think will estimate the number of centimeters per year.\n\nslopes(\n  mod,\n  eps = 1,\n  newdata = datagrid(\n    age = 0:80,\n    male = c(0,1)\n  )\n) |&gt; \n  as_tibble() |&gt; \n  filter(term == \"age\") -&gt;\n  age_slopes\n\nage_slopes |&gt; \n  ggplot(aes(age, estimate, color = male))+\n    geom_ribbon(\n      aes(\n        ymin = conf.low,\n        ymax = conf.high,\n        fill = male\n      ),\n      alpha = 0.5\n    )\n\n\n\n\nAs a quick and dirty heuristic, I‚Äôll just check what the earliest age is that the high and low sides of the confidence interval have different signs.\n\nage_slopes |&gt; \n  filter(sign(conf.low) != sign(conf.high))  |&gt; \n  arrange(age) |&gt; \n  group_by(male) |&gt; \n  slice(1) |&gt; \n  select(term, age, male, estimate, conf.low, conf.high)\n\n# A tibble: 2 √ó 6\n# Groups:   male [2]\n  term    age male  estimate conf.low conf.high\n  &lt;chr&gt; &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 age      24 0        0.258  -0.0508     0.567\n2 age      28 1        0.176  -0.103      0.455\n\n\nLooks like the age women probably stopped growing is ~24 and for men ~28. So I‚Äôll filter the data for age &gt;= 30 just to be safe."
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#height-normality",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#height-normality",
    "title": "Linear models, part 2",
    "section": "Height normality",
    "text": "Height normality\n\nstable_height &lt;- Howell1 |&gt; \n  filter(age &gt;= 30)\n\n\nstable_height |&gt; \n  ggplot(aes(height))+\n    stat_slab()\n\n\n\n\n\nstable_height |&gt; \n  ggplot(aes(height, factor(male)))+\n    stat_slab()"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#the-model",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#the-model",
    "title": "Linear models, part 2",
    "section": "The Model",
    "text": "The Model\nRethinking gives the following model specification.\n\\[\nh_i \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\n\\[\n\\mu \\sim \\mathcal{N}(178, 20)\n\\]\n\\[\n\\sigma \\sim \\mathcal{U}(0,50)\n\\]\n\n\n\n\nflowchart TD\n\nsubgraph uniform1[\"uniform\"]\n  a[\"a=0\"]\n  b[\"b=50\"]\nend\nuniform1 -.-&gt; sigma1\n\nsubgraph normal2[\"normal\"]\n  mu2[\"Œº=178\"]\n  sigma2[\"œÉ=20\"]\nend\nnormal2 -.-&gt; mu1\n\nsubgraph normal1[\"normal\"]\n  mu1[\"Œº\"]\n  sigma1[\"œÉ\"]\nend\n\nnormal1 -.-&gt; h[\"h·µ¢\"]\n\n\n\n\n\nJust for some heuristics, I‚Äôll calculate the mean, standard error of the mean, and standard deviation of the data.\n\nstable_height |&gt; \n  summarise(\n     mean = mean(height),\n     sd = sd(height),\n     sem = sd/sqrt(n())\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(decimals = 1)\n\n\n\n\n\n  \n    \n    \n      mean\n      sd\n      sem\n    \n  \n  \n    154.6\n7.8\n0.5\n  \n  \n  \n\n\n\n\nSo, the \\(\\sigma\\) for the hyperprior is much higher than the standard error, which is good, cause I guess we‚Äôd want our prior to be looser than the uncertainty we have about the sample mean.\nI think I‚Äôd like to look at our sample estimates and how they compare to the priors.\n\nbind_rows(\n  tibble(\n    x = seq(118, 238, length = 100),\n    dens = dnorm(\n      x,\n      mean = 178,\n      sd = 20\n    ),\n    param = \"mu\"\n  ),\n  tibble(\n    x = seq(0, 50, length = 100),\n    dens = dunif(x, 0, 50),\n    param = \"sigma\"\n  )\n)-&gt;\n  model_priors\n\nbind_rows(\n  tibble(\n    param = \"mu\",\n    x = 154.6,\n    dens = dnorm(\n      x,\n      mean = 178,\n      sd = 20\n    ),\n  ),\n  tibble(\n    param = \"sigma\",\n    x = 7.8,\n    dens = dunif(x, 0, 50)\n  )\n)-&gt;\n  sample_estimates\n\n\nmodel_priors |&gt; \n  ggplot(aes(x, dens))+\n    geom_area(fill = \"grey80\")+\n    geom_point(\n      data = sample_estimates,\n      size = 3\n    )+\n    geom_segment(\n      data = sample_estimates,\n      aes(\n        xend = x,\n        yend = 0\n      ),\n      linewidth = 1\n    )+\n    facet_wrap(\n      ~param, \n      scales = \"free_x\"\n      )+\n    theme_no_y()\n\n\n\n\nI‚Äôll try setting up the priors like they are in the book without looking at Solomon Kurz‚Äô translation, then double check I did it right.\n\nlibrary(brms)\n\nI know that you can set up a model formula with just bf().\n\nheight_formula &lt;- bf(\n  height ~ 1\n)\n\nAnd I know you can get a table of the default priors it plans to use with get_prior().\n\nget_prior(height_formula, data = stable_height) |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    student_t(3, 153.7, 9.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n    student_t(3, 0, 9.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n  \n  \n  \n\n\n\n\n{ggdist} has a way of parsing and plotting these distributions pretty directly, but to get it how I want it to be requires getting a little hacky with ggplot2.\n\n\nCode\nget_prior(height_formula, data = stable_height) |&gt; \n  parse_dist(prior) |&gt; \n  ggplot(aes(dist = .dist, args = .args))+\n    stat_slab(aes(fill = after_stat(y&gt;0)))+\n    facet_wrap(~class, scales = \"free_x\")+\n    scale_fill_manual(\n      values = c(\"#ffffff00\", ptol_blue),\n      guide = \"none\")+\n    coord_flip()\n\n\n\n\n\nAnyway, to set up the priors like it is in the book, we need to do this. (Note from future Joe: I‚Äôd gotten this close, but had messed up how non-standard evaluation works and had to check Solomon Kurz‚Äô book. e.g, there‚Äôs no function called normal()).\n\nc(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  )\n) -&gt; example_priors\n\nexample_priors |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    normal(178, 20)\nIntercept\n\n\n\n\n\nNA\nNA\nuser\n    uniform(0, 50)\nsigma\n\n\n\n\n\n0\n50\nuser\n  \n  \n  \n\n\n\n\n\nbrm(\n  height_formula,\n  prior = example_priors,\n  family = gaussian,\n  data = stable_height,\n  sample_prior = T,\n  file = \"height_mod.rds\"\n) -&gt;\n  height_mod\n\n\nheight_mod\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: stable_height (Number of observations: 251) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.50   153.64   155.60 1.00     3070     2230\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.88      0.36     7.20     8.63 1.00     3523     2251\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWell! Estimated parameters here are basically right on top of the maximum likelihood estimates from the sample, including the standard error of the Intercept.\nHaving read over the marginaleffects book, I know I can get posterior draws of the predictions with predictions() |&gt; posterior_draws()\n\npredictions(\n  height_mod,\n  newdata = datagrid()\n) |&gt; \n  posterior_draws() |&gt; \n  ggplot(aes(draw))+\n    stat_slabinterval()+\n    theme_no_y()\n\n\n\n\nSince this was an intercept-only model, this is basically a distribution of the estimate for the intercept, rather than predicted observed values. So I can actually compare this to the prior.\n\npredictions(\n  height_mod,\n  newdata = datagrid()\n) |&gt; \n  posterior_draws() |&gt; \n  ggplot(aes(draw))+\n    stat_slabinterval()+\n    geom_line(\n      data = model_priors |&gt; \n        filter(param == \"mu\"),\n      aes(\n        x = x,\n        y = dens/max(dens),\n        )\n    )+\n    theme_no_y()\n\n\n\n\nTo compare the predicted observed values from the model to the actual data, we can use brms::pp_check().\n\npp_check(height_mod)+\n  khroma::scale_color_bright()\n\n\n\n\n\nGeneral look at parameters\nTo get the posterior samples of the parameters, I think we need to turn to tidybayes.\n\nlibrary(tidybayes)\n\nTo get the parameter names that we want to get samples from, tidybayes::get_variables() on the model.\n\nget_variables(height_mod)\n\n [1] \"b_Intercept\"     \"sigma\"           \"prior_Intercept\" \"prior_sigma\"    \n [5] \"lprior\"          \"lp__\"            \"accept_stat__\"   \"stepsize__\"     \n [9] \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n\n\nThe non standard evaluation here still kind of freaks me out. I‚Äôll use spread_draws() which will put the posterior draw for each parameter in its own column.\n\nheight_mod |&gt; \n  spread_draws(\n   b_Intercept,\n   sigma\n  ) -&gt;\n  height_param_wide\n\nhead(height_param_wide)\n\n# A tibble: 6 √ó 5\n  .chain .iteration .draw b_Intercept sigma\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1      1          1     1        155.  8.18\n2      1          2     2        155.  7.51\n3      1          3     3        155.  7.91\n4      1          4     4        155.  7.77\n5      1          5     5        154.  7.77\n6      1          6     6        154.  7.77\n\n\n\nheight_param_wide |&gt; \n  ggplot(aes(b_Intercept, sigma))+\n    geom_point()+\n    theme(aspect.ratio = 1)\n\n\n\n\nTo get the parameters long-wise, we need to use gather_draws(). I‚Äôm assuming the function names for {tidybayes} were settled in back when the pivoting functions in {tidyr} were still gather() and spread().1\n\nheight_mod |&gt; \n  gather_draws(\n   b_Intercept,\n   sigma\n  ) -&gt;\n  height_param_long\n\nhead(height_param_long)\n\n# A tibble: 6 √ó 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable   .value\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1      1          1     1 b_Intercept   155.\n2      1          2     2 b_Intercept   155.\n3      1          3     3 b_Intercept   155.\n4      1          4     4 b_Intercept   155.\n5      1          5     5 b_Intercept   154.\n6      1          6     6 b_Intercept   154.\n\n\n\nheight_param_long |&gt; \n  ggplot(\n    aes(.value,)\n  )+\n    stat_slab()+\n    theme_no_y()+\n    facet_wrap(\n      ~.variable,\n      scales = \"free_x\"\n    )\n\n\n\n\n\nlibrary(ggdensity)\n\n\nheight_param_wide |&gt;  \n  ggplot(aes(b_Intercept, sigma))+\n    stat_hdr(fill = ptol_blue)+\n    theme(aspect.ratio = 1)"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#the-linear-model",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#the-linear-model",
    "title": "Linear models, part 2",
    "section": "The linear model",
    "text": "The linear model\nThe next thing the book moves onto is modelling height with weight.\n\nstable_height |&gt; \n  ggplot(aes(weight, height))+\n    geom_point()\n\n\n\n\nWe‚Äôre going to standardize the weight measure. That way, the intercept & prior for the intercept will be defined at the mean weight.\n\nstable_height |&gt; \n  mutate(\n    weight0 = weight-mean(weight)\n  )-&gt;\n  height_to_mod\n\n\nheight_weight_formula &lt;- bf(\n  height ~ 1 + weight0\n)\n\nGet the default priors\n\nget_prior(\n  height_weight_formula,\n  data = height_to_mod\n) |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    \nb\n\n\n\n\n\n\n\ndefault\n    \nb\nweight0\n\n\n\n\n\n\ndefault\n    student_t(3, 153.7, 9.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n    student_t(3, 0, 9.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n  \n  \n  \n\n\n\n\nDefine our custom priors, based on the first model in the book.\n\nheight_weight_priors &lt;- c(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  ),\n  prior(\n    prior = normal(0,10),\n    class = b\n  )\n)\n\nheight_weight_priors |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    normal(178, 20)\nIntercept\n\n\n\n\n\nNA\nNA\nuser\n    uniform(0, 50)\nsigma\n\n\n\n\n\n0\n50\nuser\n    normal(0, 10)\nb\n\n\n\n\n\nNA\nNA\nuser\n  \n  \n  \n\n\n\n\n\n\nCode\nheight_weight_priors |&gt; \n  parse_dist(prior) |&gt; \n  ggplot(aes(dist = .dist, args = .args))+\n    stat_slab()+\n    facet_wrap(~class, scales = \"free_x\")+\n    coord_flip()\n\n\n\n\n\n\nbrm(\n  height_weight_formula,\n  prior = height_weight_priors,\n  data = height_to_mod, \n  file = \"height_weight_mod.rds\",\n  file_refit = \"on_change\"\n) -&gt;\n  height_weight_mod\n\n\nheight_weight_mod\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight0 \n   Data: height_to_mod (Number of observations: 251) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.58      0.33   153.93   155.23 1.00     3643     2923\nweight0       0.91      0.05     0.81     1.01 1.00     3978     2773\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.16      0.23     4.73     5.66 1.00     3752     2830\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere‚Äôs the usual kind of ‚Äúfit + credible interval‚Äù plot.\n\nheight_weight_mod |&gt; \n  predictions(\n    newdata = datagrid(\n      weight0 = seq(-13, 18, length = 100)\n    )\n  ) |&gt; \n  posterior_draws() |&gt; \n  mutate(\n    weight = weight0 + mean(height_to_mod$weight)\n  ) |&gt; \n  ggplot(\n    aes(weight, draw)\n  )+\n    stat_lineribbon()+\n    labs(\n      y = \"height\"\n    )+\n    scale_fill_brewer(palette = \"Blues\")\n\n\n\n\nHere‚Äôs the ‚Äúall of the predicted fitted lines‚Äù plot.\n\nheight_weight_mod |&gt; \n  predictions(\n    newdata = datagrid(\n      weight0 = seq(-13, 18, length = 100)\n    )\n  ) |&gt; \n  posterior_draws() |&gt; \n  mutate(\n    weight = weight0 + mean(height_to_mod$weight)\n  ) |&gt; \n  filter(\n    as.numeric(drawid) &lt;= 100\n  ) |&gt; \n  ggplot(\n    aes(weight, draw)\n  )+\n    geom_line(\n      aes(group = drawid),\n      alpha = 0.1\n    )+\n    labs(\n      y = \"height\"\n    )\n\n\n\n\n\npp_check(height_weight_mod)+\n  khroma::scale_color_bright()\n\n\n\n\n\nheight_weight_mod |&gt; \n  get_variables()\n\n [1] \"b_Intercept\"   \"b_weight0\"     \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nheight_weight_mod |&gt; \n  spread_draws(\n    `b_.*`,\n    sigma,\n    regex = T\n  ) |&gt; \n  ggplot(aes(b_Intercept, b_weight0))+\n    stat_hdr()+\n    theme(aspect.ratio = 1)"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#footnotes",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#footnotes",
    "title": "Linear models, part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI still have a soft spot for reshape2::melt() and reshape2::cast().‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-05-09_01-golem-of-prague/index.html",
    "href": "posts/2023-05-09_01-golem-of-prague/index.html",
    "title": "01 Golem of Prague Chapter",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-05-09_01-golem-of-prague/index.html#hypotheses-models-statistics",
    "href": "posts/2023-05-09_01-golem-of-prague/index.html#hypotheses-models-statistics",
    "title": "01 Golem of Prague Chapter",
    "section": "Hypotheses != Models != Statistics",
    "text": "Hypotheses != Models != Statistics\n\n\n\n\nflowchart LR\n  H0(\"H0&lt;br&gt;Evolution is Neutral\") &lt;--&gt; P0A[\"Process&lt;br&gt;Neutral Equilibrium\"] \n  H0 &lt;--&gt; P0B[\"Process&lt;br&gt;Neutral Non-Equilibrium\"]\n  H1(\"H1&lt;br&gt;Selection Matters\") &lt;--&gt; P1A[\"Process&lt;br&gt;Constant Selection\"]\n  H1 &lt;--&gt; P1B[\"Process&lt;br&gt;Fluctuating Selection\"]\n  \n  P0A &lt;--&gt; MII([\"Model2&lt;br&gt;Power Law\"])\n  P1B &lt;--&gt; MII\n  P1A &lt;--&gt; MIII([\"Model3&lt;br&gt;'something different'\"])\n  P0B &lt;--&gt; MI([\"Model1&lt;br&gt;another thing\"])\n\n\n\n\n\nRejecting Model 1 does not result in a unique identification of a process, or even a hypothesis."
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html",
    "href": "posts/2023-06-14_09-reporting/index.html",
    "title": "Reporting a linear model",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#loading",
    "href": "posts/2023-06-14_09-reporting/index.html#loading",
    "title": "Reporting a linear model",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(gt)\n\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(broom.mixed)\n\nsource(here::here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#the-plan",
    "href": "posts/2023-06-14_09-reporting/index.html#the-plan",
    "title": "Reporting a linear model",
    "section": "The plan",
    "text": "The plan\nPart of why I‚Äôm working through Statistical Rethinking as a blog is so that I can take some time and mess around with finessing how I‚Äôll visualize and report models like this, so I‚Äôm going to try to work over these basic models I just fit.\n\nData loading, prep, and model fits\n\nread_delim(\n  \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\", \n  delim = \";\"\n) -&gt;\n  Howell1\n\n\nstable_height &lt;- Howell1 |&gt; \n  filter(age &gt;= 30)\n\nstable_height |&gt; \n  mutate(\n    weight0 = weight-mean(weight)\n  )-&gt;\n  height_to_mod\n\n\n\n\n\n\n\nheight_mod\n\n\n\n\n\n\nheight_formula &lt;- bf(\n  height ~ 1\n)\n\n\nc(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  )\n) -&gt;\n  height_mod_priors\n\n\nbrm(\n  height_formula,\n  prior = height_mod_priors,\n  family = gaussian,\n  data = stable_height,\n  sample_prior = T,\n  save_pars = save_pars(all = TRUE),\n  file = \"height_mod.rds\",\n  file_refit = \"on_change\"\n) -&gt;\n  height_mod\n\n\n\n\n\n\n\n\n\n\nheight_weight_mod\n\n\n\n\n\n\nheight_weight_formula &lt;- bf(\n  height ~ 1 + weight0\n)\n\n\nheight_weight_priors &lt;- c(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  ),\n  prior(\n    prior = normal(0,10),\n    class = b\n  )\n)\n\n\nbrm(\n  height_weight_formula,\n  prior = height_weight_priors,\n  data = height_to_mod, \n  file = \"height_weight_mod.rds\",\n  save_pars = save_pars(all = TRUE),\n  file_refit = \"on_change\"\n) -&gt;\n  height_weight_mod"
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#first-look---model-fit",
    "href": "posts/2023-06-14_09-reporting/index.html#first-look---model-fit",
    "title": "Reporting a linear model",
    "section": "First look - Model Fit",
    "text": "First look - Model Fit\n\nPosterior predictive check\nFirst I‚Äôll go with the default type of pp_check()\n\npp_check(height_mod, ndraws = 100)+\n  khroma::scale_color_bright()+\n  labs(\n    color = NULL,\n    title = \"height ~ 1\"\n  )+\n  theme_no_y()\n\n\n\n\nFigure¬†1: Posterior preditctive check for the height~1 model\n\n\n\n\nSo, the distribution of posterior predictions for the intercept only model puts a lot of probability where there‚Äôs an actual dip in the original data.\n\npp_check(height_weight_mod, ndraws = 100)+\n  khroma::scale_color_bright()+\n  labs(\n    color = NULL,\n    title = \"height ~ weight\"\n  )+\n  theme_no_y()\n\n\n\n\nFigure¬†2: Posterior predictive check for the height ~ weight model.\n\n\n\n\n\n\n\\(R^2\\)\nLet‚Äôs get some goodness of fit parameters. {brms} / {rstantools} have bayes_R2() which cites Gelman et al. (2019). Classic \\(R^2\\) is \\(1-\\frac{\\text{residuals variance}}{\\text{data variance}}\\). As Gelman et al. (2019) point out, there‚Äôs no one set of residuals, since the model parameters are all distributions rather than point estimates, so they propose an \\(R^2\\) for Bayesian models as \\(\\frac{\\text{variance of fitted values}}{\\text{variance of fitted values} + \\text{variance of residuals}}\\), for sampled fitted values and their respective residuals.\nBut, as they say\n\nA new issue then arises, though, when fitting a set of a models to a single dataset. Now that the denominator of \\(R^2\\) is no longer fixed, we can no longer interpret an increase in \\(R^2\\) as a improved fit to a fixed target.\n\nI‚Äôm glad I read the paper!\nAnyways, height_mod has an \\(R^2\\) of 0, as it should as an intercept only model.\n\nbayes_R2(height_mod)\n\n   Estimate Est.Error Q2.5 Q97.5\nR2        0         0    0     0\n\n\nI had to think for a second about how this made sense, but as an intercept only model, the predicted values for the data will be just a single number, equal to the intercept \\(\\mu\\).\n\npredictions(\n  height_mod \n) |&gt; \n  posterior_draws() -&gt;\n  height_fitted\n\nheight_fitted |&gt; \n  filter(drawid == \"1\") |&gt; \n  slice(1:6) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nI computed \\(R^2\\) by hand here. I‚Äôm a bit lost why the variance of the residuals is identical for every draw‚Ä¶\n\nheight_fitted |&gt; \n  mutate(resid = height - draw) |&gt; \n  group_by(drawid) |&gt; \n  summarise(\n    var_fit = var(draw),\n    var_resid = var(resid)\n  ) |&gt; \n  mutate(bayesr2 = var_fit/(var_fit+var_resid)) |&gt; \n  slice(1:6) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe \\(R^2\\) for the height~weight model is about 0.57\n\nbayes_R2(height_weight_mod)\n\n   Estimate  Est.Error      Q2.5    Q97.5\nR2 0.572246 0.02644878 0.5143232 0.618783\n\n\nLet‚Äôs try calculating that ‚Äúby hand‚Äù again.\n\npredictions(\n  height_weight_mod\n) |&gt; \n  posterior_draws() |&gt; \n  mutate(resid = height - draw) |&gt; \n  group_by(drawid) |&gt; \n  summarise(\n    var_fit = var(draw),\n    var_resid = var(resid)\n  ) |&gt; \n  mutate(bayesr2 = var_fit / (var_fit + var_resid)) |&gt; \n  mean_qi(bayesr2)\n\n# A tibble: 1 √ó 6\n  bayesr2 .lower .upper .width .point .interval\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1   0.572  0.514  0.619   0.95 mean   qi       \n\n\nCool. We can get this all from bayes_R2() also.\n\nbayes_R2(height_weight_mod, summary = F) |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(R2))+\n    stat_slab()+\n    scale_y_continuous(\n      expand = expansion(mult = 0)\n    )+\n    xlim(0,1)+\n    labs(\n      x = expression(R^2)\n    )+\n    theme_no_y()\n\n\n\n\nFigure¬†3: Estimate of Bayesian \\(R^2\\)\n\n\n\n\n\n\nloo\nOk‚Ä¶ Time to understand what loo() does, and what elpd means. Doing my best with Vehtari, Gelman, and Gabry (2016)\n\nelpd\n\nExpected Log Pointwise Predictive Density (we lost a ‚Äúp‚Äù somewhere).\n\n\nStarting with lpd (log pointwise predictive density). So \\(p(y_i|y)\\) is the probability of a data point \\(y_i\\) given the distribution of data \\(y\\). We log it, probably to keep things computable and addition based, and sum it up across every datapoint, \\(\\sum \\log p(y_i|y)\\). This is apparently equal to \\(\\sum \\log \\int p(y_i|\\theta)p(\\theta|y)d\\theta\\).\n\n\\(p(y_i|\\theta)\\) = the probability of each data point given the model\n\\(p(\\theta|y)\\) = the probability of the model given the data.\n\nOk, but \\(p(y_i | y)\\) is derived from probabilities over models that had seen \\(y_i\\). \\(p(y_i|y_{-i})\\) is the probability of data point \\(y_i\\) derived from a model that had not seen \\(y_i\\), a.k.a. ‚Äúleave one out‚Äù. ELPD is the summed up log probabilities across these leave-one-out models.\nAs best as I can tell, the rest of the paper is just about getting very clever about how to approximate \\(\\sum \\log p(y_i|y_{-i})\\) without needing to refit the model for each datapoint. It‚Äôs this cleverness that will sometimes result in a warning about ‚ÄúPareto k estimates‚Äù\nSo, without any further ado:\n\nloo(height_mod)\n\n\nComputed from 4000 by 251 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -874.6  8.9\np_loo         1.7  0.2\nlooic      1749.2 17.8\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nSo, if the leave-one-out probability of each data point was higher, the elpd_loo value would be closer to 0, aka exp(0)= 1.\n\nloo(height_weight_mod)\n\n\nComputed from 4000 by 251 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -768.9 12.9\np_loo         3.2  0.6\nlooic      1537.7 25.8\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nTo compare the two models:\n\nloo_compare(\n  loo(height_mod),\n  loo(height_weight_mod)\n)\n\n                  elpd_diff se_diff\nheight_weight_mod    0.0       0.0 \nheight_mod        -105.8      12.7 \n\n\nSo, the height-only model has a worse elpd. And we can be pretty sure it‚Äôs a worse elpd, because dividing it by the standard error of the difference is about -8, which according to the Stan discussion forums is a pretty big difference."
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#wrapping-it-into-a-report",
    "href": "posts/2023-06-14_09-reporting/index.html#wrapping-it-into-a-report",
    "title": "Reporting a linear model",
    "section": "Wrapping it into a report",
    "text": "Wrapping it into a report\nPosterior predictive checks of both models show considerable bimodality is not sufficiently captured by either the intercept-only model or the weight model.\n\n\nCode\nposterior_predict(height_mod) |&gt; \n  as.data.frame() |&gt; \n  mutate(.draw = row_number()) |&gt; \n  slice(1:100) |&gt; \n  pivot_longer(-.draw) |&gt; \n  mutate(model = \"height ~ 1\")-&gt;\n  height_pp\n\nposterior_predict(height_weight_mod) |&gt; \n  as.data.frame() |&gt; \n  mutate(.draw = row_number()) |&gt; \n  slice(1:100) |&gt; \n  pivot_longer(-.draw) |&gt; \n  mutate(model = \"height ~ weight\")-&gt;\n  height_weight_pp\n  \nheight_to_mod |&gt; \n  mutate(model = NULL)-&gt;\n  orig\n\nbind_rows(height_pp, height_weight_pp) |&gt; \n  ggplot(aes(value))+\n    stat_density(\n      aes(color = \"yrep\", group = .draw),\n      fill = NA,\n      position = \"identity\",\n      geom = \"line\",\n      alpha = 0.1\n    )+\n    stat_density(\n      data = orig,\n      aes(x = height, color = \"y\"),\n      fill = NA,\n      geom = \"line\",\n      linewidth = 1\n    )+\n    scale_y_continuous(expand = expansion(mult = 0))+\n    labs(\n      color = NULL\n    )+\n    facet_wrap(~model)+\n    theme_no_y()\n\n\n\n\n\nFigure¬†4: Posterior predictive checks for the two models\n\n\n\n\n\n\nCode\nbayes_R2(height_weight_mod, summary = F) |&gt; \n  as_tibble() |&gt; \n  mean_hdci(.width = 0.95) -&gt;\n  mod_r2\n\n\nThe intercept-only model necessarily has an \\(R^2\\) of 0. Mean Bayesian \\(R^2\\) for the weight model is 0.57 (95% highest density interval of [0.52, 0.62]).\nTable¬†1 displays model comparisons using Leave-One-Out Expected Log Pointwise Predictive Distribution (ELPD) (Vehtari, Gelman, and Gabry 2016).\n\n\nCode\nloo_compare(\n  loo(height_mod),\n  loo(height_weight_mod)\n) |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column() |&gt; \n  mutate(\n    model = case_when(\n      rowname == \"height_mod\" ~ \"height ~ 1\",\n      rowname == \"height_weight_mod\" ~ \"height ~ weight\"\n    )\n   ) |&gt; \n  select(model, elpd_diff, se_diff) |&gt; \n  mutate(ratio = elpd_diff/se_diff) |&gt; \n  gt() |&gt; \n    fmt_number() |&gt; \n    sub_missing() |&gt; \n    cols_label(\n      elpd_diff = \"ELPD difference\",\n      se_diff = \"difference SE\",\n      ratio  = \"diff/se\"\n    )\n\n\n\n\n\n\n\n  \n    \n    \n      model\n      ELPD difference\n      difference SE\n      diff/se\n    \n  \n  \n    height ~ weight\n0.00\n0.00\n‚Äî\n    height ~ 1\n‚àí105.75\n12.66\n‚àí8.35\n  \n  \n  \n\nTable¬†1:  Leave-One-Out Expected Log Pointwise Predictive Distribution\ncomparsion of the two models. ELPD difference contain the difference\nfrom the largest LOO ELPD."
  },
  {
    "objectID": "posts/2023-05-09_00-setup/index.html",
    "href": "posts/2023-05-09_00-setup/index.html",
    "title": "Setup",
    "section": "",
    "text": "I‚Äôve set up the blog using the default quarto blog template in RStudio, also initializing a git repo and renv.\n\nrenv::install(c(\"tidyverse\", \"brms\"))\nrenv::install(c(\"coda\", \"mvtnorm\", \"dagitty\"))\n\nThe preface wants to install the book package with devtooks::install_github(), but I‚Äôm pretty sure that‚Äôs been superseded with remotes::install_github(), and renv::install().\n\ninstall.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nrenv::install(\"rmcelreath/rethinking\")"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html",
    "href": "posts/2023-06-05_07-linear-models-1/index.html",
    "title": "Linear Models: Part 1",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#loading",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#loading",
    "title": "Linear Models: Part 1",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#simulating-a-galton-board",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#simulating-a-galton-board",
    "title": "Linear Models: Part 1",
    "section": "Simulating a Galton Board",
    "text": "Simulating a Galton Board\n\n‚ÄúSuppose you and a thousand of your closest friends line up in the halfway line of a soccer field.‚Äù\n\nOk, so the N is 1+1,000 (‚Äúyou and 1000 of your closest friends‚Äù). Apparently a soccer field is 360 feet long, and an average stride length is something like 2.3 feet.\n\n(360/2)/2.3\n\n[1] 78.26087\n\n\nWe can get in 78 steps from the halfway line to the end of the field.\n\nset.seed(500)\n\nexpand_grid(\n  person = 1:1001,\n  step = 1:78\n) |&gt; \n  mutate(\n    flip = sample(\n      c(-1, 1), \n      size = n(), \n      replace = T\n    )\n  ) |&gt; \n  mutate(\n    .by = person,\n    position = cumsum(flip)\n  ) -&gt;\n  galton_board\n\n\ngalton_board |&gt; \n  mutate(\n    .by = c(step, position),\n    n = n()\n  ) -&gt;\n  galton_board\n\n\ngalton_board |&gt; \n  ggplot(\n    aes(step, position)\n  )+\n    geom_line(\n      aes(group = person, color = n)\n    ) +\n  scale_x_reverse()+\n  khroma::scale_color_bilbao(\n      guide = \"none\"\n    )+  \n  coord_flip()\n\n\n\n\nIt‚Äôs hard to visualize well with the completely overlapping points. I‚Äôll plot histograms for very 10th step.\n\ngalton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) |&gt; \n  ggplot(aes(position, factor(step)))+\n    stat_histinterval(\n      breaks = breaks_fixed(width = 2),\n      aes(fill = after_stat(pdf))\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    scale_y_discrete(\n      limits = factor(seq(70, 10, by = -10))\n    )"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#infinitesimal-galton-board",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#infinitesimal-galton-board",
    "title": "Linear Models: Part 1",
    "section": "Infinitesimal Galton Board",
    "text": "Infinitesimal Galton Board\nSame as before, but now instead of flipping a coin for -1 and 1, values are sampled from \\(\\mathcal{U}(-1,1)\\).\n\nexpand_grid(\n  person = 1:1001,\n  step = 1:78\n) |&gt; \n  mutate(\n    flip = runif(\n      n(),\n      -1,\n      1\n    )\n  ) |&gt; \n  mutate(\n    .by = person,\n    position = cumsum(flip)\n  ) -&gt;\n  inf_galton_board\n\n\ninf_galton_board |&gt; \n  ggplot(aes(step, position))+\n    geom_line(\n      aes(group = person),\n      alpha = 0.05\n    )+\n  scale_x_reverse()+\n  coord_flip()\n\n\n\n\n\ninf_galton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) |&gt; \n  ggplot(aes(position, factor(step)))+\n    stat_slabinterval(\n      aes(fill = after_stat(pdf)), \n      fill_type = \"gradient\"\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    scale_y_discrete(\n      limits = factor(seq(70, 10, by = -10))\n    )\n\n\n\n\nNice.\nI‚Äôm not 100% sure how to get a normal density estimate superimposed in that same plot. So I‚Äôll fake it instead.\n\ninf_galton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) -&gt;\n  ten_steps\n\nten_steps |&gt; \n1  summarise(\n    .by = step,\n    mean = mean(position),\n    sd = sd(position)\n  ) |&gt; \n  nest(\n    .by = step\n  ) |&gt; \n  mutate(\n2    dist = map(\n      data,\n      ~tibble(\n        position = seq(-20, 20, length = 100),\n        dens = dnorm(\n          position, \n          mean = .x$mean, \n          sd = .x$sd\n        )\n      )\n    )\n  ) |&gt; \n  unnest(dist) |&gt; \n3  mutate(\n    dens_norm = dens/max(dens)\n  )-&gt;\n  distributions\n\n\n1\n\nCalculating the distribution parameters for each step grouping.\n\n2\n\nMapping over the distribution parameters to get density values in a tibble.\n\n3\n\nFor plotting over the stat_slab() output, normalizing the density to max out at 1.\n\n\n\n\n\nten_steps |&gt; \n  ggplot(aes(position))+\n    stat_slabinterval(\n      aes(fill = after_stat(pdf)), \n      fill_type = \"gradient\"\n    )+\n    geom_line(\n      data = distributions,\n      aes(y = dens_norm)\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    facet_wrap(\n      ~step, labeller = label_both\n    )+\n    theme_no_y()\n\n\n\n\n\nComparing parameters\nFor my own interest, I wonder how much discrete sampling from -1, 1 vs the uniform distribution affects the \\(\\sigma\\).\n\ngalton_board |&gt; \n  summarise(\n    .by = step,\n    pos_sd = sd(position)\n  ) |&gt; \n  mutate(\n    sampling = \"discrete\"\n  ) -&gt;\n  galton_sd\n\ninf_galton_board |&gt; \n  summarise(\n    .by = step,\n    pos_sd = sd(position)\n  ) |&gt; \n  mutate(\n    sampling = \"uniform\"\n  )-&gt;\n  inf_galton_sd\n\n\nbind_rows(\n  galton_sd, \n  inf_galton_sd\n) |&gt; \n  ggplot(aes(step, pos_sd))+\n    geom_line(\n      aes(color = sampling),\n      linewidth = 1\n    )+\n    expand_limits(y = 0)+\n    labs(\n      y = expression(sigma)\n    )\n\n\n\n\nMessing around with a few obvious values of \\(x\\), in \\(\\mathcal{U}(-x,x)\\), I can‚Äôt tell what would approximate the discrete sampling. 2 is too large, and 1.5 is too small. The answer is probably some horror like \\(\\frac{\\pi}{e}\\).1"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#model-diagrams",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#model-diagrams",
    "title": "Linear Models: Part 1",
    "section": "Model Diagrams",
    "text": "Model Diagrams\nHere‚Äôs the model described in the text.\n\\[\ny_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\beta x_i\n\\]\n\\[\n\\beta \\sim \\mathcal{N}(0, 10)\n\\]\n\\[\n\\sigma \\sim \\text{Exponential}(1)\n\\]\nHe also defines a sampling distribution over \\(x_1\\), but idk if that‚Äôs right. Here‚Äôs my attempt at converting that into a mermaid diagram.\n\n\n\n\nflowchart RL\n  normal1[\"N(Œº·µ¢, œÉ)\"] --&gt;|\"~\"| y[\"y·µ¢\"]\n  beta[\"Œ≤\"] --&gt; mult1([\"√ó\"])\n  x[x·µ¢] --&gt; mult1\n  mult1 --&gt; mu1[Œº·µ¢]\n  mu1 --&gt; normal1\n  \n  exp1[\"Exp(1)\"] --\"~\"--&gt; sigma1[œÉ]\n  sigma1 --&gt; normal1\n  \n  normal2[\"N(0,10)\"] --\"~\"--&gt; beta\n\n\n\n\n\nIt‚Äôs ok. No quite a Kruschke diagram.\n\nAnother example.\nLet me try to write out the diagram for something like y ~ x + (1|z).\n\\[\ny \\sim(\\mu_i, \\sigma_0)\n\\]\n\\[\n\\mu_i = \\beta_0 + \\beta_1x_i + \\gamma_i\n\\]\n\\[\n\\beta_0 \\sim \\mathcal{N}(0,10)\n\\]\n\\[\n\\beta_2 \\sim \\mathcal{N}(0,2)\n\\]\n\\[\n\\gamma_i = \\Gamma_{z_i}\n\\]\n\\[\n\\Gamma_j \\sim \\mathcal{N}(0,\\sigma_1)\n\\]\n\\[\n\\sigma_0 \\sim \\text{Exponential}(1)\n\\]\n\\[\n\\sigma_1 \\sim \\text{Exponential}(1)\n\\]\n\nGeeze, idk. That double subscript feels rough, and I don‚Äôt know the convention for describing the random effects.\n\n\n\n\nflowchart TD\n  normal1[\"N(Œº·µ¢, œÉ‚ÇÄ)\"] --\"~\"--&gt; y[y·µ¢]\n  beta0[\"Œ≤‚ÇÄ\"] --&gt; plus([\"+\"])\n  beta1[\"Œ≤‚ÇÅ\"] --&gt; plus\n  gamma[\"Œ≥·µ¢\"] --&gt; plus\n  plus --&gt; mu[\"Œº·µ¢\"]\n  mu --&gt; normal1\n  normal2[\"N(0,10)\"] --\"~\"--&gt; beta0\n  normal3[\"N(0,2)\"] --\"~\"--&gt; beta1\n  Gamma[\"Œì[z·µ¢]\"] --&gt; gamma\n  normal4[\"N(0, œÉ‚ÇÅ)\"] --\"~\"--&gt; Gamma\n  exponent0[\"Exp(1)\"] --\"~\"--&gt; sigma0[\"œÉ‚ÇÄ\"]\n  sigma0 --&gt; normal1\n  exponent1[\"Exp(1)\"] --\"~\"--&gt; sigma1[\"œÉ‚ÇÅ\"]\n  sigma1 --&gt; normal4\n  \n\n\n\n\n\nYeah, this is too tall. Will have to think about this. The Krushke style diagram is the most compressed version imo."
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#footnotes",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#footnotes",
    "title": "Linear Models: Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot literally \\(\\frac{\\pi}{e}\\) though, cause that‚Äôs too small at 1.156‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html",
    "title": "Garden of Forking paths part 2",
    "section": "",
    "text": "Listening\nrenv::install(\"khroma\")\nlibrary(tidyverse)\nlibrary(khroma)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#a-nicer-table-version.",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#a-nicer-table-version.",
    "title": "Garden of Forking paths part 2",
    "section": "A nicer table version.",
    "text": "A nicer table version.\nI‚Äôd like to re-represent the Bayesian Update in a nicer GT table. Some options are\n\nPlotting extensions from {gtExtras}\nEmojis\n\n\nrenv::install(\"gtExtras\")\nrenv::install(\"svglite\")\nrenv::install(\"emoji\")\n\n\nlibrary(gtExtras)\nlibrary(emoji)\n\nFirst, trying the ‚Äúwin/losses‚Äù column plot from {gtExtra} to illustrate the blue vs white marbles.\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(1, blue_marbs), \n                     rep(0, white_marbs)))\n  ) -&gt; \n  marbles_wl\n\nThe cell background will have to be off-white for the white ticks to show\n\nmarbles_wl |&gt; \n  gt() |&gt; \n  gt_plt_winloss(marbles, palette = c(\"blue\", \"white\", \"grey\")) |&gt; \n  tab_style(style = cell_fill(color = \"antiquewhite\"), \n            locations = cells_body())\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\n          \n    1\n3\n          \n    2\n2\n          \n    3\n1\n          \n    4\n0\n          \n  \n  \n  \n\nTable¬†1:  Representing marble compositions with ‚Äòwin-loss‚Äô plots \n\n\n\nI‚Äôm not overwhelmed by the result. I‚Äôll try emojis instead.\n\nblue_marb &lt;- emojis |&gt; \n  filter(str_detect(name, \"blue\"), \n         str_detect(name, \"circle\")) |&gt; \n  pull(emoji)\n\nblue_marb\n\n[1] \"üîµ\"\n\n\n\nwhite_marb &lt;- emojis |&gt; \n  filter(str_detect(name, \"white\"), \n         str_detect(name, \"circle\")) |&gt; \n  pull(emoji)\n\nwhite_marb\n\n[1] \"‚ö™\"\n\n\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(blue_marb, blue_marbs), \n                     rep(white_marb, white_marbs)))\n  ) -&gt; \n  marbles_emoji\n\n\nmarbles_emoji |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\n‚ö™, ‚ö™, ‚ö™, ‚ö™\n    1\n3\nüîµ, ‚ö™, ‚ö™, ‚ö™\n    2\n2\nüîµ, üîµ, ‚ö™, ‚ö™\n    3\n1\nüîµ, üîµ, üîµ, ‚ö™\n    4\n0\nüîµ, üîµ, üîµ, üîµ\n  \n  \n  \n\nTable¬†2:  Representing marble compositions with emoji \n\n\n\nUpdate: As it turns out, getting tables with emoji and the plots with gt_plt_*() do not play nice with LaTeX. For now, I‚Äôm saving the tables to png with gtsave() just for the pdf output.\nYes, this is it.\n\nRerunning the sampling\nI‚Äôll re-run the sampling from the previous post.\n\nsampling_df &lt;- function(marbles, \n                        n = 1000, \n                        size = 3, \n                        pattern = c(blue_marb, white_marb, blue_marb)){\n  sampling_tibble &lt;- tibble(samp = 1:n)   \n  sampling_tibble |&gt; \n    mutate(\n      chosen = map(samp, \n                   ~sample(marbles, \n                           size = 3, \n                           replace = T)),\n      match = map_lgl(chosen, \n                      ~all(.x == pattern))                 \n    ) |&gt; \n    summarise(prop_match = mean(match))-&gt;                         \n    sampling_tibble\n  return(sampling_tibble)\n}\n\n\nmarbles_emoji |&gt; \n ungroup() |&gt; \n  mutate(\n    prob = map(marbles, ~sampling_df(.x, n = 10000))\n  ) |&gt; \n  unnest(prob) |&gt; \n  mutate(norm_probs = prop_match/sum(prop_match))-&gt;\n  marble_probs\n\nI want to label the column of probabilities with the key sequence.\n\nkey_seq &lt;- str_glue(\"{blue_marb}, {white_marb}, {blue_marb}\")\n\ngtExtras::gt_plt_bar_pct() will plot a bar chart within the table.\n\nmarble_probs |&gt; \n  select(marbles, norm_probs) |&gt; \n  mutate(norm_probs = norm_probs * 100) |&gt; \n  gt() |&gt; \n  cols_label(\n    norm_probs = str_glue(\"p(marbles | {key_seq})\")\n  ) |&gt; \n  gt_plt_bar_pct(norm_probs, \n                 scaled = T, \n                 fill = \"steelblue\")\n\n\n\n\n\n\n  \n    \n    \n      marbles\n      p(marbles | üîµ, ‚ö™, üîµ)\n    \n  \n  \n    ‚ö™, ‚ö™, ‚ö™, ‚ö™\n\n    üîµ, ‚ö™, ‚ö™, ‚ö™\n\n    üîµ, üîµ, ‚ö™, ‚ö™\n\n    üîµ, üîµ, üîµ, ‚ö™\n\n    üîµ, üîµ, üîµ, üîµ\n\n  \n  \n  \n\nTable¬†3:  Probability of each marble composition given (üîµ, ‚ö™Ô∏è, üîµ) samples\nwith replacement. \n\n\n\nThere we go!\n\n\nWith the Bayesian Update\n\nmarble_probs |&gt; \n  mutate(new_prob = blue_marbs/sum(blue_marbs),\n         multiplied = norm_probs * new_prob,\n         norm_new = multiplied/sum(multiplied)) |&gt; \n  select(marbles, norm_probs, norm_new) |&gt; \n  mutate(norm_probs = norm_probs * 100,\n         norm_new = norm_new * 100) |&gt; \n  gt() |&gt; \n  cols_label(\n    norm_probs = str_glue(\"p(marbles | {key_seq})\"),\n    norm_new = str_glue(\"after {blue_marb}\")\n  ) |&gt; \n  gt_plt_bar_pct(norm_probs, \n                 scaled = T, \n                 fill = \"steelblue\") |&gt; \n  gt_plt_bar_pct(norm_new, \n                 scaled = T, \n                 fill = \"steelblue\") |&gt; \n  cols_width(2 ~ px(200),\n             3 ~ px(200))\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n    \n      marbles\n      p(marbles | üîµ, ‚ö™, üîµ)\n      after üîµ\n    \n  \n  \n    ‚ö™, ‚ö™, ‚ö™, ‚ö™\n\n\n    üîµ, ‚ö™, ‚ö™, ‚ö™\n\n\n    üîµ, üîµ, ‚ö™, ‚ö™\n\n\n    üîµ, üîµ, üîµ, ‚ö™\n\n\n    üîµ, üîµ, üîµ, üîµ\n\n\n  \n  \n  \n\nTable¬†4:  Probability of each marble composition given an additional (üîµ)\nsample"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#bayesian-updating",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#bayesian-updating",
    "title": "Garden of Forking paths part 2",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\nI‚Äôll try to illustrate Baysian updating with an animated plotly plot.\n\nrenv::install(\"plotly\")\nrenv::install(\"slider\")\n\n\nlibrary(plotly)\nlibrary(slider)\n\nI know enough to know that the distribution that‚Äôs being updated is the beta. (Apparently is the binomial. Still a but lost on their distinction!) So I‚Äôll get the density for each update.\n\nplot(\n  seq(0,1, length =100),\n  dbeta(seq(0,1, length =100), 1, 1),\n  type = 'l'\n)\n\n\n\n\n\nwater_land_sequence &lt;- c(\"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\n\nI‚Äôll use slider::slide() to generate a data frame of sample updates. I‚Äôll need a function that takes a sequence of W and L and converts them into counts.\n\nw_l_count &lt;- function(x){\n  tibble(\n    water = sum(x == \"W\"),\n    land = sum(x == \"L\")\n  )\n}\n\n\nslide(water_land_sequence, \n      .f = w_l_count, \n      .before = Inf,\n      .after = 0) |&gt; \n  bind_rows() |&gt; \n  mutate(seq = row_number()) |&gt; \n  bind_rows(\n    tibble(\n      water = 0,\n      land = 0, \n      seq = 0\n    )\n  ) |&gt; \n  arrange(seq) -&gt;\n  sequence_counts\n\n\nsequence_counts |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      water\n      land\n      seq\n    \n  \n  \n    0\n0\n0\n    1\n0\n1\n    1\n1\n2\n    2\n1\n3\n    3\n1\n4\n    3\n2\n5\n    4\n2\n6\n    4\n3\n7\n    5\n3\n8\n  \n  \n  \n\nTable¬†5:  Table of water, land count updates \n\n\n\nNow to get the densities.\n\nsequence_counts |&gt; \n  rowwise() |&gt; \n  mutate(\n    density = map2(\n      water, land, ~tibble(\n        prop = seq(0.0001, 0.9999, length = 100),\n        density_unstd = dbinom(water, size = water + land, prob = prop),\n        density = density_unstd/sum(density_unstd)\n        )\n    )\n  ) |&gt; \n  unnest(density)-&gt;\n  density_updates\n\n\ndensity_updates |&gt; \n  ggplot(aes(prop, density))+\n    geom_line(aes(group = seq, color = seq))\n\n\n\n\nFigure¬†1: beta distribution updates\n\n\n\n\nGood first step.\nI had to turn to the plotly book to get the animated lines correct https://plotly-r.com/animating-views.html.\n\nsequence_counts |&gt; \n  mutate(\n    annotation = str_glue(\"W:{water}, L:{land}\")\n  ) -&gt; \n  wl_annotate\n\n\ndensity_updates |&gt; \n  plot_ly() |&gt; \n  add_lines(\n    x = ~prop,\n    y = ~density,\n    frame = ~seq,\n    line = list(simplify = F, width = 3)\n  ) |&gt; \n  add_text(\n    data = wl_annotate,\n    text = ~annotation,\n    frame = ~seq,\n    x = 0.1,\n    y = 0.025,\n    textfont = list(size = 20)\n  ) |&gt;\n  layout(\n    showlegend = F\n  )\n\n\n\n\nFigure¬†2: Animated Bayesian Updating\n\n\n\nUpdate: I started making this just for the pdf, which can‚Äôt have the animation, but it‚Äôs actually kind of nice enough to include in the html.\n\ndensity_updates |&gt; \n  group_by(prop) |&gt; \n  arrange(seq) |&gt; \n  mutate(\n    prev_density = lag(density),\n    facet_lab = str_glue(\"W:{water}, L{land}\")\n  ) |&gt; \n  ggplot(aes(prop, density))+\n    geom_area(aes(y = prev_density), linetype = 2, alpha = 0.2)+\n    geom_area(alpha = 0.6, fill = \"steelblue\")+\n    scale_y_continuous(expand = expansion(mult = c(0,0)))+\n    facet_wrap(~facet_lab)+\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 8),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†3: Static figure of Bayesian Updating"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#on-priors",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#on-priors",
    "title": "Garden of Forking paths part 2",
    "section": "On priors",
    "text": "On priors\n\nThe fact that statistical inference uses mathematics does not imply that there is only one reasonable or useful way to conduct an analysis."
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#grid-approximation",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#grid-approximation",
    "title": "Garden of Forking paths part 2",
    "section": "Grid approximation",
    "text": "Grid approximation\nOk, I‚Äôll do one grid approximation for the hell of it.\n‚Ä¶\nGot distracted and went down a rabbit hole on the beta vs binomial distributions.\n\ntibble(\n  prob = seq(0.0001, 0.9999, length = 50), \n  prior_unstd = case_when(\n    prob &lt; 0.5 ~ 0,\n    .default = 1\n  ),\n  prior_std = prior_unstd/sum(prior_unstd),\n  likelihood_binom = dbinom(6, size = 9, prob = prob),\n  l_binom_std = likelihood_binom/sum(likelihood_binom),\n  likelihood_beta = dbeta(prob, 6, 9-6),\n  l_beta_std = likelihood_beta/sum(likelihood_beta)\n) |&gt; \n  ggplot(aes(prob))+\n    geom_point(aes(y = l_binom_std, color = \"binom\"))+\n    geom_point(aes(y = l_beta_std, color = \"beta\"))+\n    scale_color_bright()+\n    labs(\n      x = \"probability\",\n      color = \"distribution\",\n      y = NULL\n    )\n\n\n\n\nFigure¬†4: Comparing normalized densities for beta(6,3) and binom(6, 9, p)\n\n\n\n\nGlad I did this. I guess\n\ndbinom \\(\\propto P(O, S | p)\\)\ndbeta \\(\\propto P(p|O,S)\\)\n\nI guess I‚Äôd want to see dbinom plotted out with O on the x axis?\n\ntibble(\n  probability = seq(0.0001, 0.9999, length = 10)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    densities = map(\n      probability,\n      ~tibble(obs = 0:9, \n              density = dbinom(obs, size = 9, prob = .x))\n    )\n  ) |&gt; \n  unnest(densities) -&gt;\n  binomial_densities \n\nbinomial_densities |&gt; \n  ggplot(aes(obs, density, color = probability))+\n    geom_point()+\n    geom_line(aes(group = probability)) +\n    geom_rect(\n      color = \"red\",\n      fill = NA,\n      xmin = 5.5,\n      xmax = 6.5,\n      ymin = 0,\n      ymax = 1\n    )+\n    scale_color_batlow()\n\n\n\n\nFigure¬†5: Binomial distributions of various successes out of 9 trials, for various p\n\n\n\n\nWhat we‚Äôre plotting out is what‚Äôs in the red box, flipped on its side.\n\nbinomial_densities |&gt; \n  filter(obs == 6) |&gt; \n  ggplot(aes(probability, density))+\n    geom_line()+\n    geom_point(aes(color = probability))+\n    scale_color_batlow()\n\n\n\n\nFigure¬†6: Normalized binomial density for 6 successes out of 9 trials for various p."
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#update",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#update",
    "title": "Garden of Forking paths part 2",
    "section": "Update!",
    "text": "Update!\nThanks TJ!\n\n\ni forgot how i know this but they are the same if you plug in likelihood_beta = dbeta(prob, 1 + 6, 1 + 9-6),\n\n‚Äî tj mahr üççüçï (@tjmahr) May 10, 2023\n\n\n\ntibble(\n  prob = seq(0.0001, 0.9999, length = 50), \n  prior_unstd = case_when(\n    prob &lt; 0.5 ~ 0,\n    .default = 1\n  ),\n  prior_std = prior_unstd/sum(prior_unstd),\n  likelihood_binom = dbinom(6, size = 9, prob = prob),\n  l_binom_std = likelihood_binom/sum(likelihood_binom),\n  likelihood_beta = dbeta(prob, (6+1), (9-6)+1),\n  l_beta_std = likelihood_beta/sum(likelihood_beta)\n) |&gt; \n  ggplot(aes(prob))+\n    geom_point(aes(y = l_binom_std, color = \"binom\", size = \"binom\"))+\n    geom_point(aes(y = l_beta_std, color = \"beta\",size = \"beta\"))+\n    scale_color_bright()+\n    labs(\n      x = \"probability\",\n      color = \"distribution\",\n      size = \"distribution\",\n      y = NULL\n    )\n\n\n\n\nFigure¬†7: Comparing normalized densities for beta(6+1,3+1) and binom(6, 9, p)"
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html",
    "title": "02 Small Worlds and Large Worlds",
    "section": "",
    "text": "listening\nIn the analogy, models are ‚ÄúSmall‚Äù, self-contained worlds."
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html#garden-of-forking-paths.",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html#garden-of-forking-paths.",
    "title": "02 Small Worlds and Large Worlds",
    "section": "Garden of forking paths.",
    "text": "Garden of forking paths.\nI was thinking of working out the probabilities by doing random sampling‚Ä¶\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)\nsource(here(\"_defaults.R\"))\n\nGenerating the marble dataframe\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(\"blue\", blue_marbs), rep(\"white\", white_marbs)))\n  ) -&gt; \n  marbles\n\n\nmarbles |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\nwhite, white, white, white\n    1\n3\nblue, white, white, white\n    2\n2\nblue, blue, white, white\n    3\n1\nblue, blue, blue, white\n    4\n0\nblue, blue, blue, blue\n  \n  \n  \n\nTable¬†1:  The marble sampling distributions \n\n\n\nIn retrospect, I‚Äôm glad I did this, because I thought we were sampling without replacement.\nHere‚Äôs a function that will repeatedly sample from a set of marbles, and compare the result to a reference group.\n\nsampling_df &lt;- function(marbles, n = 1000, size = 3, pattern = c(\"blue\", \"white\", \"blue\")){\n1  sampling_tibble &lt;- tibble(samp = 1:n)\n  sampling_tibble |&gt; \n    mutate(\n2      chosen = map(samp, ~sample(marbles, size = 3, replace = T)),\n3      match = map_lgl(chosen, ~all(.x == pattern))\n    ) |&gt; \n4    summarise(prop_match = mean(match))-&gt;\n    sampling_tibble\n  return(sampling_tibble)\n}\n\n\n1\n\nI‚Äôll capture everything within a tibble.\n\n2\n\nRowwise, sample from marbles with replacement.\n\n3\n\nReturn T or F if the sequence matches the pattern exactly.\n\n4\n\nThe mean() of the T, F column to get the proportion that match.\n\n\n\n\n\nsampling_df(\n  marbles = marbles$marbles[[4]],\n  n = 5000\n) \n\n# A tibble: 1 √ó 1\n  prop_match\n       &lt;dbl&gt;\n1      0.140\n\n\n\nmarbles |&gt; \n ungroup() |&gt; \n  mutate(\n    prob = map(marbles, ~sampling_df(.x, n = 10000))\n  ) |&gt; \n  unnest(prob) |&gt; \n  mutate(norm_probs = prop_match/sum(prop_match))-&gt;\n  marble_probs\n\n\nmarble_probs |&gt; \n  ggplot(aes(blue_marbs, norm_probs))+\n    geom_col(fill = \"steelblue4\")+\n    labs(\n      title = \"blue, white, blue\",\n      x = \"# of blue marbles\",\n      y = \"probability\"\n    ) + \n  ylim(0,1)-&gt;probs1\nprobs1\n\n\n\n\nFigure¬†1: Probability of each composition of marbles"
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html#updating-probabilities",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html#updating-probabilities",
    "title": "02 Small Worlds and Large Worlds",
    "section": "Updating probabilities",
    "text": "Updating probabilities\nWhat if we draw one more blue\n\nmarble_probs |&gt; \n  mutate(new_obs_prob = blue_marbs / sum(blue_marbs),\n         posterior_prob = norm_probs * new_obs_prob,\n         posterior_norm = posterior_prob/sum(posterior_prob))-&gt;\n  marble_probs\n\n\nmarble_probs |&gt; \n  ggplot(aes(blue_marbs, posterior_norm))+\n    geom_col(fill = \"steelblue4\")+\n    ylim(0,1)+\n      labs(\n      title = \"probability update after blue\",\n      x = \"# of blue marbles\",\n      y = \"probability\"\n    ) -&gt;\n  probs2\n\nprobs1 | probs2\n\n\n\n\nFigure¬†2: Bayesian update"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html",
    "href": "posts/2023-06-02_05-sampling2/index.html",
    "title": "Sampling Summaries",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#loading",
    "href": "posts/2023-06-02_05-sampling2/index.html#loading",
    "title": "Sampling Summaries",
    "section": "Loading",
    "text": "Loading\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#setting-up-the-grid-samples",
    "href": "posts/2023-06-02_05-sampling2/index.html#setting-up-the-grid-samples",
    "title": "Sampling Summaries",
    "section": "Setting up the grid samples",
    "text": "Setting up the grid samples\n\ntibble(\n  p = seq(0, 1, length = 1000),\n  dens = dbeta(p, 6+1, 3+1),\n  posterior = dens/sum(dens)\n) -&gt;\n  posterior_grid\n\n\nposterior_grid |&gt; \n  ggplot(aes(p, posterior))+\n    geom_area(fill = ptol_blue, color = \"black\")\n\n\n\n\n\nSampling from the posterior\n\nposterior_grid |&gt; \n  sample_n(\n    size = 1e4,\n    replace = T,\n    weight = posterior\n  )-&gt;\n  posterior_samples\n\nThis isn‚Äôt MCMC sampling, but I‚Äôll plot it as a line just for consistency for how MCMC chains look.\n\nposterior_samples |&gt; \n  mutate(\n    sample = row_number()\n  ) |&gt; \n  ggplot(aes(sample, p))+\n    geom_line()\n\n\n\n\nComparing to the sampling to the original density function.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_density(\n      fill = ptol_blue\n      ) +\n    geom_line(\n      data = posterior_grid,\n      aes(y = dens),\n      color = ptol_red,\n      linewidth = 1\n    )"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#quantiles",
    "href": "posts/2023-06-02_05-sampling2/index.html#quantiles",
    "title": "Sampling Summaries",
    "section": "Quantiles",
    "text": "Quantiles\nFirst manually\n\nposterior_samples |&gt; \n  reframe(\n    lowhi = quantile(p, probs = c(0.25, 0.75))\n  ) |&gt; \n  pull(lowhi)-&gt;\n  fifty_quantile\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    ggdist::stat_slab(\n      color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n    labs(\n      fill = \"fifty\",\n      y = NULL\n    )+\n    scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n    theme(\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nI think I‚Äôll create a shortcut theme for having no y axis.\n\ntheme_no_y &lt;- function(){\n  theme(\n      axis.text.y = element_blank(),\n      axis.title.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n}\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_slab() +\n    theme_no_y()\n\n\n\n\n\nTidybayes functions\nI‚Äôm not 100% sure how all of the tidybayes functions work.\n\nposterior_samples |&gt; \n  summarise(\n    median_qi(p, .width = 0.5)\n  )\n\n# A tibble: 1 √ó 6\n      y  ymin  ymax .width .point .interval\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.644 0.542 0.738    0.5 median qi       \n\n\n\nposterior_samples |&gt; \n  reframe(\n    quantile  = quantile(p, probs = c(0.25, 0.5, 0.75))\n  )\n\n# A tibble: 3 √ó 1\n  quantile\n     &lt;dbl&gt;\n1    0.542\n2    0.644\n3    0.738\n\n\nOk, *_qi() returns the quantile interval.\nI‚Äôd like to make the plot according to the statistics calculated by stat_halfeye(), but can‚Äôt seem to get it to work.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      aes(\n        fill = after_stat(x &gt; xmin)\n      )\n    )\n\n\n\n\nI‚Äôll just do the same filling I did before.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_qi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()-&gt;\n  fifty_qi\nfifty_qi\n\n\n\n\n\n\nHPDI\nLemme try hpdi now.\n\nposterior_samples |&gt; \n  summarise(\n    mean_hdi(p, .width = 0.5)\n  )-&gt;\n  posterior_hdi\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_hdi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= posterior_hdi$ymin & \n            x &lt;=  posterior_hdi$ymax\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()-&gt;\n  fifty_hdi\nfifty_hdi\n\n\n\n\n\nfifty_qi/fifty_hdi\n\n\n\n\nThey‚Äôre very similar, but if I mix the qi fill and the hdi interval, they‚Äôre different.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_hdi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n         x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Workthrough Blog",
    "section": "",
    "text": "Reporting a linear model\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear models, part 2\n\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nPredictive Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models: Part 1\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nSampling Summaries\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting Sampling\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nGarden of Forking paths part 2\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\n01 Golem of Prague Chapter\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nSetup\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\n02 Small Worlds and Large Worlds\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog to document my progress in working through Richard McElreath‚Äôs Statistical Rethinking: A Bayesian Course with Examples in R and Stan, which I‚Äôll be supplementing with Solomon Kurz‚Äô bookdown project, Statistical Rethinking with brms, ggplot2, and the tidyverse."
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#next-time",
    "href": "posts/2023-06-14_09-reporting/index.html#next-time",
    "title": "Reporting a linear model",
    "section": "Next time:",
    "text": "Next time:\nWriting up a report on the actual parameters."
  }
]