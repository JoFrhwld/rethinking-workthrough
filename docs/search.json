[
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html",
    "title": "Predictive Distributions",
    "section": "",
    "text": "Listening\nOne last post to work through different predictive distributions given 6W and 3W."
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#loading",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#loading",
    "title": "Predictive Distributions",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(here)\nlibrary(ggblend)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#prior-predictive",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#prior-predictive",
    "title": "Predictive Distributions",
    "section": "Prior predictive",
    "text": "Prior predictive\nThe prior was a Uniform distribution between 0 and 1, or \\(\\mathcal{U}(0,1)\\). The outcome will also be uniform, but I‚Äôll go through all the steps for completeness.\n\ntibble(\n  p = runif(1e4)\n) -&gt;\n  prior_samp\n\nI‚Äôll make use of vectorization of rbinom() to generate possible samples with probabilities in prior_samp$p.\n\nset.seed(2023)\nprior_samp |&gt; \n  mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n1  count(pred_binom) |&gt;\n  mutate(prob = n/sum(n)) -&gt;\n  prior_pred\n\n\n1\n\nI‚Äôm going straight from generated samples to summarising for the distribution.\n\n\n\n\n\nprior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n1    theme_no_y()\n\n\n1\n\nI‚Äôve moved theme_no_y() into _defaults.R which I source at the top."
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#maximum-likelihood",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#maximum-likelihood",
    "title": "Predictive Distributions",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe maximum likelihood probability of W is \\(\\frac{6}{9}=0.6\\overline{6}\\). We can get predicted values for that probability from binomial distribution.\n\ntibble(\n  pred_binom = 0:9,\n  dens = dbinom(\n    pred_binom, \n    size = 9, \n    prob = 6/9\n  ),\n  prob = dens\n) -&gt; \n  ml_pred\n\nml_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#posterior-prediction",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#posterior-prediction",
    "title": "Predictive Distributions",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\nFor the posterior prediction, first we sample probabilities from the beta distribution\n\\[\np \\sim \\text{Beta}(W+1, L+1)\n\\]\n(a.k.a. dbeta())\nThen for each \\(p\\), we generate predictions from the binomial distribution.\n\\[\nY|p \\sim \\text{Bin}(9,p)\n\\]\nThis chapter has been doing grid sampling, but I‚Äôll just use the rbeta() function for simplicity.\n\ntibble(\n  p = rbeta(1e4, 6+1, 3+1)\n) -&gt;\n  posterior_samp\n\nThen, it‚Äôs the same operation as the prior predictive distribution.\n\nposterior_samp |&gt; \n   mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n  count(pred_binom) |&gt; \n  mutate(prob = n/sum(n)) -&gt;\n  posterior_pred\n\n\nposterior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col() +\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    ) +\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#comparisons",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#comparisons",
    "title": "Predictive Distributions",
    "section": "Comparisons",
    "text": "Comparisons\nNow, I want to compare the posterior predictive distribution to the maximum likelihood and the prior. This is one concept I had, which has the prior and the ML values as thinner bars within the posterior predictive bars.\n\nposterior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    geom_segment(\n      data = ml_pred,\n      aes(\n        x = pred_binom-0.1,\n        xend = pred_binom-0.1,\n        yend = 0,\n        color = \"maximum likelihood\"\n      ),\n      linewidth = 2\n    )+\n    geom_segment(\n      data = prior_pred,\n      aes(\n        x = pred_binom+0.1,\n        xend = pred_binom+0.1,\n        yend = 0,\n        color = \"prior\"\n      ),\n      linewidth = 2\n    )+  \n    scale_x_continuous(\n      breaks = seq(1, 9, length = 2)\n    )+\n    labs(\n      color = NULL,\n      x = \"predicted W\"\n    )+\n    theme_no_y()+\n    theme(\n      legend.position = \"top\"\n    )\n\n\n\n\nOverplotting them is also a possibility, and a good occasion to test out {ggblend}. I had some issues getting this to work with the available graphic devices & quarto.\n\n```{r}\n#| dev: \"png\"\n#| dev-args:\n#|   - type: \"cairo\"\nbind_rows(\n  posterior_pred |&gt; \n    mutate(pred = \"posterior\"),\n  ml_pred |&gt; \n    mutate(pred = \"maximum likelihood\"),\n  prior_pred |&gt; \n    mutate(pred = \"prior\")\n) |&gt; \n  ggplot(aes(pred_binom, prob, fill = pred))+\n    geom_area(position = \"identity\", alpha = 0.5) * \n      (blend(\"lighten\") + blend(\"darken\")) +\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n    theme_no_y()+\n    theme(\n      legend.position = \"top\"\n    )\n```"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#visualizing-the-posterior-prediction-process",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#visualizing-the-posterior-prediction-process",
    "title": "Predictive Distributions",
    "section": "Visualizing the posterior prediction process",
    "text": "Visualizing the posterior prediction process\n\nposterior_samp |&gt; \n  slice(1:4) |&gt; \n  mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n  arrange(\n    p\n  ) -&gt;\n  example_samp\n\n\nexample_samp |&gt; \n  mutate(\n    dens = dbeta(p, 6+1, 3+1),\n    n = row_number()\n  )-&gt;\n  example_samp\n\ntibble(\n  p = seq(0, 1, length = 100),\n  dens = dbeta(p, 6+1, 3+1)\n) |&gt; \n  ggplot(aes(p, dens))+\n    geom_area(fill = \"grey\") +\n    geom_segment(\n      data = example_samp,\n      aes(\n        xend = p,\n        yend = 0,\n        color = factor(n)\n      ),\n      linewidth = 1.5,\n      lineend = \"round\",\n      show.legend = F\n    )+\n    theme_no_y()+\n    labs(\n      title = \"Posterior\"\n    )-&gt;\n  posterior_plot\n\nThis involved messing around with {rlang} data masking that I still don‚Äôt quite follow. Both the p and pred arguments had to be (just once!) prefixed with !!, but not n.\n\nsample_plot_fun &lt;- function(p, pred, n){\n  this_scale &lt;- as.vector(khroma::color(\"bright\")(4))\n  tibble(\n    ws = 0:9,\n    dens = dbinom(ws, size = 9, prob = !!p)\n  ) |&gt; \n    ggplot(aes(ws, dens))+\n      geom_col(\n        aes(fill = ws == !!pred)\n      )+\n      scale_fill_manual(\n        values = c(\"grey\", this_scale[n]),\n        guide = \"none\"\n      ) +\n      scale_x_continuous(\n        breaks = seq(1,9, by = 2)\n      )+\n      labs(\n        x = \"prediction\",\n        title = str_glue(\"p = {round(p, digits = 2)}\")\n      ) +\n      theme_no_y()+\n      theme(\n        plot.title = element_text(size = 10)\n      )\n}\n\n\nexample_samp |&gt; \n  rowwise() |&gt; \n  mutate(\n    plot = list(\n      sample_plot_fun(p, pred_binom, n)\n    )\n  )-&gt;\n  samp_plots\n\n\nlibrary(patchwork)\n\nGoing to use some patchwork and purr::reduce() fanciness.\n\nposterior_plot/\n(samp_plots |&gt; \n  ungroup() |&gt; \n  pull(plot) |&gt; \n  reduce(.f = `+`) +\n  plot_layout(nrow = 1))\n\n\n\n\nPretty pleased with this!"
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html",
    "href": "posts/2023-05-15_04-sampling/index.html",
    "title": "Starting Sampling",
    "section": "",
    "text": "Listening\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(here)\nlibrary(gt)\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#classic-base-rate-issues",
    "href": "posts/2023-05-15_04-sampling/index.html#classic-base-rate-issues",
    "title": "Starting Sampling",
    "section": "Classic Base Rate Issues",
    "text": "Classic Base Rate Issues\n\n\n\n\nflowchart TD\n  p[\"population&lt;br&gt;10,000 individuals\"] --&gt; |0.01| v[\"üßõ‚Äç‚ôÇÔ∏è&lt;br&gt;100 individuals\"]\n  p--&gt; |0.99| h[\"üë®&lt;br&gt;9,900 individuals\"]\n  \n  v --&gt; |0.95| vpos[\"üßõ‚Äç‚ôÇÔ∏è‚ûï&lt;br&gt;95 individuals\"]\n  v --&gt; |0.05| vneg[\"üßõ‚Äç‚ôÇÔ∏è‚ûñTest Negative&lt;br&gt;5 individuals\"]\n  \n  h --&gt; |0.01| hpos[\"üë®‚ûï&lt;br&gt;99 individuals\"]\n  h --&gt; |0.99| hneg[\"üë®‚ûñTest Negative&lt;br&gt;9,801 individuals\"]\n  \n  vpos --o pos[\"(üßõ‚Äç‚ôÇÔ∏è,üë®)‚ûï&lt;br&gt;194\"]\n  hpos --o pos\n  \n  vneg --o neg[\"(üßõ‚Äç‚ôÇÔ∏è, üë®)‚ûñ&lt;br&gt;9806\"]\n  hneg --o neg\n\n\n\n\n\nPlot of the base rate vs P(vampire | positive test)\n\ntibble(\n  # might as well get logarithmic\n  base_rate = 10^(seq(-3, -1, length = 20)),\n  vamp_and_pos = base_rate * 0.95,\n  vamp_and_neg = base_rate * 0.05,\n  human_and_pos = (1-base_rate) * 0.01,\n  human_and_neg = (1-base_rate) * 0.99,\n  p_vamp_pos = vamp_and_pos/(vamp_and_pos + human_and_pos), \n  p_hum_neg = human_and_neg/(vamp_and_neg + human_and_neg)\n) -&gt; test_metrics\n\n\ntest_metrics |&gt; \n  ggplot(aes(base_rate, p_vamp_pos))+\n    geom_point(color = \"steelblue\", \n               size = 3)+\n    geom_line(color = \"steelblue\",\n              linewidth = 1)+\n    scale_x_log10()+\n    ylim(0,1)+\n    labs(x = \"P(vampire)\",\n         y = \"P(vampire | positive)\",\n         subtitle = \"P(positive | vampire) = 0.95\\nP(positive | human) = 0.01\",\n         title = \"Positive Predictive Value\") +\n    theme(plot.subtitle = element_text(size = 12))\n\n\n\n\nFigure¬†1: Probability someone is a vampire, given that they tested positive, relative to the base rate of being a vampire\n\n\n\n\n\ntest_metrics |&gt; \n  ggplot(aes(base_rate, p_hum_neg))+\n    geom_point(color = \"steelblue\", \n               size = 3)+\n    geom_line(color = \"steelblue\",\n              linewidth = 1)+\n    scale_x_log10()+\n    labs(x = \"P(vampire)\",\n         y = \"P(human | negative)\",\n         subtitle = \"P(positive | vampire) = 0.95\\nP(positive | human) = 0.01\",\n         title = \"Negative Predictive Value\") +\n    theme(plot.subtitle = element_text(size = 12))\n\n\n\n\nFigure¬†2: Probability of being a human given a negative test, relative to the base rate of being a vampire."
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#tibble-grid-sampling",
    "href": "posts/2023-05-15_04-sampling/index.html#tibble-grid-sampling",
    "title": "Starting Sampling",
    "section": "Tibble grid sampling",
    "text": "Tibble grid sampling\nEstimating posterior density from grid sampling.\n\ngrid &lt;- tibble(\n  # The grid\n  prob = seq(0.0001, 0.9999, length = 5000), \n  \n  # the prior\n  prior_unstd = exp(-abs(prob - .5) / .25),\n  prior_std = prior_unstd/sum(prior_unstd),\n  \n  # the data\n  data = dbinom(6, size = 9, prob = prob),\n  \n  # the posterior\n  posterior_unstd = prior_std * data,\n  posterior = posterior_unstd / sum(posterior_unstd)\n)\n\n\ngrid |&gt; \n  ggplot(aes(prob, prior_std))+\n    geom_line()+\n    labs(y = \"prior density\",\n         title = \"Prior\") -&gt; \n  prior_plot\n\ngrid |&gt; \n  ggplot(aes(prob, data))+\n    geom_line()+\n    labs(y = \"data density\",\n         title = \"Data\") -&gt; \n  data_plot\n\ngrid |&gt; \n  ggplot(aes(prob, posterior))+\n    geom_line() +\n    labs(y = \"posterior density\",\n         title = \"Posterior\") -&gt; \n  posterior_plot\n\nprior_plot | data_plot | posterior_plot\n\n\n\n\nFigure¬†3: Prior, Data, Posterior\n\n\n\n\nSampling from the posterior, using sample_n().\n\ngrid |&gt; \n  sample_n(size = 1e4, \n           replace = T,\n           weight = posterior)-&gt;\n  posterior_samples\n\n\nhead(posterior_samples)\n\n# A tibble: 6 √ó 6\n   prob prior_unstd prior_std  data posterior_unstd posterior\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 0.703       0.444  0.000205 0.266       0.0000546  0.000419\n2 0.694       0.461  0.000213 0.269       0.0000574  0.000441\n3 0.670       0.507  0.000234 0.273       0.0000640  0.000491\n4 0.559       0.789  0.000365 0.220       0.0000803  0.000616\n5 0.620       0.619  0.000286 0.262       0.0000749  0.000575\n6 0.632       0.591  0.000273 0.267       0.0000729  0.000559\n\n\nI‚Äôm going to mess around with finessing the visualizations here.\n\nrenv::install(\"tidybayes\")\n\n\nlibrary(tidybayes)\n\n\nposterior_samples |&gt; \n  pull(prob) |&gt; \n  density() |&gt; \n  tidy() |&gt; \n  rename(prob = x, density = y) -&gt;\n  posterior_dens\n\nposterior_dens |&gt; \n  ggplot(aes(prob, density/max(density)))+\n    geom_area(fill = \"grey60\")+\n    geom_line(aes(y = posterior/max(posterior)),\n              linetype = 2,\n              data = grid)+\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†4: Comparison of the kernel density estimate vs the actual posterior distribution.\n\n\n\n\n\nposterior_samples |&gt; \n  median_hdci(prob, .width = c(0.5, 0.95)) -&gt;\n  intervals\nintervals |&gt; \n  gt() |&gt; \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n    \n  \n  \n    0.59\n0.48\n0.65\n0.50\nmedian\nhdci\n    0.59\n0.37\n0.85\n0.95\nmedian\nhdci\n  \n  \n  \n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(aes(fill = after_stat(pdf)), \n              fill_type = \"gradient\")+\n    scale_y_continuous(expand = expansion(mult = c(0,0)))+\n    khroma::scale_fill_batlow() +\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†5: Posterior density, colored according to the probability density function.\n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(x &gt;= 0.5)),\n      fill_type = \"gradient\"\n    ) +\n    scale_fill_manual(\n      values = c(\n        \"grey70\",\n        \"steelblue\"\n      )\n    )+\n   scale_y_continuous(expand = expansion(mult = c(0,0)))+\n   theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†6: The posterior density colored according to a critical value (0.5)\n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_halfeye(\n      aes(fill = after_stat(level)),\n      fill_type = \"gradient\",\n      point_interval = \"median_hdi\"\n    ) +\n   scale_y_continuous(expand = expansion(mult = c(0.05,0)))+\n   scale_fill_manual(\n     values = c(\"steelblue\", \"steelblue4\")\n   )+\n   theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†7: Posterior samples, colored by their highest density interval levels.\n\n\n\n\nCan I get the plot into a {gt} table? I thin I‚Äôll need to map over the widths? I‚Äôm going off of this gt help page: https://gt.rstudio.com/reference/ggplot_image.html. Let me get the plot right first.\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(level)),\n      .width = 0.66,\n      fill_type = \"gradient\",\n      point_interval = \"median_hdci\"\n    ) +\n    stat_slab(\n      fill = NA,\n      color = \"black\"\n    )+\n    scale_x_continuous(\n      limits = c(0,1),\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_y_continuous(\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_fill_manual(values = \"steelblue\", \n                      guide = \"none\")+\n    theme_void()\n\n\n\n\nFigure¬†8: Table figure experimentation\n\n\n\n\n\nmake_table_plot &lt;- function(.width, data) {\n  ggplot(data, aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(level)),\n      .width = .width,\n      point_interval = \"median_hdci\"\n    ) +\n    stat_slab(\n      fill = NA,\n      color = \"black\"\n    )+\n    scale_x_continuous(\n      limits = c(0,1),\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_y_continuous(\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_fill_manual(values = \"steelblue\", \n                      guide = \"none\")+\n    theme_void()\n}\n\nMap that function over the intervals table I made before.\n\nintervals |&gt; \n  mutate(\n    ggplot = map(.width, ~make_table_plot(.x, posterior_samples)),\n    \n    ## adding an empty column\n    dist = NA\n  ) -&gt; to_tibble\n\nto_tibble |&gt; \n  select(-ggplot) |&gt; \n  gt() |&gt; \n  text_transform(\n    locations = cells_body(columns = dist),\n    fn = \\(x) map(to_tibble$ggplot, ggplot_image, aspect_ratio = 2)\n  )\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n      dist\n    \n  \n  \n    0.587\n0.4815000\n0.6503000\n0.50\nmedian\nhdci\n\n    0.587\n0.3695874\n0.8489874\n0.95\nmedian\nhdci\n\n  \n  \n  \n\n\n\n\nI‚Äôd like more control over how the image appears in the table. Looks like I‚Äôll have to ggsave, and then embed.\n\nmake_custom_table_plot &lt;- function(p){\n  filename &lt;- tempfile(fileext = \".png\")\n  ggsave(plot = p, \n         filename = filename, \n         device = ragg::agg_png, \n         res = 100, \n         width =1.5,\n         height = 0.75)\n  local_image(filename=filename)\n}\n\n\nto_tibble |&gt; \n  select(-ggplot) |&gt; \n  gt() |&gt; \n  text_transform(\n    locations = cells_body(columns = vars(dist)),\n    fn = \\(x) map(to_tibble$ggplot, make_custom_table_plot)\n  )\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n      dist\n    \n  \n  \n    0.587\n0.4815000\n0.6503000\n0.50\nmedian\nhdci\n\n    0.587\n0.3695874\n0.8489874\n0.95\nmedian\nhdci\n\n  \n  \n  \n\n\n\n\nThere we go!\nTurns out this local_image() thing doesn‚Äôt play nice with conversion to pdf (üòï)."
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#brms",
    "href": "posts/2023-05-15_04-sampling/index.html#brms",
    "title": "Starting Sampling",
    "section": "BRMS",
    "text": "BRMS\n\nlibrary(brms)\n\n\ntibble(\n  water = 6,\n  samples = 9\n)-&gt; \n  water_to_model\n\n\nwater_form &lt;- bf(\n   water | trials(samples) ~ 1,\n   family = binomial(link = \"identity\")\n)\n\n\nbrm(\n  water | trials(samples) ~ 1,\n  data = water_to_model,\n  family = binomial(link = \"identity\"),\n  prior(beta(1, 1), class = Intercept, ub = 1, lb = 0),\n  file_refit = \"on_change\",\n  file = \"water_fit.rds\"\n) -&gt;\n  water_model\n\n\nwater_model\n\n Family: binomial \n  Links: mu = identity \nFormula: water | trials(samples) ~ 1 \n   Data: water_to_model (Number of observations: 1) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.64      0.14     0.35     0.88 1.00     1598     1945\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nlibrary(gtsummary)\n\n\nwater_model |&gt; \n  gtsummary::tbl_regression(intercept = T)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n    \n  \n  \n    (Intercept)\n0.64\n0.35, 0.88\n  \n  \n  \n    \n      1 CI = Credible Interval\n    \n  \n\n\n\n\nLet‚Äôs do this again.\n\nwater_model |&gt; \n  get_variables()\n\n[1] \"b_Intercept\"   \"lprior\"        \"lp__\"          \"accept_stat__\"\n[5] \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"  \n[9] \"energy__\"     \n\n\n\nwater_model |&gt; \n  gather_draws(b_Intercept)-&gt;\n  model_draws\n\nmodel_draws |&gt; \n  head() |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\nmodel_draws |&gt; \n  ggplot(aes(.value, .variable)) + \n    stat_halfeye(\n      point_interval = median_hdi,\n      aes(fill = after_stat(level)),\n      fill_type = \"gradient\"\n    ) +\n    xlim(0,1)+\n    scale_fill_manual(\n      values = c(\"steelblue4\", \"steelblue\"),\n    )"
  },
  {
    "objectID": "posts/2023-05-09_01-golem-of-prague/index.html",
    "href": "posts/2023-05-09_01-golem-of-prague/index.html",
    "title": "01 Golem of Prague Chapter",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-05-09_01-golem-of-prague/index.html#hypotheses-models-statistics",
    "href": "posts/2023-05-09_01-golem-of-prague/index.html#hypotheses-models-statistics",
    "title": "01 Golem of Prague Chapter",
    "section": "Hypotheses != Models != Statistics",
    "text": "Hypotheses != Models != Statistics\n\n\n\n\nflowchart LR\n  H0(\"H0&lt;br&gt;Evolution is Neutral\") &lt;--&gt; P0A[\"Process&lt;br&gt;Neutral Equilibrium\"] \n  H0 &lt;--&gt; P0B[\"Process&lt;br&gt;Neutral Non-Equilibrium\"]\n  H1(\"H1&lt;br&gt;Selection Matters\") &lt;--&gt; P1A[\"Process&lt;br&gt;Constant Selection\"]\n  H1 &lt;--&gt; P1B[\"Process&lt;br&gt;Fluctuating Selection\"]\n  \n  P0A &lt;--&gt; MII([\"Model2&lt;br&gt;Power Law\"])\n  P1B &lt;--&gt; MII\n  P1A &lt;--&gt; MIII([\"Model3&lt;br&gt;'something different'\"])\n  P0B &lt;--&gt; MI([\"Model1&lt;br&gt;another thing\"])\n\n\n\n\n\nRejecting Model 1 does not result in a unique identification of a process, or even a hypothesis."
  },
  {
    "objectID": "posts/2023-05-09_00-setup/index.html",
    "href": "posts/2023-05-09_00-setup/index.html",
    "title": "Setup",
    "section": "",
    "text": "I‚Äôve set up the blog using the default quarto blog template in RStudio, also initializing a git repo and renv.\n\nrenv::install(c(\"tidyverse\", \"brms\"))\nrenv::install(c(\"coda\", \"mvtnorm\", \"dagitty\"))\n\nThe preface wants to install the book package with devtooks::install_github(), but I‚Äôm pretty sure that‚Äôs been superseded with remotes::install_github(), and renv::install().\n\ninstall.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nrenv::install(\"rmcelreath/rethinking\")"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html",
    "href": "posts/2023-06-05_07-linear-models-1/index.html",
    "title": "Linear Models: Part 1",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#loading",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#loading",
    "title": "Linear Models: Part 1",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#simulating-a-galton-board",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#simulating-a-galton-board",
    "title": "Linear Models: Part 1",
    "section": "Simulating a Galton Board",
    "text": "Simulating a Galton Board\n\n‚ÄúSuppose you and a thousand of your closest friends line up in the halfway line of a soccer field.‚Äù\n\nOk, so the N is 1+1,000 (‚Äúyou and 1000 of your closest friends‚Äù). Apparently a soccer field is 360 feet long, and an average stride length is something like 2.3 feet.\n\n(360/2)/2.3\n\n[1] 78.26087\n\n\nWe can get in 78 steps from the halfway line to the end of the field.\n\nset.seed(500)\n\nexpand_grid(\n  person = 1:1001,\n  step = 1:78\n) |&gt; \n  mutate(\n    flip = sample(\n      c(-1, 1), \n      size = n(), \n      replace = T\n    )\n  ) |&gt; \n  mutate(\n    .by = person,\n    position = cumsum(flip)\n  ) -&gt;\n  galton_board\n\n\ngalton_board |&gt; \n  mutate(\n    .by = c(step, position),\n    n = n()\n  ) -&gt;\n  galton_board\n\n\ngalton_board |&gt; \n  ggplot(\n    aes(step, position)\n  )+\n    geom_line(\n      aes(group = person, color = n)\n    ) +\n  scale_x_reverse()+\n  khroma::scale_color_bilbao(\n      guide = \"none\"\n    )+  \n  coord_flip()\n\n\n\n\nIt‚Äôs hard to visualize well with the completely overlapping points. I‚Äôll plot histograms for very 10th step.\n\ngalton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) |&gt; \n  ggplot(aes(position, factor(step)))+\n    stat_histinterval(\n      breaks = breaks_fixed(width = 2),\n      aes(fill = after_stat(pdf))\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    scale_y_discrete(\n      limits = factor(seq(70, 10, by = -10))\n    )"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#infinitesimal-galton-board",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#infinitesimal-galton-board",
    "title": "Linear Models: Part 1",
    "section": "Infinitesimal Galton Board",
    "text": "Infinitesimal Galton Board\nSame as before, but now instead of flipping a coin for -1 and 1, values are sampled from \\(\\mathcal{U}(-1,1)\\).\n\nexpand_grid(\n  person = 1:1001,\n  step = 1:78\n) |&gt; \n  mutate(\n    flip = runif(\n      n(),\n      -1,\n      1\n    )\n  ) |&gt; \n  mutate(\n    .by = person,\n    position = cumsum(flip)\n  ) -&gt;\n  inf_galton_board\n\n\ninf_galton_board |&gt; \n  ggplot(aes(step, position))+\n    geom_line(\n      aes(group = person),\n      alpha = 0.05\n    )+\n  scale_x_reverse()+\n  coord_flip()\n\n\n\n\n\ninf_galton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) |&gt; \n  ggplot(aes(position, factor(step)))+\n    stat_slabinterval(\n      aes(fill = after_stat(pdf)), \n      fill_type = \"gradient\"\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    scale_y_discrete(\n      limits = factor(seq(70, 10, by = -10))\n    )\n\n\n\n\nNice.\nI‚Äôm not 100% sure how to get a normal density estimate superimposed in that same plot. So I‚Äôll fake it instead.\n\ninf_galton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) -&gt;\n  ten_steps\n\nten_steps |&gt; \n1  summarise(\n    .by = step,\n    mean = mean(position),\n    sd = sd(position)\n  ) |&gt; \n  nest(\n    .by = step\n  ) |&gt; \n  mutate(\n2    dist = map(\n      data,\n      ~tibble(\n        position = seq(-20, 20, length = 100),\n        dens = dnorm(\n          position, \n          mean = .x$mean, \n          sd = .x$sd\n        )\n      )\n    )\n  ) |&gt; \n  unnest(dist) |&gt; \n3  mutate(\n    dens_norm = dens/max(dens)\n  )-&gt;\n  distributions\n\n\n1\n\nCalculating the distribution parameters for each step grouping.\n\n2\n\nMapping over the distribution parameters to get density values in a tibble.\n\n3\n\nFor plotting over the stat_slab() output, normalizing the density to max out at 1.\n\n\n\n\n\nten_steps |&gt; \n  ggplot(aes(position))+\n    stat_slabinterval(\n      aes(fill = after_stat(pdf)), \n      fill_type = \"gradient\"\n    )+\n    geom_line(\n      data = distributions,\n      aes(y = dens_norm)\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    facet_wrap(\n      ~step, labeller = label_both\n    )+\n    theme_no_y()\n\n\n\n\n\nComparing parameters\nFor my own interest, I wonder how much discrete sampling from -1, 1 vs the uniform distribution affects the \\(\\sigma\\).\n\ngalton_board |&gt; \n  summarise(\n    .by = step,\n    pos_sd = sd(position)\n  ) |&gt; \n  mutate(\n    sampling = \"discrete\"\n  ) -&gt;\n  galton_sd\n\ninf_galton_board |&gt; \n  summarise(\n    .by = step,\n    pos_sd = sd(position)\n  ) |&gt; \n  mutate(\n    sampling = \"uniform\"\n  )-&gt;\n  inf_galton_sd\n\n\nbind_rows(\n  galton_sd, \n  inf_galton_sd\n) |&gt; \n  ggplot(aes(step, pos_sd))+\n    geom_line(\n      aes(color = sampling),\n      linewidth = 1\n    )+\n    expand_limits(y = 0)+\n    labs(\n      y = expression(sigma)\n    )\n\n\n\n\nMessing around with a few obvious values of \\(x\\), in \\(\\mathcal{U}(-x,x)\\), I can‚Äôt tell what would approximate the discrete sampling. 2 is too large, and 1.5 is too small. The answer is probably some horror like \\(\\frac{\\pi}{e}\\).1"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#model-diagrams",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#model-diagrams",
    "title": "Linear Models: Part 1",
    "section": "Model Diagrams",
    "text": "Model Diagrams\nHere‚Äôs the model described in the text.\n\\[\ny_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\beta x_i\n\\]\n\\[\n\\beta \\sim \\mathcal{N}(0, 10)\n\\]\n\\[\n\\sigma \\sim \\text{Exponential}(1)\n\\]\nHe also defines a sampling distribution over \\(x_1\\), but idk if that‚Äôs right. Here‚Äôs my attempt at converting that into a mermaid diagram.\n\n\n\n\nflowchart RL\n  normal1[\"N(Œº·µ¢, œÉ)\"] --&gt;|\"~\"| y[\"y·µ¢\"]\n  beta[\"Œ≤\"] --&gt; mult1([\"√ó\"])\n  x[x·µ¢] --&gt; mult1\n  mult1 --&gt; mu1[Œº·µ¢]\n  mu1 --&gt; normal1\n  \n  exp1[\"Exp(1)\"] --\"~\"--&gt; sigma1[œÉ]\n  sigma1 --&gt; normal1\n  \n  normal2[\"N(0,10)\"] --\"~\"--&gt; beta\n\n\n\n\n\nIt‚Äôs ok. No quite a Kruschke diagram.\n\nAnother example.\nLet me try to write out the diagram for something like y ~ x + (1|z).\n\\[\ny \\sim(\\mu_i, \\sigma_0)\n\\]\n\\[\n\\mu_i = \\beta_0 + \\beta_1x_i + \\gamma_i\n\\]\n\\[\n\\beta_0 \\sim \\mathcal{N}(0,10)\n\\]\n\\[\n\\beta_2 \\sim \\mathcal{N}(0,2)\n\\]\n\\[\n\\gamma_i = \\Gamma_{z_i}\n\\]\n\\[\n\\Gamma_j \\sim \\mathcal{N}(0,\\sigma_1)\n\\]\n\\[\n\\sigma_0 \\sim \\text{Exponential}(1)\n\\]\n\\[\n\\sigma_1 \\sim \\text{Exponential}(1)\n\\]\n\nGeeze, idk. That double subscript feels rough, and I don‚Äôt know the convention for describing the random effects.\n\n\n\n\nflowchart TD\n  normal1[\"N(Œº·µ¢, œÉ‚ÇÄ)\"] --\"~\"--&gt; y[y·µ¢]\n  beta0[\"Œ≤‚ÇÄ\"] --&gt; plus([\"+\"])\n  beta1[\"Œ≤‚ÇÅ\"] --&gt; plus\n  gamma[\"Œ≥·µ¢\"] --&gt; plus\n  plus --&gt; mu[\"Œº·µ¢\"]\n  mu --&gt; normal1\n  normal2[\"N(0,10)\"] --\"~\"--&gt; beta0\n  normal3[\"N(0,2)\"] --\"~\"--&gt; beta1\n  Gamma[\"Œì[z·µ¢]\"] --&gt; gamma\n  normal4[\"N(0, œÉ‚ÇÅ)\"] --\"~\"--&gt; Gamma\n  exponent0[\"Exp(1)\"] --\"~\"--&gt; sigma0[\"œÉ‚ÇÄ\"]\n  sigma0 --&gt; normal1\n  exponent1[\"Exp(1)\"] --\"~\"--&gt; sigma1[\"œÉ‚ÇÅ\"]\n  sigma1 --&gt; normal4\n  \n\n\n\n\n\nYeah, this is too tall. Will have to think about this. The Krushke style diagram is the most compressed version imo."
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#footnotes",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#footnotes",
    "title": "Linear Models: Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot literally \\(\\frac{\\pi}{e}\\) though, cause that‚Äôs too small at 1.156‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html",
    "title": "Garden of Forking paths part 2",
    "section": "",
    "text": "Listening\nrenv::install(\"khroma\")\nlibrary(tidyverse)\nlibrary(khroma)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#a-nicer-table-version.",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#a-nicer-table-version.",
    "title": "Garden of Forking paths part 2",
    "section": "A nicer table version.",
    "text": "A nicer table version.\nI‚Äôd like to re-represent the Bayesian Update in a nicer GT table. Some options are\n\nPlotting extensions from {gtExtras}\nEmojis\n\n\nrenv::install(\"gtExtras\")\nrenv::install(\"svglite\")\nrenv::install(\"emoji\")\n\n\nlibrary(gtExtras)\nlibrary(emoji)\n\nFirst, trying the ‚Äúwin/losses‚Äù column plot from {gtExtra} to illustrate the blue vs white marbles.\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(1, blue_marbs), \n                     rep(0, white_marbs)))\n  ) -&gt; \n  marbles_wl\n\nThe cell background will have to be off-white for the white ticks to show\n\nmarbles_wl |&gt; \n  gt() |&gt; \n  gt_plt_winloss(marbles, palette = c(\"blue\", \"white\", \"grey\")) |&gt; \n  tab_style(style = cell_fill(color = \"antiquewhite\"), \n            locations = cells_body())\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\n          \n    1\n3\n          \n    2\n2\n          \n    3\n1\n          \n    4\n0\n          \n  \n  \n  \n\nTable¬†1:  Representing marble compositions with ‚Äòwin-loss‚Äô plots \n\n\n\nI‚Äôm not overwhelmed by the result. I‚Äôll try emojis instead.\n\nblue_marb &lt;- emojis |&gt; \n  filter(str_detect(name, \"blue\"), \n         str_detect(name, \"circle\")) |&gt; \n  pull(emoji)\n\nblue_marb\n\n[1] \"üîµ\"\n\n\n\nwhite_marb &lt;- emojis |&gt; \n  filter(str_detect(name, \"white\"), \n         str_detect(name, \"circle\")) |&gt; \n  pull(emoji)\n\nwhite_marb\n\n[1] \"‚ö™\"\n\n\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(blue_marb, blue_marbs), \n                     rep(white_marb, white_marbs)))\n  ) -&gt; \n  marbles_emoji\n\n\nmarbles_emoji |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\n‚ö™, ‚ö™, ‚ö™, ‚ö™\n    1\n3\nüîµ, ‚ö™, ‚ö™, ‚ö™\n    2\n2\nüîµ, üîµ, ‚ö™, ‚ö™\n    3\n1\nüîµ, üîµ, üîµ, ‚ö™\n    4\n0\nüîµ, üîµ, üîµ, üîµ\n  \n  \n  \n\nTable¬†2:  Representing marble compositions with emoji \n\n\n\nUpdate: As it turns out, getting tables with emoji and the plots with gt_plt_*() do not play nice with LaTeX. For now, I‚Äôm saving the tables to png with gtsave() just for the pdf output.\nYes, this is it.\n\nRerunning the sampling\nI‚Äôll re-run the sampling from the previous post.\n\nsampling_df &lt;- function(marbles, \n                        n = 1000, \n                        size = 3, \n                        pattern = c(blue_marb, white_marb, blue_marb)){\n  sampling_tibble &lt;- tibble(samp = 1:n)   \n  sampling_tibble |&gt; \n    mutate(\n      chosen = map(samp, \n                   ~sample(marbles, \n                           size = 3, \n                           replace = T)),\n      match = map_lgl(chosen, \n                      ~all(.x == pattern))                 \n    ) |&gt; \n    summarise(prop_match = mean(match))-&gt;                         \n    sampling_tibble\n  return(sampling_tibble)\n}\n\n\nmarbles_emoji |&gt; \n ungroup() |&gt; \n  mutate(\n    prob = map(marbles, ~sampling_df(.x, n = 10000))\n  ) |&gt; \n  unnest(prob) |&gt; \n  mutate(norm_probs = prop_match/sum(prop_match))-&gt;\n  marble_probs\n\nI want to label the column of probabilities with the key sequence.\n\nkey_seq &lt;- str_glue(\"{blue_marb}, {white_marb}, {blue_marb}\")\n\ngtExtras::gt_plt_bar_pct() will plot a bar chart within the table.\n\nmarble_probs |&gt; \n  select(marbles, norm_probs) |&gt; \n  mutate(norm_probs = norm_probs * 100) |&gt; \n  gt() |&gt; \n  cols_label(\n    norm_probs = str_glue(\"p(marbles | {key_seq})\")\n  ) |&gt; \n  gt_plt_bar_pct(norm_probs, \n                 scaled = T, \n                 fill = \"steelblue\")\n\n\n\n\n\n\n  \n    \n    \n      marbles\n      p(marbles | üîµ, ‚ö™, üîµ)\n    \n  \n  \n    ‚ö™, ‚ö™, ‚ö™, ‚ö™\n\n    üîµ, ‚ö™, ‚ö™, ‚ö™\n\n    üîµ, üîµ, ‚ö™, ‚ö™\n\n    üîµ, üîµ, üîµ, ‚ö™\n\n    üîµ, üîµ, üîµ, üîµ\n\n  \n  \n  \n\nTable¬†3:  Probability of each marble composition given (üîµ, ‚ö™Ô∏è, üîµ) samples\nwith replacement. \n\n\n\nThere we go!\n\n\nWith the Bayesian Update\n\nmarble_probs |&gt; \n  mutate(new_prob = blue_marbs/sum(blue_marbs),\n         multiplied = norm_probs * new_prob,\n         norm_new = multiplied/sum(multiplied)) |&gt; \n  select(marbles, norm_probs, norm_new) |&gt; \n  mutate(norm_probs = norm_probs * 100,\n         norm_new = norm_new * 100) |&gt; \n  gt() |&gt; \n  cols_label(\n    norm_probs = str_glue(\"p(marbles | {key_seq})\"),\n    norm_new = str_glue(\"after {blue_marb}\")\n  ) |&gt; \n  gt_plt_bar_pct(norm_probs, \n                 scaled = T, \n                 fill = \"steelblue\") |&gt; \n  gt_plt_bar_pct(norm_new, \n                 scaled = T, \n                 fill = \"steelblue\") |&gt; \n  cols_width(2 ~ px(200),\n             3 ~ px(200))\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n    \n      marbles\n      p(marbles | üîµ, ‚ö™, üîµ)\n      after üîµ\n    \n  \n  \n    ‚ö™, ‚ö™, ‚ö™, ‚ö™\n\n\n    üîµ, ‚ö™, ‚ö™, ‚ö™\n\n\n    üîµ, üîµ, ‚ö™, ‚ö™\n\n\n    üîµ, üîµ, üîµ, ‚ö™\n\n\n    üîµ, üîµ, üîµ, üîµ\n\n\n  \n  \n  \n\nTable¬†4:  Probability of each marble composition given an additional (üîµ)\nsample"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#bayesian-updating",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#bayesian-updating",
    "title": "Garden of Forking paths part 2",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\nI‚Äôll try to illustrate Baysian updating with an animated plotly plot.\n\nrenv::install(\"plotly\")\nrenv::install(\"slider\")\n\n\nlibrary(plotly)\nlibrary(slider)\n\nI know enough to know that the distribution that‚Äôs being updated is the beta. (Apparently is the binomial. Still a but lost on their distinction!) So I‚Äôll get the density for each update.\n\nplot(\n  seq(0,1, length =100),\n  dbeta(seq(0,1, length =100), 1, 1),\n  type = 'l'\n)\n\n\n\n\n\nwater_land_sequence &lt;- c(\"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\n\nI‚Äôll use slider::slide() to generate a data frame of sample updates. I‚Äôll need a function that takes a sequence of W and L and converts them into counts.\n\nw_l_count &lt;- function(x){\n  tibble(\n    water = sum(x == \"W\"),\n    land = sum(x == \"L\")\n  )\n}\n\n\nslide(water_land_sequence, \n      .f = w_l_count, \n      .before = Inf,\n      .after = 0) |&gt; \n  bind_rows() |&gt; \n  mutate(seq = row_number()) |&gt; \n  bind_rows(\n    tibble(\n      water = 0,\n      land = 0, \n      seq = 0\n    )\n  ) |&gt; \n  arrange(seq) -&gt;\n  sequence_counts\n\n\nsequence_counts |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      water\n      land\n      seq\n    \n  \n  \n    0\n0\n0\n    1\n0\n1\n    1\n1\n2\n    2\n1\n3\n    3\n1\n4\n    3\n2\n5\n    4\n2\n6\n    4\n3\n7\n    5\n3\n8\n  \n  \n  \n\nTable¬†5:  Table of water, land count updates \n\n\n\nNow to get the densities.\n\nsequence_counts |&gt; \n  rowwise() |&gt; \n  mutate(\n    density = map2(\n      water, land, ~tibble(\n        prop = seq(0.0001, 0.9999, length = 100),\n        density_unstd = dbinom(water, size = water + land, prob = prop),\n        density = density_unstd/sum(density_unstd)\n        )\n    )\n  ) |&gt; \n  unnest(density)-&gt;\n  density_updates\n\n\ndensity_updates |&gt; \n  ggplot(aes(prop, density))+\n    geom_line(aes(group = seq, color = seq))\n\n\n\n\nFigure¬†1: beta distribution updates\n\n\n\n\nGood first step.\nI had to turn to the plotly book to get the animated lines correct https://plotly-r.com/animating-views.html.\n\nsequence_counts |&gt; \n  mutate(\n    annotation = str_glue(\"W:{water}, L:{land}\")\n  ) -&gt; \n  wl_annotate\n\n\ndensity_updates |&gt; \n  plot_ly() |&gt; \n  add_lines(\n    x = ~prop,\n    y = ~density,\n    frame = ~seq,\n    line = list(simplify = F, width = 3)\n  ) |&gt; \n  add_text(\n    data = wl_annotate,\n    text = ~annotation,\n    frame = ~seq,\n    x = 0.1,\n    y = 0.025,\n    textfont = list(size = 20)\n  ) |&gt;\n  layout(\n    showlegend = F\n  )\n\n\n\n\nFigure¬†2: Animated Bayesian Updating\n\n\n\nUpdate: I started making this just for the pdf, which can‚Äôt have the animation, but it‚Äôs actually kind of nice enough to include in the html.\n\ndensity_updates |&gt; \n  group_by(prop) |&gt; \n  arrange(seq) |&gt; \n  mutate(\n    prev_density = lag(density),\n    facet_lab = str_glue(\"W:{water}, L{land}\")\n  ) |&gt; \n  ggplot(aes(prop, density))+\n    geom_area(aes(y = prev_density), linetype = 2, alpha = 0.2)+\n    geom_area(alpha = 0.6, fill = \"steelblue\")+\n    scale_y_continuous(expand = expansion(mult = c(0,0)))+\n    facet_wrap(~facet_lab)+\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 8),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†3: Static figure of Bayesian Updating"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#on-priors",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#on-priors",
    "title": "Garden of Forking paths part 2",
    "section": "On priors",
    "text": "On priors\n\nThe fact that statistical inference uses mathematics does not imply that there is only one reasonable or useful way to conduct an analysis."
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#grid-approximation",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#grid-approximation",
    "title": "Garden of Forking paths part 2",
    "section": "Grid approximation",
    "text": "Grid approximation\nOk, I‚Äôll do one grid approximation for the hell of it.\n‚Ä¶\nGot distracted and went down a rabbit hole on the beta vs binomial distributions.\n\ntibble(\n  prob = seq(0.0001, 0.9999, length = 50), \n  prior_unstd = case_when(\n    prob &lt; 0.5 ~ 0,\n    .default = 1\n  ),\n  prior_std = prior_unstd/sum(prior_unstd),\n  likelihood_binom = dbinom(6, size = 9, prob = prob),\n  l_binom_std = likelihood_binom/sum(likelihood_binom),\n  likelihood_beta = dbeta(prob, 6, 9-6),\n  l_beta_std = likelihood_beta/sum(likelihood_beta)\n) |&gt; \n  ggplot(aes(prob))+\n    geom_point(aes(y = l_binom_std, color = \"binom\"))+\n    geom_point(aes(y = l_beta_std, color = \"beta\"))+\n    scale_color_bright()+\n    labs(\n      x = \"probability\",\n      color = \"distribution\",\n      y = NULL\n    )\n\n\n\n\nFigure¬†4: Comparing normalized densities for beta(6,3) and binom(6, 9, p)\n\n\n\n\nGlad I did this. I guess\n\ndbinom \\(\\propto P(O, S | p)\\)\ndbeta \\(\\propto P(p|O,S)\\)\n\nI guess I‚Äôd want to see dbinom plotted out with O on the x axis?\n\ntibble(\n  probability = seq(0.0001, 0.9999, length = 10)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    densities = map(\n      probability,\n      ~tibble(obs = 0:9, \n              density = dbinom(obs, size = 9, prob = .x))\n    )\n  ) |&gt; \n  unnest(densities) -&gt;\n  binomial_densities \n\nbinomial_densities |&gt; \n  ggplot(aes(obs, density, color = probability))+\n    geom_point()+\n    geom_line(aes(group = probability)) +\n    geom_rect(\n      color = \"red\",\n      fill = NA,\n      xmin = 5.5,\n      xmax = 6.5,\n      ymin = 0,\n      ymax = 1\n    )+\n    scale_color_batlow()\n\n\n\n\nFigure¬†5: Binomial distributions of various successes out of 9 trials, for various p\n\n\n\n\nWhat we‚Äôre plotting out is what‚Äôs in the red box, flipped on its side.\n\nbinomial_densities |&gt; \n  filter(obs == 6) |&gt; \n  ggplot(aes(probability, density))+\n    geom_line()+\n    geom_point(aes(color = probability))+\n    scale_color_batlow()\n\n\n\n\nFigure¬†6: Normalized binomial density for 6 successes out of 9 trials for various p."
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#update",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#update",
    "title": "Garden of Forking paths part 2",
    "section": "Update!",
    "text": "Update!\nThanks TJ!\n\n\ni forgot how i know this but they are the same if you plug in likelihood_beta = dbeta(prob, 1 + 6, 1 + 9-6),\n\n‚Äî tj mahr üççüçï (@tjmahr) May 10, 2023\n\n\n\ntibble(\n  prob = seq(0.0001, 0.9999, length = 50), \n  prior_unstd = case_when(\n    prob &lt; 0.5 ~ 0,\n    .default = 1\n  ),\n  prior_std = prior_unstd/sum(prior_unstd),\n  likelihood_binom = dbinom(6, size = 9, prob = prob),\n  l_binom_std = likelihood_binom/sum(likelihood_binom),\n  likelihood_beta = dbeta(prob, (6+1), (9-6)+1),\n  l_beta_std = likelihood_beta/sum(likelihood_beta)\n) |&gt; \n  ggplot(aes(prob))+\n    geom_point(aes(y = l_binom_std, color = \"binom\", size = \"binom\"))+\n    geom_point(aes(y = l_beta_std, color = \"beta\",size = \"beta\"))+\n    scale_color_bright()+\n    labs(\n      x = \"probability\",\n      color = \"distribution\",\n      size = \"distribution\",\n      y = NULL\n    )\n\n\n\n\nFigure¬†7: Comparing normalized densities for beta(6+1,3+1) and binom(6, 9, p)"
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html",
    "title": "02 Small Worlds and Large Worlds",
    "section": "",
    "text": "listening\nIn the analogy, models are ‚ÄúSmall‚Äù, self-contained worlds."
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html#garden-of-forking-paths.",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html#garden-of-forking-paths.",
    "title": "02 Small Worlds and Large Worlds",
    "section": "Garden of forking paths.",
    "text": "Garden of forking paths.\nI was thinking of working out the probabilities by doing random sampling‚Ä¶\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)\nsource(here(\"_defaults.R\"))\n\nGenerating the marble dataframe\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(\"blue\", blue_marbs), rep(\"white\", white_marbs)))\n  ) -&gt; \n  marbles\n\n\nmarbles |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\nwhite, white, white, white\n    1\n3\nblue, white, white, white\n    2\n2\nblue, blue, white, white\n    3\n1\nblue, blue, blue, white\n    4\n0\nblue, blue, blue, blue\n  \n  \n  \n\nTable¬†1:  The marble sampling distributions \n\n\n\nIn retrospect, I‚Äôm glad I did this, because I thought we were sampling without replacement.\nHere‚Äôs a function that will repeatedly sample from a set of marbles, and compare the result to a reference group.\n\nsampling_df &lt;- function(marbles, n = 1000, size = 3, pattern = c(\"blue\", \"white\", \"blue\")){\n1  sampling_tibble &lt;- tibble(samp = 1:n)\n  sampling_tibble |&gt; \n    mutate(\n2      chosen = map(samp, ~sample(marbles, size = 3, replace = T)),\n3      match = map_lgl(chosen, ~all(.x == pattern))\n    ) |&gt; \n4    summarise(prop_match = mean(match))-&gt;\n    sampling_tibble\n  return(sampling_tibble)\n}\n\n\n1\n\nI‚Äôll capture everything within a tibble.\n\n2\n\nRowwise, sample from marbles with replacement.\n\n3\n\nReturn T or F if the sequence matches the pattern exactly.\n\n4\n\nThe mean() of the T, F column to get the proportion that match.\n\n\n\n\n\nsampling_df(\n  marbles = marbles$marbles[[4]],\n  n = 5000\n) \n\n# A tibble: 1 √ó 1\n  prop_match\n       &lt;dbl&gt;\n1      0.140\n\n\n\nmarbles |&gt; \n ungroup() |&gt; \n  mutate(\n    prob = map(marbles, ~sampling_df(.x, n = 10000))\n  ) |&gt; \n  unnest(prob) |&gt; \n  mutate(norm_probs = prop_match/sum(prop_match))-&gt;\n  marble_probs\n\n\nmarble_probs |&gt; \n  ggplot(aes(blue_marbs, norm_probs))+\n    geom_col(fill = \"steelblue4\")+\n    labs(\n      title = \"blue, white, blue\",\n      x = \"# of blue marbles\",\n      y = \"probability\"\n    ) + \n  ylim(0,1)-&gt;probs1\nprobs1\n\n\n\n\nFigure¬†1: Probability of each composition of marbles"
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html#updating-probabilities",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html#updating-probabilities",
    "title": "02 Small Worlds and Large Worlds",
    "section": "Updating probabilities",
    "text": "Updating probabilities\nWhat if we draw one more blue\n\nmarble_probs |&gt; \n  mutate(new_obs_prob = blue_marbs / sum(blue_marbs),\n         posterior_prob = norm_probs * new_obs_prob,\n         posterior_norm = posterior_prob/sum(posterior_prob))-&gt;\n  marble_probs\n\n\nmarble_probs |&gt; \n  ggplot(aes(blue_marbs, posterior_norm))+\n    geom_col(fill = \"steelblue4\")+\n    ylim(0,1)+\n      labs(\n      title = \"probability update after blue\",\n      x = \"# of blue marbles\",\n      y = \"probability\"\n    ) -&gt;\n  probs2\n\nprobs1 | probs2\n\n\n\n\nFigure¬†2: Bayesian update"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html",
    "href": "posts/2023-06-02_05-sampling2/index.html",
    "title": "Sampling Summaries",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#loading",
    "href": "posts/2023-06-02_05-sampling2/index.html#loading",
    "title": "Sampling Summaries",
    "section": "Loading",
    "text": "Loading\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#setting-up-the-grid-samples",
    "href": "posts/2023-06-02_05-sampling2/index.html#setting-up-the-grid-samples",
    "title": "Sampling Summaries",
    "section": "Setting up the grid samples",
    "text": "Setting up the grid samples\n\ntibble(\n  p = seq(0, 1, length = 1000),\n  dens = dbeta(p, 6+1, 3+1),\n  posterior = dens/sum(dens)\n) -&gt;\n  posterior_grid\n\n\nposterior_grid |&gt; \n  ggplot(aes(p, posterior))+\n    geom_area(fill = ptol_blue, color = \"black\")\n\n\n\n\n\nSampling from the posterior\n\nposterior_grid |&gt; \n  sample_n(\n    size = 1e4,\n    replace = T,\n    weight = posterior\n  )-&gt;\n  posterior_samples\n\nThis isn‚Äôt MCMC sampling, but I‚Äôll plot it as a line just for consistency for how MCMC chains look.\n\nposterior_samples |&gt; \n  mutate(\n    sample = row_number()\n  ) |&gt; \n  ggplot(aes(sample, p))+\n    geom_line()\n\n\n\n\nComparing to the sampling to the original density function.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_density(\n      fill = ptol_blue\n      ) +\n    geom_line(\n      data = posterior_grid,\n      aes(y = dens),\n      color = ptol_red,\n      linewidth = 1\n    )"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#quantiles",
    "href": "posts/2023-06-02_05-sampling2/index.html#quantiles",
    "title": "Sampling Summaries",
    "section": "Quantiles",
    "text": "Quantiles\nFirst manually\n\nposterior_samples |&gt; \n  reframe(\n    lowhi = quantile(p, probs = c(0.25, 0.75))\n  ) |&gt; \n  pull(lowhi)-&gt;\n  fifty_quantile\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    ggdist::stat_slab(\n      color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n    labs(\n      fill = \"fifty\",\n      y = NULL\n    )+\n    scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n    theme(\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nI think I‚Äôll create a shortcut theme for having no y axis.\n\ntheme_no_y &lt;- function(){\n  theme(\n      axis.text.y = element_blank(),\n      axis.title.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n}\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_slab() +\n    theme_no_y()\n\n\n\n\n\nTidybayes functions\nI‚Äôm not 100% sure how all of the tidybayes functions work.\n\nposterior_samples |&gt; \n  summarise(\n    median_qi(p, .width = 0.5)\n  )\n\n# A tibble: 1 √ó 6\n      y  ymin  ymax .width .point .interval\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.644 0.542 0.738    0.5 median qi       \n\n\n\nposterior_samples |&gt; \n  reframe(\n    quantile  = quantile(p, probs = c(0.25, 0.5, 0.75))\n  )\n\n# A tibble: 3 √ó 1\n  quantile\n     &lt;dbl&gt;\n1    0.542\n2    0.644\n3    0.738\n\n\nOk, *_qi() returns the quantile interval.\nI‚Äôd like to make the plot according to the statistics calculated by stat_halfeye(), but can‚Äôt seem to get it to work.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      aes(\n        fill = after_stat(x &gt; xmin)\n      )\n    )\n\n\n\n\nI‚Äôll just do the same filling I did before.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_qi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()-&gt;\n  fifty_qi\nfifty_qi\n\n\n\n\n\n\nHPDI\nLemme try hpdi now.\n\nposterior_samples |&gt; \n  summarise(\n    mean_hdi(p, .width = 0.5)\n  )-&gt;\n  posterior_hdi\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_hdi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= posterior_hdi$ymin & \n            x &lt;=  posterior_hdi$ymax\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()-&gt;\n  fifty_hdi\nfifty_hdi\n\n\n\n\n\nfifty_qi/fifty_hdi\n\n\n\n\nThey‚Äôre very similar, but if I mix the qi fill and the hdi interval, they‚Äôre different.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_hdi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n         x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Workthrough Blog",
    "section": "",
    "text": "Predictive Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models: Part 1\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nSampling Summaries\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting Sampling\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nGarden of Forking paths part 2\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\n01 Golem of Prague Chapter\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nSetup\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\n02 Small Worlds and Large Worlds\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog to document my progress in working through Richard McElreath‚Äôs Statistical Rethinking: A Bayesian Course with Examples in R and Stan, which I‚Äôll be supplementing with Solomon Kurz‚Äô bookdown project, Statistical Rethinking with brms, ggplot2, and the tidyverse."
  }
]