[
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html",
    "title": "Predictive Distributions",
    "section": "",
    "text": "Listening\nOne last post to work through different predictive distributions given 6W and 3W."
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#loading",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#loading",
    "title": "Predictive Distributions",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(here)\nlibrary(ggblend)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#prior-predictive",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#prior-predictive",
    "title": "Predictive Distributions",
    "section": "Prior predictive",
    "text": "Prior predictive\nThe prior was a Uniform distribution between 0 and 1, or \\(\\mathcal{U}(0,1)\\). The outcome will also be uniform, but I‚Äôll go through all the steps for completeness.\n\ntibble(\n  p = runif(1e4)\n) -&gt;\n  prior_samp\n\nI‚Äôll make use of vectorization of rbinom() to generate possible samples with probabilities in prior_samp$p.\n\nset.seed(2023)\nprior_samp |&gt; \n  mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n1  count(pred_binom) |&gt;\n  mutate(prob = n/sum(n)) -&gt;\n  prior_pred\n\n\n1\n\nI‚Äôm going straight from generated samples to summarising for the distribution.\n\n\n\n\n\nprior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n1    theme_no_y()\n\n\n1\n\nI‚Äôve moved theme_no_y() into _defaults.R which I source at the top."
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#maximum-likelihood",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#maximum-likelihood",
    "title": "Predictive Distributions",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe maximum likelihood probability of W is \\(\\frac{6}{9}=0.6\\overline{6}\\). We can get predicted values for that probability from binomial distribution.\n\ntibble(\n  pred_binom = 0:9,\n  dens = dbinom(\n    pred_binom, \n    size = 9, \n    prob = 6/9\n  ),\n  prob = dens\n) -&gt; \n  ml_pred\n\nml_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#posterior-prediction",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#posterior-prediction",
    "title": "Predictive Distributions",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\nFor the posterior prediction, first we sample probabilities from the beta distribution\n\\[\np \\sim \\text{Beta}(W+1, L+1)\n\\]\n(a.k.a. dbeta())\nThen for each \\(p\\), we generate predictions from the binomial distribution.\n\\[\nY|p \\sim \\text{Bin}(9,p)\n\\]\nThis chapter has been doing grid sampling, but I‚Äôll just use the rbeta() function for simplicity.\n\ntibble(\n  p = rbeta(1e4, 6+1, 3+1)\n) -&gt;\n  posterior_samp\n\nThen, it‚Äôs the same operation as the prior predictive distribution.\n\nposterior_samp |&gt; \n   mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n  count(pred_binom) |&gt; \n  mutate(prob = n/sum(n)) -&gt;\n  posterior_pred\n\n\nposterior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col() +\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    ) +\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#comparisons",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#comparisons",
    "title": "Predictive Distributions",
    "section": "Comparisons",
    "text": "Comparisons\nNow, I want to compare the posterior predictive distribution to the maximum likelihood and the prior. This is one concept I had, which has the prior and the ML values as thinner bars within the posterior predictive bars.\n\nposterior_pred |&gt; \n  ggplot(aes(pred_binom, prob))+\n    geom_col()+\n    geom_segment(\n      data = ml_pred,\n      aes(\n        x = pred_binom-0.1,\n        xend = pred_binom-0.1,\n        yend = 0,\n        color = \"maximum likelihood\"\n      ),\n      linewidth = 2\n    )+\n    geom_segment(\n      data = prior_pred,\n      aes(\n        x = pred_binom+0.1,\n        xend = pred_binom+0.1,\n        yend = 0,\n        color = \"prior\"\n      ),\n      linewidth = 2\n    )+  \n    scale_x_continuous(\n      breaks = seq(1, 9, length = 2)\n    )+\n    labs(\n      color = NULL,\n      x = \"predicted W\"\n    )+\n    theme_no_y()+\n    theme(\n      legend.position = \"top\"\n    )\n\n\n\n\nOverplotting them is also a possibility, and a good occasion to test out {ggblend}. I had some issues getting this to work with the available graphic devices & quarto.\n\n```{r}\n#| dev: \"png\"\n#| dev-args:\n#|   - type: \"cairo\"\nbind_rows(\n  posterior_pred |&gt; \n    mutate(pred = \"posterior\"),\n  ml_pred |&gt; \n    mutate(pred = \"maximum likelihood\"),\n  prior_pred |&gt; \n    mutate(pred = \"prior\")\n) |&gt; \n  ggplot(aes(pred_binom, prob, fill = pred))+\n    geom_area(position = \"identity\", alpha = 0.5) * \n      (blend(\"lighten\") + blend(\"darken\")) +\n    scale_x_continuous(\n      name = \"predicted W\",\n      breaks = seq(1, 9, by = 2)\n    )+\n    theme_no_y()+\n    theme(\n      legend.position = \"top\"\n    )\n```"
  },
  {
    "objectID": "posts/2023-06-05_06-posterior-predictive/index.html#visualizing-the-posterior-prediction-process",
    "href": "posts/2023-06-05_06-posterior-predictive/index.html#visualizing-the-posterior-prediction-process",
    "title": "Predictive Distributions",
    "section": "Visualizing the posterior prediction process",
    "text": "Visualizing the posterior prediction process\n\nposterior_samp |&gt; \n  slice(1:4) |&gt; \n  mutate(\n    pred_binom = rbinom(\n      n(), \n      size = 9, \n      prob = p\n    )\n  ) |&gt; \n  arrange(\n    p\n  ) -&gt;\n  example_samp\n\n\nexample_samp |&gt; \n  mutate(\n    dens = dbeta(p, 6+1, 3+1),\n    n = row_number()\n  )-&gt;\n  example_samp\n\ntibble(\n  p = seq(0, 1, length = 100),\n  dens = dbeta(p, 6+1, 3+1)\n) |&gt; \n  ggplot(aes(p, dens))+\n    geom_area(fill = \"grey\") +\n    geom_segment(\n      data = example_samp,\n      aes(\n        xend = p,\n        yend = 0,\n        color = factor(n)\n      ),\n      linewidth = 1.5,\n      lineend = \"round\",\n      show.legend = F\n    )+\n    theme_no_y()+\n    labs(\n      title = \"Posterior\"\n    )-&gt;\n  posterior_plot\n\nThis involved messing around with {rlang} data masking that I still don‚Äôt quite follow. Both the p and pred arguments had to be (just once!) prefixed with !!, but not n.\n\nsample_plot_fun &lt;- function(p, pred, n){\n  this_scale &lt;- as.vector(khroma::color(\"bright\")(4))\n  tibble(\n    ws = 0:9,\n    dens = dbinom(ws, size = 9, prob = !!p)\n  ) |&gt; \n    ggplot(aes(ws, dens))+\n      geom_col(\n        aes(fill = ws == !!pred)\n      )+\n      scale_fill_manual(\n        values = c(\"grey\", this_scale[n]),\n        guide = \"none\"\n      ) +\n      scale_x_continuous(\n        breaks = seq(1,9, by = 2)\n      )+\n      labs(\n        x = \"prediction\",\n        title = str_glue(\"p = {round(p, digits = 2)}\")\n      ) +\n      theme_no_y()+\n      theme(\n        plot.title = element_text(size = 10)\n      )\n}\n\n\nexample_samp |&gt; \n  rowwise() |&gt; \n  mutate(\n    plot = list(\n      sample_plot_fun(p, pred_binom, n)\n    )\n  )-&gt;\n  samp_plots\n\n\nlibrary(patchwork)\n\nGoing to use some patchwork and purr::reduce() fanciness.\n\nposterior_plot/\n(samp_plots |&gt; \n  ungroup() |&gt; \n  pull(plot) |&gt; \n  reduce(.f = `+`) +\n  plot_layout(nrow = 1))\n\n\n\n\nPretty pleased with this!"
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html",
    "href": "posts/2023-05-15_04-sampling/index.html",
    "title": "Starting Sampling",
    "section": "",
    "text": "Listening\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(here)\nlibrary(gt)\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#classic-base-rate-issues",
    "href": "posts/2023-05-15_04-sampling/index.html#classic-base-rate-issues",
    "title": "Starting Sampling",
    "section": "Classic Base Rate Issues",
    "text": "Classic Base Rate Issues\n\n\n\n\nflowchart TD\n  p[\"population&lt;br&gt;10,000 individuals\"] --&gt; |0.01| v[\"üßõ‚Äç‚ôÇÔ∏è&lt;br&gt;100 individuals\"]\n  p--&gt; |0.99| h[\"üë®&lt;br&gt;9,900 individuals\"]\n  \n  v --&gt; |0.95| vpos[\"üßõ‚Äç‚ôÇÔ∏è‚ûï&lt;br&gt;95 individuals\"]\n  v --&gt; |0.05| vneg[\"üßõ‚Äç‚ôÇÔ∏è‚ûñTest Negative&lt;br&gt;5 individuals\"]\n  \n  h --&gt; |0.01| hpos[\"üë®‚ûï&lt;br&gt;99 individuals\"]\n  h --&gt; |0.99| hneg[\"üë®‚ûñTest Negative&lt;br&gt;9,801 individuals\"]\n  \n  vpos --o pos[\"(üßõ‚Äç‚ôÇÔ∏è,üë®)‚ûï&lt;br&gt;194\"]\n  hpos --o pos\n  \n  vneg --o neg[\"(üßõ‚Äç‚ôÇÔ∏è, üë®)‚ûñ&lt;br&gt;9806\"]\n  hneg --o neg\n\n\n\n\n\nPlot of the base rate vs P(vampire | positive test)\n\ntibble(\n  # might as well get logarithmic\n  base_rate = 10^(seq(-3, -1, length = 20)),\n  vamp_and_pos = base_rate * 0.95,\n  vamp_and_neg = base_rate * 0.05,\n  human_and_pos = (1-base_rate) * 0.01,\n  human_and_neg = (1-base_rate) * 0.99,\n  p_vamp_pos = vamp_and_pos/(vamp_and_pos + human_and_pos), \n  p_hum_neg = human_and_neg/(vamp_and_neg + human_and_neg)\n) -&gt; test_metrics\n\n\ntest_metrics |&gt; \n  ggplot(aes(base_rate, p_vamp_pos))+\n    geom_point(color = \"steelblue\", \n               size = 3)+\n    geom_line(color = \"steelblue\",\n              linewidth = 1)+\n    scale_x_log10()+\n    ylim(0,1)+\n    labs(x = \"P(vampire)\",\n         y = \"P(vampire | positive)\",\n         subtitle = \"P(positive | vampire) = 0.95\\nP(positive | human) = 0.01\",\n         title = \"Positive Predictive Value\") +\n    theme(plot.subtitle = element_text(size = 12))\n\n\n\n\nFigure¬†1: Probability someone is a vampire, given that they tested positive, relative to the base rate of being a vampire\n\n\n\n\n\ntest_metrics |&gt; \n  ggplot(aes(base_rate, p_hum_neg))+\n    geom_point(color = \"steelblue\", \n               size = 3)+\n    geom_line(color = \"steelblue\",\n              linewidth = 1)+\n    scale_x_log10()+\n    labs(x = \"P(vampire)\",\n         y = \"P(human | negative)\",\n         subtitle = \"P(positive | vampire) = 0.95\\nP(positive | human) = 0.01\",\n         title = \"Negative Predictive Value\") +\n    theme(plot.subtitle = element_text(size = 12))\n\n\n\n\nFigure¬†2: Probability of being a human given a negative test, relative to the base rate of being a vampire."
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#tibble-grid-sampling",
    "href": "posts/2023-05-15_04-sampling/index.html#tibble-grid-sampling",
    "title": "Starting Sampling",
    "section": "Tibble grid sampling",
    "text": "Tibble grid sampling\nEstimating posterior density from grid sampling.\n\ngrid &lt;- tibble(\n  # The grid\n  prob = seq(0.0001, 0.9999, length = 5000), \n  \n  # the prior\n  prior_unstd = exp(-abs(prob - .5) / .25),\n  prior_std = prior_unstd/sum(prior_unstd),\n  \n  # the data\n  data = dbinom(6, size = 9, prob = prob),\n  \n  # the posterior\n  posterior_unstd = prior_std * data,\n  posterior = posterior_unstd / sum(posterior_unstd)\n)\n\n\ngrid |&gt; \n  ggplot(aes(prob, prior_std))+\n    geom_line()+\n    labs(y = \"prior density\",\n         title = \"Prior\") -&gt; \n  prior_plot\n\ngrid |&gt; \n  ggplot(aes(prob, data))+\n    geom_line()+\n    labs(y = \"data density\",\n         title = \"Data\") -&gt; \n  data_plot\n\ngrid |&gt; \n  ggplot(aes(prob, posterior))+\n    geom_line() +\n    labs(y = \"posterior density\",\n         title = \"Posterior\") -&gt; \n  posterior_plot\n\nprior_plot | data_plot | posterior_plot\n\n\n\n\nFigure¬†3: Prior, Data, Posterior\n\n\n\n\nSampling from the posterior, using sample_n().\n\ngrid |&gt; \n  sample_n(size = 1e4, \n           replace = T,\n           weight = posterior)-&gt;\n  posterior_samples\n\n\nhead(posterior_samples)\n\n# A tibble: 6 √ó 6\n   prob prior_unstd prior_std  data posterior_unstd posterior\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 0.703       0.444  0.000205 0.266       0.0000546  0.000419\n2 0.694       0.461  0.000213 0.269       0.0000574  0.000441\n3 0.670       0.507  0.000234 0.273       0.0000640  0.000491\n4 0.559       0.789  0.000365 0.220       0.0000803  0.000616\n5 0.620       0.619  0.000286 0.262       0.0000749  0.000575\n6 0.632       0.591  0.000273 0.267       0.0000729  0.000559\n\n\nI‚Äôm going to mess around with finessing the visualizations here.\n\nrenv::install(\"tidybayes\")\n\n\nlibrary(tidybayes)\n\n\nposterior_samples |&gt; \n  pull(prob) |&gt; \n  density() |&gt; \n  tidy() |&gt; \n  rename(prob = x, density = y) -&gt;\n  posterior_dens\n\nposterior_dens |&gt; \n  ggplot(aes(prob, density/max(density)))+\n    geom_area(fill = \"grey60\")+\n    geom_line(aes(y = posterior/max(posterior)),\n              linetype = 2,\n              data = grid)+\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†4: Comparison of the kernel density estimate vs the actual posterior distribution.\n\n\n\n\n\nposterior_samples |&gt; \n  median_hdci(prob, .width = c(0.5, 0.95)) -&gt;\n  intervals\nintervals |&gt; \n  gt() |&gt; \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n    \n  \n  \n    0.59\n0.48\n0.65\n0.50\nmedian\nhdci\n    0.59\n0.37\n0.85\n0.95\nmedian\nhdci\n  \n  \n  \n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(aes(fill = after_stat(pdf)), \n              fill_type = \"gradient\")+\n    scale_y_continuous(expand = expansion(mult = c(0,0)))+\n    khroma::scale_fill_batlow() +\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†5: Posterior density, colored according to the probability density function.\n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(x &gt;= 0.5)),\n      fill_type = \"gradient\"\n    ) +\n    scale_fill_manual(\n      values = c(\n        \"grey70\",\n        \"steelblue\"\n      )\n    )+\n   scale_y_continuous(expand = expansion(mult = c(0,0)))+\n   theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†6: The posterior density colored according to a critical value (0.5)\n\n\n\n\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_halfeye(\n      aes(fill = after_stat(level)),\n      fill_type = \"gradient\",\n      point_interval = \"median_hdi\"\n    ) +\n   scale_y_continuous(expand = expansion(mult = c(0.05,0)))+\n   scale_fill_manual(\n     values = c(\"steelblue\", \"steelblue4\")\n   )+\n   theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†7: Posterior samples, colored by their highest density interval levels.\n\n\n\n\nCan I get the plot into a {gt} table? I thin I‚Äôll need to map over the widths? I‚Äôm going off of this gt help page: https://gt.rstudio.com/reference/ggplot_image.html. Let me get the plot right first.\n\nposterior_samples |&gt; \n  ggplot(aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(level)),\n      .width = 0.66,\n      fill_type = \"gradient\",\n      point_interval = \"median_hdci\"\n    ) +\n    stat_slab(\n      fill = NA,\n      color = \"black\"\n    )+\n    scale_x_continuous(\n      limits = c(0,1),\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_y_continuous(\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_fill_manual(values = \"steelblue\", \n                      guide = \"none\")+\n    theme_void()\n\n\n\n\nFigure¬†8: Table figure experimentation\n\n\n\n\n\nmake_table_plot &lt;- function(.width, data) {\n  ggplot(data, aes(prob))+\n    stat_slab(\n      aes(fill = after_stat(level)),\n      .width = .width,\n      point_interval = \"median_hdci\"\n    ) +\n    stat_slab(\n      fill = NA,\n      color = \"black\"\n    )+\n    scale_x_continuous(\n      limits = c(0,1),\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_y_continuous(\n      expand = expansion(mult = c(0,0))\n    )+\n    scale_fill_manual(values = \"steelblue\", \n                      guide = \"none\")+\n    theme_void()\n}\n\nMap that function over the intervals table I made before.\n\nintervals |&gt; \n  mutate(\n    ggplot = map(.width, ~make_table_plot(.x, posterior_samples)),\n    \n    ## adding an empty column\n    dist = NA\n  ) -&gt; to_tibble\n\nto_tibble |&gt; \n  select(-ggplot) |&gt; \n  gt() |&gt; \n  text_transform(\n    locations = cells_body(columns = dist),\n    fn = \\(x) map(to_tibble$ggplot, ggplot_image, aspect_ratio = 2)\n  )\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n      dist\n    \n  \n  \n    0.587\n0.4815000\n0.6503000\n0.50\nmedian\nhdci\n\n    0.587\n0.3695874\n0.8489874\n0.95\nmedian\nhdci\n\n  \n  \n  \n\n\n\n\nI‚Äôd like more control over how the image appears in the table. Looks like I‚Äôll have to ggsave, and then embed.\n\nmake_custom_table_plot &lt;- function(p){\n  filename &lt;- tempfile(fileext = \".png\")\n  ggsave(plot = p, \n         filename = filename, \n         device = ragg::agg_png, \n         res = 100, \n         width =1.5,\n         height = 0.75)\n  local_image(filename=filename)\n}\n\n\nto_tibble |&gt; \n  select(-ggplot) |&gt; \n  gt() |&gt; \n  text_transform(\n    locations = cells_body(columns = vars(dist)),\n    fn = \\(x) map(to_tibble$ggplot, make_custom_table_plot)\n  )\n\n\n\n\n\n  \n    \n    \n      prob\n      .lower\n      .upper\n      .width\n      .point\n      .interval\n      dist\n    \n  \n  \n    0.587\n0.4815000\n0.6503000\n0.50\nmedian\nhdci\n\n    0.587\n0.3695874\n0.8489874\n0.95\nmedian\nhdci\n\n  \n  \n  \n\n\n\n\nThere we go!\nTurns out this local_image() thing doesn‚Äôt play nice with conversion to pdf (üòï)."
  },
  {
    "objectID": "posts/2023-05-15_04-sampling/index.html#brms",
    "href": "posts/2023-05-15_04-sampling/index.html#brms",
    "title": "Starting Sampling",
    "section": "BRMS",
    "text": "BRMS\n\nlibrary(brms)\n\n\ntibble(\n  water = 6,\n  samples = 9\n)-&gt; \n  water_to_model\n\n\nwater_form &lt;- bf(\n   water | trials(samples) ~ 1,\n   family = binomial(link = \"identity\")\n)\n\n\nbrm(\n  water | trials(samples) ~ 1,\n  data = water_to_model,\n  family = binomial(link = \"identity\"),\n  prior(beta(1, 1), class = Intercept, ub = 1, lb = 0),\n  file_refit = \"on_change\",\n  file = \"water_fit.rds\"\n) -&gt;\n  water_model\n\n\nwater_model\n\n Family: binomial \n  Links: mu = identity \nFormula: water | trials(samples) ~ 1 \n   Data: water_to_model (Number of observations: 1) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.64      0.14     0.35     0.88 1.00     1598     1945\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nlibrary(gtsummary)\n\n\nwater_model |&gt; \n  gtsummary::tbl_regression(intercept = T)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n    \n  \n  \n    (Intercept)\n0.64\n0.35, 0.88\n  \n  \n  \n    \n      1 CI = Credible Interval\n    \n  \n\n\n\n\nLet‚Äôs do this again.\n\nwater_model |&gt; \n  get_variables()\n\n[1] \"b_Intercept\"   \"lprior\"        \"lp__\"          \"accept_stat__\"\n[5] \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"  \n[9] \"energy__\"     \n\n\n\nwater_model |&gt; \n  gather_draws(b_Intercept)-&gt;\n  model_draws\n\nmodel_draws |&gt; \n  head() |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\nmodel_draws |&gt; \n  ggplot(aes(.value, .variable)) + \n    stat_halfeye(\n      point_interval = median_hdi,\n      aes(fill = after_stat(level)),\n      fill_type = \"gradient\"\n    ) +\n    xlim(0,1)+\n    scale_fill_manual(\n      values = c(\"steelblue4\", \"steelblue\"),\n    )"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html",
    "href": "posts/2023-06-06_08-linear-models-2/index.html",
    "title": "Linear models, part 2",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#loading",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#loading",
    "title": "Linear models, part 2",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(ggblend)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#finessing-the-model-diagram",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#finessing-the-model-diagram",
    "title": "Linear models, part 2",
    "section": "Finessing the model diagram",
    "text": "Finessing the model diagram\nI think I‚Äôve improved on the model diagram from the last post. Some things I‚Äôm still struggling with:\n\nFormatting the node text. Can‚Äôt seem to get either html nor markdown formatting to work.\nThe background color on edge labels is absent, making the ‚Äú~‚Äù illegible.\n\n\nflowchart TD\n  subgraph exp2[\"Exp\"]\n    one2[\"1\"]\n  end\n  exp2 -.-&gt; sigma4\n  \n  subgraph normal4[\"normal\"]\n    mu4[\"Œº=0\"]\n    sigma4[\"œÉ\"]\n  end\n  normal4 -.-&gt; Gamma[Œ≥j]\n  Gamma --&gt; gamma1\n  \n  subgraph normal3[\"normal\"]\n    mu3[\"Œº=0\"]\n    sigma3[\"œÉ=10\"]\n  end\n  normal3 -.-&gt; beta1\n  \n  subgraph normal2[\"normal\"]\n    mu2[\"Œº=0\"]\n    sigma2[\"œÉ=10\"]\n  end\n  normal2 -.-&gt; beta0\n  \n  subgraph sum1[\"+\"]\n    beta0[\"Œ≤‚ÇÄ\"]\n    subgraph mult1[\"√ó\"]\n      beta1[\"Œ≤‚ÇÅ\"]\n      x[\"x·µ¢\"]\n    end\n    gamma1[\"Œ≥j[i]\"]\n  end\n  \n  sum1 --&gt; mu1\n  \n  subgraph exp1[\"Exp\"]\n    one1[1]\n  end\n  exp1 -.-&gt;|\"~\"| sigma1\n  \n  subgraph normal1[\"normal\"]\n    mu1[\"Œº·µ¢\"]\n    sigma1[\"œÉ\"]\n  end\n  \n  normal1 -.-&gt;|\"~\"| y[\"y·µ¢\"]\n\n\n\nflowchart TD\n  subgraph exp2[\"Exp\"]\n    one2[\"1\"]\n  end\n  exp2 -.-&gt; sigma4\n  \n  subgraph normal4[\"normal\"]\n    mu4[\"Œº=0\"]\n    sigma4[\"œÉ\"]\n  end\n  normal4 -.-&gt; Gamma[Œ≥j]\n  Gamma --&gt; gamma1\n  \n  subgraph normal3[\"normal\"]\n    mu3[\"Œº=0\"]\n    sigma3[\"œÉ=10\"]\n  end\n  normal3 -.-&gt; beta1\n  \n  subgraph normal2[\"normal\"]\n    mu2[\"Œº=0\"]\n    sigma2[\"œÉ=10\"]\n  end\n  normal2 -.-&gt; beta0\n  \n  subgraph sum1[\"+\"]\n    beta0[\"Œ≤‚ÇÄ\"]\n    subgraph mult1[\"√ó\"]\n      beta1[\"Œ≤‚ÇÅ\"]\n      x[\"x·µ¢\"]\n    end\n    gamma1[\"Œ≥j[i]\"]\n  end\n  \n  sum1 --&gt; mu1\n  \n  subgraph exp1[\"Exp\"]\n    one1[1]\n  end\n  exp1 -.-&gt;|\"~\"| sigma1\n  \n  subgraph normal1[\"normal\"]\n    mu1[\"Œº·µ¢\"]\n    sigma1[\"œÉ\"]\n  end\n  \n  normal1 -.-&gt;|\"~\"| y[\"y·µ¢\"]\n\n\n\n\n\nHere‚Äôs the global water model. I‚Äôll replace the \\(\\mathcal{U}(0,1)\\) prior with the equivalent beta distribution, just for the consistency of going beta -&gt; binomial/bernoulli. I‚Äôll also notate the beta distribution with mean and precision.\n\\[\n\\mathcal{U}(0,1) = \\text{Beta}(a=1, b=1) = \\text{Beta}(\\mu=0.5, \\phi=2)\n\\]\nbecause\n\\[\na = \\mu\\phi\n\\]\n\\[\nb = (1-\\mu)\\phi\n\\]\n\n\n\n\nflowchart TD\n\nsubgraph beta1[\"beta\"]\n  mu[\"Œº=0.5\"]\n  phi[\"œï=2\"]\nend\nbeta1 -.-&gt; p\n\nsubgraph binomial1[\"binomial\"]\n  N\n  p\nend\n\nbinomial1 -.-&gt; W"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#height-data",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#height-data",
    "title": "Linear models, part 2",
    "section": "Height data",
    "text": "Height data\n{cmdstanr} is a dependency for {rmcelreath/rethinking}, and I don‚Äôt want to deal with that right now, so I‚Äôm just going to read the data from github.\n\nread_delim(\n  \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\", \n  delim = \";\"\n) -&gt;\n  Howell1\n\n\nlibrary(gt)\nlibrary(gtsummary)\n\n{gtsummary} has a summary table function that‚Äôs pretty ok. Not sure how to incorporate histograms into it like rethinking::precis().\n\nHowell1 |&gt; \n  tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 5441\n    \n  \n  \n    height\n149 (125, 157)\n    weight\n40 (22, 47)\n    age\n27 (12, 43)\n    male\n257 (47%)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nI‚Äôll get histograms with some pivoting and ggdist::stat_slab(density=\"histogram\")\n\nHowell1 |&gt; \n  mutate(row = row_number()) |&gt; \n  pivot_longer(\n    -row,\n    names_to = \"variable\",\n    values_to = \"value\"\n    ) |&gt; \n  ggplot(aes(value))+\n    stat_slab(\n      normalize = \"panels\", \n      density = \"histogram\"\n    )+\n    facet_wrap(\n      ~variable, \n      scales = \"free\"\n    )+\n    theme_no_y()\n\n\n\n\nHeight has a pretty long leftward tail because children are included in the data.\n\nHowell1 |&gt; \n  ggplot(aes(height, factor(male)))+\n    stat_slab()\n\n\n\n\n\nHowell1 |&gt; \n  ggplot(aes(age, height, color = factor(male)))+\n    geom_point()+\n    stat_smooth(method = \"gam\", formula = y ~ s(x, bs = 'cs'))\n\n\n\n\n\nAside, experimenting with {marginaleffects}\nThe Rethinking book just cuts the age at 18, but the trend for men and women in the figure above looks like it‚Äôs still increasing until at least 25. I‚Äôll mess around with marginaleffects::slopes() to see when the growth trend really stops.\n\nlibrary(mgcv)\nlibrary(marginaleffects)\n\nmgcv::gam() doesn‚Äôt like it when the s(by=‚Ä¶) argument isn‚Äôt a factor, so preparing for modelling.\n\nHowell1 |&gt; \n  mutate(male = factor(male)) -&gt;\n  height_to_mod\n\n\nmod &lt;- gam(height ~ male + s(age, by = male), data = height_to_mod)\n\nI‚Äôd have to double check the documentation for how to specify which variable you want the slope across, but I know how to do it with a new dataframe, so I‚Äôll just do that and filter. I set eps to 1, which I think will estimate the number of centimeters per year.\n\nslopes(\n  mod,\n  eps = 1,\n  newdata = datagrid(\n    age = 0:80,\n    male = c(0,1)\n  )\n) |&gt; \n  as_tibble() |&gt; \n  filter(term == \"age\") -&gt;\n  age_slopes\n\nage_slopes |&gt; \n  ggplot(aes(age, estimate, color = male))+\n    geom_ribbon(\n      aes(\n        ymin = conf.low,\n        ymax = conf.high,\n        fill = male\n      ),\n      alpha = 0.5\n    )\n\n\n\n\nAs a quick and dirty heuristic, I‚Äôll just check what the earliest age is that the high and low sides of the confidence interval have different signs.\n\nage_slopes |&gt; \n  filter(sign(conf.low) != sign(conf.high))  |&gt; \n  arrange(age) |&gt; \n  group_by(male) |&gt; \n  slice(1) |&gt; \n  select(term, age, male, estimate, conf.low, conf.high)\n\n# A tibble: 2 √ó 6\n# Groups:   male [2]\n  term    age male  estimate conf.low conf.high\n  &lt;chr&gt; &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 age      24 0        0.258  -0.0508     0.567\n2 age      28 1        0.176  -0.103      0.455\n\n\nLooks like the age women probably stopped growing is ~24 and for men ~28. So I‚Äôll filter the data for age &gt;= 30 just to be safe."
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#height-normality",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#height-normality",
    "title": "Linear models, part 2",
    "section": "Height normality",
    "text": "Height normality\n\nstable_height &lt;- Howell1 |&gt; \n  filter(age &gt;= 30)\n\n\nstable_height |&gt; \n  ggplot(aes(height))+\n    stat_slab()\n\n\n\n\n\nstable_height |&gt; \n  ggplot(aes(height, factor(male)))+\n    stat_slab()"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#the-model",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#the-model",
    "title": "Linear models, part 2",
    "section": "The Model",
    "text": "The Model\nRethinking gives the following model specification.\n\\[\nh_i \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\n\\[\n\\mu \\sim \\mathcal{N}(178, 20)\n\\]\n\\[\n\\sigma \\sim \\mathcal{U}(0,50)\n\\]\n\n\n\n\nflowchart TD\n\nsubgraph uniform1[\"uniform\"]\n  a[\"a=0\"]\n  b[\"b=50\"]\nend\nuniform1 -.-&gt; sigma1\n\nsubgraph normal2[\"normal\"]\n  mu2[\"Œº=178\"]\n  sigma2[\"œÉ=20\"]\nend\nnormal2 -.-&gt; mu1\n\nsubgraph normal1[\"normal\"]\n  mu1[\"Œº\"]\n  sigma1[\"œÉ\"]\nend\n\nnormal1 -.-&gt; h[\"h·µ¢\"]\n\n\n\n\n\nJust for some heuristics, I‚Äôll calculate the mean, standard error of the mean, and standard deviation of the data.\n\nstable_height |&gt; \n  summarise(\n     mean = mean(height),\n     sd = sd(height),\n     sem = sd/sqrt(n())\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(decimals = 1)\n\n\n\n\n\n  \n    \n    \n      mean\n      sd\n      sem\n    \n  \n  \n    154.6\n7.8\n0.5\n  \n  \n  \n\n\n\n\nSo, the \\(\\sigma\\) for the hyperprior is much higher than the standard error, which is good, cause I guess we‚Äôd want our prior to be looser than the uncertainty we have about the sample mean.\nI think I‚Äôd like to look at our sample estimates and how they compare to the priors.\n\nbind_rows(\n  tibble(\n    x = seq(118, 238, length = 100),\n    dens = dnorm(\n      x,\n      mean = 178,\n      sd = 20\n    ),\n    param = \"mu\"\n  ),\n  tibble(\n    x = seq(0, 50, length = 100),\n    dens = dunif(x, 0, 50),\n    param = \"sigma\"\n  )\n)-&gt;\n  model_priors\n\nbind_rows(\n  tibble(\n    param = \"mu\",\n    x = 154.6,\n    dens = dnorm(\n      x,\n      mean = 178,\n      sd = 20\n    ),\n  ),\n  tibble(\n    param = \"sigma\",\n    x = 7.8,\n    dens = dunif(x, 0, 50)\n  )\n)-&gt;\n  sample_estimates\n\n\nmodel_priors |&gt; \n  ggplot(aes(x, dens))+\n    geom_area(fill = \"grey80\")+\n    geom_point(\n      data = sample_estimates,\n      size = 3\n    )+\n    geom_segment(\n      data = sample_estimates,\n      aes(\n        xend = x,\n        yend = 0\n      ),\n      linewidth = 1\n    )+\n    facet_wrap(\n      ~param, \n      scales = \"free_x\"\n      )+\n    theme_no_y()\n\n\n\n\nI‚Äôll try setting up the priors like they are in the book without looking at Solomon Kurz‚Äô translation, then double check I did it right.\n\nlibrary(brms)\n\nI know that you can set up a model formula with just bf().\n\nheight_formula &lt;- bf(\n  height ~ 1\n)\n\nAnd I know you can get a table of the default priors it plans to use with get_prior().\n\nget_prior(height_formula, data = stable_height) |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    student_t(3, 153.7, 9.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n    student_t(3, 0, 9.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n  \n  \n  \n\n\n\n\n{ggdist} has a way of parsing and plotting these distributions pretty directly, but to get it how I want it to be requires getting a little hacky with ggplot2.\n\n\nCode\nget_prior(height_formula, data = stable_height) |&gt; \n  parse_dist(prior) |&gt; \n  ggplot(aes(dist = .dist, args = .args))+\n    stat_slab(aes(fill = after_stat(y&gt;0)))+\n    facet_wrap(~class, scales = \"free_x\")+\n    scale_fill_manual(\n      values = c(\"#ffffff00\", ptol_blue),\n      guide = \"none\")+\n    coord_flip()\n\n\n\n\n\nAnyway, to set up the priors like it is in the book, we need to do this. (Note from future Joe: I‚Äôd gotten this close, but had messed up how non-standard evaluation works and had to check Solomon Kurz‚Äô book. e.g, there‚Äôs no function called normal()).\n\nc(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  )\n) -&gt; example_priors\n\nexample_priors |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    normal(178, 20)\nIntercept\n\n\n\n\n\nNA\nNA\nuser\n    uniform(0, 50)\nsigma\n\n\n\n\n\n0\n50\nuser\n  \n  \n  \n\n\n\n\n\nbrm(\n  height_formula,\n  prior = example_priors,\n  family = gaussian,\n  data = stable_height,\n  sample_prior = T,\n  file = \"height_mod.rds\"\n) -&gt;\n  height_mod\n\n\nheight_mod\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: stable_height (Number of observations: 251) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.50   153.64   155.60 1.00     3070     2230\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.88      0.36     7.20     8.63 1.00     3523     2251\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWell! Estimated parameters here are basically right on top of the maximum likelihood estimates from the sample, including the standard error of the Intercept.\nHaving read over the marginaleffects book, I know I can get posterior draws of the predictions with predictions() |&gt; posterior_draws()\n\npredictions(\n  height_mod,\n  newdata = datagrid()\n) |&gt; \n  posterior_draws() |&gt; \n  ggplot(aes(draw))+\n    stat_slabinterval()+\n    theme_no_y()\n\n\n\n\nSince this was an intercept-only model, this is basically a distribution of the estimate for the intercept, rather than predicted observed values. So I can actually compare this to the prior.\n\npredictions(\n  height_mod,\n  newdata = datagrid()\n) |&gt; \n  posterior_draws() |&gt; \n  ggplot(aes(draw))+\n    stat_slabinterval()+\n    geom_line(\n      data = model_priors |&gt; \n        filter(param == \"mu\"),\n      aes(\n        x = x,\n        y = dens/max(dens),\n        )\n    )+\n    theme_no_y()\n\n\n\n\nTo compare the predicted observed values from the model to the actual data, we can use brms::pp_check().\n\npp_check(height_mod)+\n  khroma::scale_color_bright()\n\n\n\n\n\nGeneral look at parameters\nTo get the posterior samples of the parameters, I think we need to turn to tidybayes.\n\nlibrary(tidybayes)\n\nTo get the parameter names that we want to get samples from, tidybayes::get_variables() on the model.\n\nget_variables(height_mod)\n\n [1] \"b_Intercept\"     \"sigma\"           \"prior_Intercept\" \"prior_sigma\"    \n [5] \"lprior\"          \"lp__\"            \"accept_stat__\"   \"stepsize__\"     \n [9] \"treedepth__\"     \"n_leapfrog__\"    \"divergent__\"     \"energy__\"       \n\n\nThe non standard evaluation here still kind of freaks me out. I‚Äôll use spread_draws() which will put the posterior draw for each parameter in its own column.\n\nheight_mod |&gt; \n  spread_draws(\n   b_Intercept,\n   sigma\n  ) -&gt;\n  height_param_wide\n\nhead(height_param_wide)\n\n# A tibble: 6 √ó 5\n  .chain .iteration .draw b_Intercept sigma\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1      1          1     1        155.  8.18\n2      1          2     2        155.  7.51\n3      1          3     3        155.  7.91\n4      1          4     4        155.  7.77\n5      1          5     5        154.  7.77\n6      1          6     6        154.  7.77\n\n\n\nheight_param_wide |&gt; \n  ggplot(aes(b_Intercept, sigma))+\n    geom_point()+\n    theme(aspect.ratio = 1)\n\n\n\n\nTo get the parameters long-wise, we need to use gather_draws(). I‚Äôm assuming the function names for {tidybayes} were settled in back when the pivoting functions in {tidyr} were still gather() and spread().1\n\nheight_mod |&gt; \n  gather_draws(\n   b_Intercept,\n   sigma\n  ) -&gt;\n  height_param_long\n\nhead(height_param_long)\n\n# A tibble: 6 √ó 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable   .value\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1      1          1     1 b_Intercept   155.\n2      1          2     2 b_Intercept   155.\n3      1          3     3 b_Intercept   155.\n4      1          4     4 b_Intercept   155.\n5      1          5     5 b_Intercept   154.\n6      1          6     6 b_Intercept   154.\n\n\n\nheight_param_long |&gt; \n  ggplot(\n    aes(.value,)\n  )+\n    stat_slab()+\n    theme_no_y()+\n    facet_wrap(\n      ~.variable,\n      scales = \"free_x\"\n    )\n\n\n\n\n\nlibrary(ggdensity)\n\n\nheight_param_wide |&gt;  \n  ggplot(aes(b_Intercept, sigma))+\n    stat_hdr(fill = ptol_blue)+\n    theme(aspect.ratio = 1)"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#the-linear-model",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#the-linear-model",
    "title": "Linear models, part 2",
    "section": "The linear model",
    "text": "The linear model\nThe next thing the book moves onto is modelling height with weight.\n\nstable_height |&gt; \n  ggplot(aes(weight, height))+\n    geom_point()\n\n\n\n\nWe‚Äôre going to standardize the weight measure. That way, the intercept & prior for the intercept will be defined at the mean weight.\n\nstable_height |&gt; \n  mutate(\n    weight0 = weight-mean(weight)\n  )-&gt;\n  height_to_mod\n\n\nheight_weight_formula &lt;- bf(\n  height ~ 1 + weight0\n)\n\nGet the default priors\n\nget_prior(\n  height_weight_formula,\n  data = height_to_mod\n) |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    \nb\n\n\n\n\n\n\n\ndefault\n    \nb\nweight0\n\n\n\n\n\n\ndefault\n    student_t(3, 153.7, 9.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n    student_t(3, 0, 9.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n  \n  \n  \n\n\n\n\nDefine our custom priors, based on the first model in the book.\n\nheight_weight_priors &lt;- c(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  ),\n  prior(\n    prior = normal(0,10),\n    class = b\n  )\n)\n\nheight_weight_priors |&gt; \n  gt()\n\n\n\n\n\n  \n    \n    \n      prior\n      class\n      coef\n      group\n      resp\n      dpar\n      nlpar\n      lb\n      ub\n      source\n    \n  \n  \n    normal(178, 20)\nIntercept\n\n\n\n\n\nNA\nNA\nuser\n    uniform(0, 50)\nsigma\n\n\n\n\n\n0\n50\nuser\n    normal(0, 10)\nb\n\n\n\n\n\nNA\nNA\nuser\n  \n  \n  \n\n\n\n\n\n\nCode\nheight_weight_priors |&gt; \n  parse_dist(prior) |&gt; \n  ggplot(aes(dist = .dist, args = .args))+\n    stat_slab()+\n    facet_wrap(~class, scales = \"free_x\")+\n    coord_flip()\n\n\n\n\n\n\nbrm(\n  height_weight_formula,\n  prior = height_weight_priors,\n  data = height_to_mod, \n  file = \"height_weight_mod.rds\",\n  file_refit = \"on_change\"\n) -&gt;\n  height_weight_mod\n\n\nheight_weight_mod\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight0 \n   Data: height_to_mod (Number of observations: 251) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.58      0.33   153.93   155.23 1.00     3643     2923\nweight0       0.91      0.05     0.81     1.01 1.00     3978     2773\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.16      0.23     4.73     5.66 1.00     3752     2830\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere‚Äôs the usual kind of ‚Äúfit + credible interval‚Äù plot.\n\nheight_weight_mod |&gt; \n  predictions(\n    newdata = datagrid(\n      weight0 = seq(-13, 18, length = 100)\n    )\n  ) |&gt; \n  posterior_draws() |&gt; \n  mutate(\n    weight = weight0 + mean(height_to_mod$weight)\n  ) |&gt; \n  ggplot(\n    aes(weight, draw)\n  )+\n    stat_lineribbon()+\n    labs(\n      y = \"height\"\n    )+\n    scale_fill_brewer(palette = \"Blues\")\n\n\n\n\nHere‚Äôs the ‚Äúall of the predicted fitted lines‚Äù plot.\n\nheight_weight_mod |&gt; \n  predictions(\n    newdata = datagrid(\n      weight0 = seq(-13, 18, length = 100)\n    )\n  ) |&gt; \n  posterior_draws() |&gt; \n  mutate(\n    weight = weight0 + mean(height_to_mod$weight)\n  ) |&gt; \n  filter(\n    as.numeric(drawid) &lt;= 100\n  ) |&gt; \n  ggplot(\n    aes(weight, draw)\n  )+\n    geom_line(\n      aes(group = drawid),\n      alpha = 0.1\n    )+\n    labs(\n      y = \"height\"\n    )\n\n\n\n\n\npp_check(height_weight_mod)+\n  khroma::scale_color_bright()\n\n\n\n\n\nheight_weight_mod |&gt; \n  get_variables()\n\n [1] \"b_Intercept\"   \"b_weight0\"     \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nheight_weight_mod |&gt; \n  spread_draws(\n    `b_.*`,\n    sigma,\n    regex = T\n  ) |&gt; \n  ggplot(aes(b_Intercept, b_weight0))+\n    stat_hdr()+\n    theme(aspect.ratio = 1)"
  },
  {
    "objectID": "posts/2023-06-06_08-linear-models-2/index.html#footnotes",
    "href": "posts/2023-06-06_08-linear-models-2/index.html#footnotes",
    "title": "Linear models, part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI still have a soft spot for reshape2::melt() and reshape2::cast().‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-05-09_01-golem-of-prague/index.html",
    "href": "posts/2023-05-09_01-golem-of-prague/index.html",
    "title": "01 Golem of Prague Chapter",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-05-09_01-golem-of-prague/index.html#hypotheses-models-statistics",
    "href": "posts/2023-05-09_01-golem-of-prague/index.html#hypotheses-models-statistics",
    "title": "01 Golem of Prague Chapter",
    "section": "Hypotheses != Models != Statistics",
    "text": "Hypotheses != Models != Statistics\n\n\n\n\nflowchart LR\n  H0(\"H0&lt;br&gt;Evolution is Neutral\") &lt;--&gt; P0A[\"Process&lt;br&gt;Neutral Equilibrium\"] \n  H0 &lt;--&gt; P0B[\"Process&lt;br&gt;Neutral Non-Equilibrium\"]\n  H1(\"H1&lt;br&gt;Selection Matters\") &lt;--&gt; P1A[\"Process&lt;br&gt;Constant Selection\"]\n  H1 &lt;--&gt; P1B[\"Process&lt;br&gt;Fluctuating Selection\"]\n  \n  P0A &lt;--&gt; MII([\"Model2&lt;br&gt;Power Law\"])\n  P1B &lt;--&gt; MII\n  P1A &lt;--&gt; MIII([\"Model3&lt;br&gt;'something different'\"])\n  P0B &lt;--&gt; MI([\"Model1&lt;br&gt;another thing\"])\n\n\n\n\n\nRejecting Model 1 does not result in a unique identification of a process, or even a hypothesis."
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html",
    "href": "posts/2023-06-14_09-reporting/index.html",
    "title": "Reporting a linear model",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#loading",
    "href": "posts/2023-06-14_09-reporting/index.html#loading",
    "title": "Reporting a linear model",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(gt)\n\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(broom.mixed)\n\nsource(here::here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#the-plan",
    "href": "posts/2023-06-14_09-reporting/index.html#the-plan",
    "title": "Reporting a linear model",
    "section": "The plan",
    "text": "The plan\nPart of why I‚Äôm working through Statistical Rethinking as a blog is so that I can take some time and mess around with finessing how I‚Äôll visualize and report models like this, so I‚Äôm going to try to work over these basic models I just fit.\n\nData loading, prep, and model fits\n\nread_delim(\n  \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\", \n  delim = \";\"\n) -&gt;\n  Howell1\n\n\nstable_height &lt;- Howell1 |&gt; \n  filter(age &gt;= 30)\n\nstable_height |&gt; \n  mutate(\n    weight0 = weight-mean(weight)\n  )-&gt;\n  height_to_mod\n\n\n\n\n\n\n\nheight_mod\n\n\n\n\n\n\nheight_formula &lt;- bf(\n  height ~ 1\n)\n\n\nc(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  )\n) -&gt;\n  height_mod_priors\n\n\nbrm(\n  height_formula,\n  prior = height_mod_priors,\n  family = gaussian,\n  data = stable_height,\n  sample_prior = T,\n  save_pars = save_pars(all = TRUE),\n  file = \"height_mod.rds\",\n  file_refit = \"on_change\"\n) -&gt;\n  height_mod\n\n\n\n\n\n\n\n\n\n\nheight_weight_mod\n\n\n\n\n\n\nheight_weight_formula &lt;- bf(\n  height ~ 1 + weight0\n)\n\n\nheight_weight_priors &lt;- c(\n  prior(\n    prior = normal(178, 20),\n    class = Intercept\n  ),\n  prior(\n    prior = uniform(0,50),\n    lb = 0,\n    ub = 50,\n    class = sigma\n  ),\n  prior(\n    prior = normal(0,10),\n    class = b\n  )\n)\n\n\nbrm(\n  height_weight_formula,\n  prior = height_weight_priors,\n  data = height_to_mod, \n  file = \"height_weight_mod.rds\",\n  save_pars = save_pars(all = TRUE),\n  file_refit = \"on_change\"\n) -&gt;\n  height_weight_mod"
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#first-look---model-fit",
    "href": "posts/2023-06-14_09-reporting/index.html#first-look---model-fit",
    "title": "Reporting a linear model",
    "section": "First look - Model Fit",
    "text": "First look - Model Fit\n\nPosterior predictive check\nFirst I‚Äôll go with the default type of pp_check()\n\npp_check(height_mod, ndraws = 100)+\n  khroma::scale_color_bright()+\n  labs(\n    color = NULL,\n    title = \"height ~ 1\"\n  )+\n  theme_no_y()\n\n\n\n\nFigure¬†1: Posterior preditctive check for the height~1 model\n\n\n\n\nSo, the distribution of posterior predictions for the intercept only model puts a lot of probability where there‚Äôs an actual dip in the original data.\n\npp_check(height_weight_mod, ndraws = 100)+\n  khroma::scale_color_bright()+\n  labs(\n    color = NULL,\n    title = \"height ~ weight\"\n  )+\n  theme_no_y()\n\n\n\n\nFigure¬†2: Posterior predictive check for the height ~ weight model.\n\n\n\n\n\n\n\\(R^2\\)\nLet‚Äôs get some goodness of fit parameters. {brms} / {rstantools} have bayes_R2() which cites Gelman et al. (2019). Classic \\(R^2\\) is \\(1-\\frac{\\text{residuals variance}}{\\text{data variance}}\\). As Gelman et al. (2019) point out, there‚Äôs no one set of residuals, since the model parameters are all distributions rather than point estimates, so they propose an \\(R^2\\) for Bayesian models as \\(\\frac{\\text{variance of fitted values}}{\\text{variance of fitted values} + \\text{variance of residuals}}\\), for sampled fitted values and their respective residuals.\nBut, as they say\n\nA new issue then arises, though, when fitting a set of a models to a single dataset. Now that the denominator of \\(R^2\\) is no longer fixed, we can no longer interpret an increase in \\(R^2\\) as a improved fit to a fixed target.\n\nI‚Äôm glad I read the paper!\nAnyways, height_mod has an \\(R^2\\) of 0, as it should as an intercept only model.\n\nbayes_R2(height_mod)\n\n   Estimate Est.Error Q2.5 Q97.5\nR2        0         0    0     0\n\n\nI had to think for a second about how this made sense, but as an intercept only model, the predicted values for the data will be just a single number, equal to the intercept \\(\\mu\\).\n\npredictions(\n  height_mod \n) |&gt; \n  posterior_draws() -&gt;\n  height_fitted\n\nheight_fitted |&gt; \n  filter(drawid == \"1\") |&gt; \n  slice(1:6) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nI computed \\(R^2\\) by hand here. I‚Äôm a bit lost why the variance of the residuals is identical for every draw‚Ä¶\n\nheight_fitted |&gt; \n  mutate(resid = height - draw) |&gt; \n  group_by(drawid) |&gt; \n  summarise(\n    var_fit = var(draw),\n    var_resid = var(resid)\n  ) |&gt; \n  mutate(bayesr2 = var_fit/(var_fit+var_resid)) |&gt; \n  slice(1:6) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe \\(R^2\\) for the height~weight model is about 0.57\n\nbayes_R2(height_weight_mod)\n\n   Estimate  Est.Error      Q2.5    Q97.5\nR2 0.572246 0.02644878 0.5143232 0.618783\n\n\nLet‚Äôs try calculating that ‚Äúby hand‚Äù again.\n\npredictions(\n  height_weight_mod\n) |&gt; \n  posterior_draws() |&gt; \n  mutate(resid = height - draw) |&gt; \n  group_by(drawid) |&gt; \n  summarise(\n    var_fit = var(draw),\n    var_resid = var(resid)\n  ) |&gt; \n  mutate(bayesr2 = var_fit / (var_fit + var_resid)) |&gt; \n  mean_qi(bayesr2)\n\n# A tibble: 1 √ó 6\n  bayesr2 .lower .upper .width .point .interval\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1   0.572  0.514  0.619   0.95 mean   qi       \n\n\nCool. We can get this all from bayes_R2() also.\n\nbayes_R2(height_weight_mod, summary = F) |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(R2))+\n    stat_slab()+\n    scale_y_continuous(\n      expand = expansion(mult = 0)\n    )+\n    xlim(0,1)+\n    labs(\n      x = expression(R^2)\n    )+\n    theme_no_y()\n\n\n\n\nFigure¬†3: Estimate of Bayesian \\(R^2\\)\n\n\n\n\n\n\nloo\nOk‚Ä¶ Time to understand what loo() does, and what elpd means. Doing my best with Vehtari, Gelman, and Gabry (2016)\n\nelpd\n\nExpected Log Pointwise Predictive Density (we lost a ‚Äúp‚Äù somewhere).\n\n\nStarting with lpd (log pointwise predictive density). So \\(p(y_i|y)\\) is the probability of a data point \\(y_i\\) given the distribution of data \\(y\\). We log it, probably to keep things computable and addition based, and sum it up across every datapoint, \\(\\sum \\log p(y_i|y)\\). This is apparently equal to \\(\\sum \\log \\int p(y_i|\\theta)p(\\theta|y)d\\theta\\).\n\n\\(p(y_i|\\theta)\\) = the probability of each data point given the model\n\\(p(\\theta|y)\\) = the probability of the model given the data.\n\nOk, but \\(p(y_i | y)\\) is derived from probabilities over models that had seen \\(y_i\\). \\(p(y_i|y_{-i})\\) is the probability of data point \\(y_i\\) derived from a model that had not seen \\(y_i\\), a.k.a. ‚Äúleave one out‚Äù. ELPD is the summed up log probabilities across these leave-one-out models.\nAs best as I can tell, the rest of the paper is just about getting very clever about how to approximate \\(\\sum \\log p(y_i|y_{-i})\\) without needing to refit the model for each datapoint. It‚Äôs this cleverness that will sometimes result in a warning about ‚ÄúPareto k estimates‚Äù\nSo, without any further ado:\n\nloo(height_mod)\n\n\nComputed from 4000 by 251 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -874.6  8.9\np_loo         1.7  0.2\nlooic      1749.2 17.8\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nSo, if the leave-one-out probability of each data point was higher, the elpd_loo value would be closer to 0, aka exp(0)= 1.\n\nloo(height_weight_mod)\n\n\nComputed from 4000 by 251 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -768.9 12.9\np_loo         3.2  0.6\nlooic      1537.7 25.8\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nTo compare the two models:\n\nloo_compare(\n  loo(height_mod),\n  loo(height_weight_mod)\n)\n\n                  elpd_diff se_diff\nheight_weight_mod    0.0       0.0 \nheight_mod        -105.8      12.7 \n\n\nSo, the height-only model has a worse elpd. And we can be pretty sure it‚Äôs a worse elpd, because dividing it by the standard error of the difference is about -8, which according to the Stan discussion forums is a pretty big difference."
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#wrapping-it-into-a-report",
    "href": "posts/2023-06-14_09-reporting/index.html#wrapping-it-into-a-report",
    "title": "Reporting a linear model",
    "section": "Wrapping it into a report",
    "text": "Wrapping it into a report\nPosterior predictive checks of both models show considerable bimodality is not sufficiently captured by either the intercept-only model or the weight model.\n\n\nCode\nposterior_predict(height_mod) |&gt; \n  as.data.frame() |&gt; \n  mutate(.draw = row_number()) |&gt; \n  slice(1:100) |&gt; \n  pivot_longer(-.draw) |&gt; \n  mutate(model = \"height ~ 1\")-&gt;\n  height_pp\n\nposterior_predict(height_weight_mod) |&gt; \n  as.data.frame() |&gt; \n  mutate(.draw = row_number()) |&gt; \n  slice(1:100) |&gt; \n  pivot_longer(-.draw) |&gt; \n  mutate(model = \"height ~ weight\")-&gt;\n  height_weight_pp\n  \nheight_to_mod |&gt; \n  mutate(model = NULL)-&gt;\n  orig\n\nbind_rows(height_pp, height_weight_pp) |&gt; \n  ggplot(aes(value))+\n    stat_density(\n      aes(color = \"yrep\", group = .draw),\n      fill = NA,\n      position = \"identity\",\n      geom = \"line\",\n      alpha = 0.1\n    )+\n    stat_density(\n      data = orig,\n      aes(x = height, color = \"y\"),\n      fill = NA,\n      geom = \"line\",\n      linewidth = 1\n    )+\n    scale_y_continuous(expand = expansion(mult = 0))+\n    labs(\n      color = NULL\n    )+\n    facet_wrap(~model)+\n    theme_no_y()\n\n\n\n\n\nFigure¬†4: Posterior predictive checks for the two models\n\n\n\n\n\n\nCode\nbayes_R2(height_weight_mod, summary = F) |&gt; \n  as_tibble() |&gt; \n  mean_hdci(.width = 0.95) -&gt;\n  mod_r2\n\n\nThe intercept-only model necessarily has an \\(R^2\\) of 0. Mean Bayesian \\(R^2\\) for the weight model is 0.57 (95% highest density interval of [0.52, 0.62]).\nTable¬†1 displays model comparisons using Leave-One-Out Expected Log Pointwise Predictive Distribution (ELPD) (Vehtari, Gelman, and Gabry 2016).\n\n\nCode\nloo_compare(\n  loo(height_mod),\n  loo(height_weight_mod)\n) |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column() |&gt; \n  mutate(\n    model = case_when(\n      rowname == \"height_mod\" ~ \"height ~ 1\",\n      rowname == \"height_weight_mod\" ~ \"height ~ weight\"\n    )\n   ) |&gt; \n  select(model, elpd_diff, se_diff) |&gt; \n  mutate(ratio = elpd_diff/se_diff) |&gt; \n  gt() |&gt; \n    fmt_number() |&gt; \n    sub_missing() |&gt; \n    cols_label(\n      elpd_diff = \"ELPD difference\",\n      se_diff = \"difference SE\",\n      ratio  = \"diff/se\"\n    )\n\n\n\n\n\n\n\n  \n    \n    \n      model\n      ELPD difference\n      difference SE\n      diff/se\n    \n  \n  \n    height ~ weight\n0.00\n0.00\n‚Äî\n    height ~ 1\n‚àí105.75\n12.66\n‚àí8.35\n  \n  \n  \n\nTable¬†1:  Leave-One-Out Expected Log Pointwise Predictive Distribution\ncomparsion of the two models. ELPD difference contain the difference\nfrom the largest LOO ELPD."
  },
  {
    "objectID": "posts/2023-06-14_09-reporting/index.html#next-time",
    "href": "posts/2023-06-14_09-reporting/index.html#next-time",
    "title": "Reporting a linear model",
    "section": "Next time:",
    "text": "Next time:\nWriting up a report on the actual parameters."
  },
  {
    "objectID": "posts/2023-09-01_14-haunted-dag/index.html",
    "href": "posts/2023-09-01_14-haunted-dag/index.html",
    "title": "Haunted DAGSs",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-09-01_14-haunted-dag/index.html#setup",
    "href": "posts/2023-09-01_14-haunted-dag/index.html#setup",
    "title": "Haunted DAGSs",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(ggforce)\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(dagitty)\nlibrary(ggdag)\n\nsource(here::here(\"_defaults.r\"))\n\n\n\nset.seed(2023-9-1)"
  },
  {
    "objectID": "posts/2023-09-01_14-haunted-dag/index.html#newsworthiness-trustworthiness",
    "href": "posts/2023-09-01_14-haunted-dag/index.html#newsworthiness-trustworthiness",
    "title": "Haunted DAGSs",
    "section": "Newsworthiness & Trustworthiness",
    "text": "Newsworthiness & Trustworthiness\nThe first example in the book is about Berkson‚Äôs Paradox, which I believe is a kind of selection bias. The question is ‚ÄúWhy do so many research results that are newsworthy seem unreliable?‚Äù The idea being that funding (or whatever, maybe ‚Äúeditorial decisions to publish a paper‚Äù) is based jointly on its trustworthiness and its newsworthiness.\n\ntibble(\n  trustworthiness = rnorm(200),\n  newsworthiness = rnorm(200),\n  score = trustworthiness + newsworthiness,\n  score_percentile = ecdf(score)(score)\n) -&gt;\n  research\n\n\n\nCode\nresearch |&gt; \n  filter(\n    score_percentile &gt;= 0.9\n  )-&gt;\n  selected\n\nresearch |&gt; \n  ggplot(\n    aes(\n      newsworthiness,\n      trustworthiness\n    )\n  )+\n    geom_point()+\n    geom_mark_hull(\n      aes(\n        filter = score_percentile &gt;= 0.9,\n        color = \"selected\"\n      ),\n      fill = \"grey\"\n    )+\n    stat_smooth(\n      aes(\n        color = \"all\"\n      ),\n      method = lm\n    )+\n    stat_smooth(\n      data = selected,\n      aes(\n        color = \"selected\"\n      ),\n      method = lm\n    )+\n    coord_fixed()+\n    theme(\n      aspect.ratio = 1\n    )+\n    labs(\n      color = NULL\n    )\n\n\n\n\n\nFigure¬†1: The selection effect on the (non)correation between newsworthiness and trustworthiness"
  },
  {
    "objectID": "posts/2023-09-01_14-haunted-dag/index.html#multicollinearity",
    "href": "posts/2023-09-01_14-haunted-dag/index.html#multicollinearity",
    "title": "Haunted DAGSs",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nIn fact, there is nothing wrong with multicollinearity. The model will work fine for prediction. You will just be frustrated trying to understand it.\n\nWhen I was starting to get into advanced statistical modelling during my PhD (some time around 2010?) everyone suddenly learned about multicollinearity and got freaked out about it, so this was genuinely new info to me. See also Vanhove (2021), Collinearity isn‚Äôt a disease that needs curing.\nThe illustration from the book was about the relationship between total body height and leg length.\n\n\n\nleg-heigh simulation\n\ntibble(\n  n = 100,\n  # height in inches\n  height = rnorm(n, mean = 70, sd = 3),\n  # legs as a proportion of height\n  leg_prop = runif(n, 0.4, 0.5),\n  left_leg = height * leg_prop,\n  right_leg = left_leg + \n    rnorm(n, sd = 0.02)\n)-&gt;\n  height_legs\n\n\nThe simulated right leg length is going to be, max, ¬±0.06 (1‚ÅÑ16th inch) the left leg.\n\n\nCode\nheight_legs |&gt; \n  ggplot(aes(left_leg, height))+\n    geom_point()+\n    theme(aspect.ratio = 1)-&gt;\n  lh\n\nheight_legs |&gt; \n  ggplot(aes(right_leg, height))+\n    geom_point()+\n    theme(aspect.ratio = 1)-&gt;\n  rh\n\nheight_legs |&gt; \n  ggplot(aes(left_leg, right_leg))+\n    geom_point()+\n    theme(aspect.ratio = 1)-&gt;\n  lr\n\nlh + rh +lr\n\n\n\n\n\nFigure¬†2: data relationships\n\n\n\n\nA model with only left or right leg is going to be fine.\n\n\n\nleft leg model\n\nbrm(\n  height ~ left_leg,\n  data = height_legs,\n  prior = c(\n    prior(normal(70, 100), class = Intercept),\n    prior(normal(0, 10), class = b)\n  ),\n  backend = \"cmdstanr\",\n  file = \"height_left\"\n)-&gt;\n  height_left_mod\n\n\nThe right leg model:\n\n\n\nright leg model\n\nbrm(\n  height ~ right_leg,\n  data = height_legs,\n  prior = c(\n    prior(normal(70, 100), class = Intercept),\n    prior(normal(0, 10), class = b)\n  ),\n  backend = \"cmdstanr\",\n  file = \"height_right\"\n)-&gt;\n  height_right_mod\n\n\nI‚Äôm going to get a little fancy to get the parameter estimates from both models all at once.\n\n\n\ngetting parameter estimates\n\nlist(\n  left = height_left_mod,\n  right = height_right_mod\n) |&gt; \n  map(\n    ~ gather_draws(\n      .x, \n      `.*_leg`,\n      regex = T\n    )\n  ) |&gt; \n  list_rbind(\n    names_to = \"model\"\n  )-&gt;\n  rl_params\n\n\n\n\nCode\nrl_params |&gt; \n  ggplot(\n    aes(\n      .value, \n      .variable,\n    )\n  )+\n    stat_halfeye(\n      aes(\n        fill = model\n      )\n    )+\n  expand_limits(\n    x = 0\n  )+\n  ylim(\n    \"b_right_leg\",\n    \"b_left_leg\"\n  )\n\n\n\n\n\nFigure¬†3: Estimated leg parameter\n\n\n\n\nBut if we include both left and right leg, the estimate of each one‚Äôs parameter gets weird.\n\n\n\nboth leg model\n\nbrm(\n  height ~ left_leg + right_leg,\n  data = height_legs,\n  prior = c(\n    prior(normal(70, 100), class = Intercept),\n    prior(normal(0, 10), class = b)\n  ),\n  backend = \"cmdstanr\",\n  file = \"height_both\"\n)-&gt;\n  height_both_mod\n\n\n\n\n\ngetting parameter estimates\n\nlist(\n  left = height_left_mod,\n  right = height_right_mod,\n  both = height_both_mod\n) |&gt; \n  map(\n    ~ gather_draws(\n      .x, \n      `.*_leg`,\n      regex = T\n    )\n  ) |&gt; \n  list_rbind(\n    names_to = \"model\"\n  )-&gt;\n  all_params\n\n\n\n\nCode\nall_params |&gt; \n  mutate(\n    model = model |&gt;  \n      fct_relevel(\"both\", after = Inf)\n  ) |&gt; \n  ggplot(\n    aes(\n      .value, \n      .variable,\n    )\n  )+\n    stat_pointinterval(\n      aes(\n        color = model\n      ),\n      position = \"dodge\"\n    )+\n  expand_limits(\n    x = 0\n  )+\n  ylim(\n    \"b_right_leg\",\n    \"b_left_leg\"\n  )\n\n\n\n\n\nFigure¬†4: Oops, multicollinear!\n\n\n\n\nThis was the kind of outcome that I was taught to do things like residualize, but McElreath says that while the model we tried to write out was:\n\\[\n\\text{height} = \\text{Intercept} + \\beta_1\\text{left} + \\beta_2\\text{right}\n\\]\nbecause the right leg and the left leg are basically the same, we have something more like\n\\[\n\\text{height} = \\text{Intercept} + (\\beta_1 + \\beta_2)\\text{leg}\n\\]\nAnd because there‚Äôs nothing else in the model to specify what the value of \\(\\beta_1\\) and \\(\\beta_2\\) are, they‚Äôre all over the place. But crucially, they ought to add up to a similar value to what we got for just left_leg and right_leg in the first two models. They also should be negatively correlated, so when one is large and positive, the other should be large and negative, so they cancel out to around the values we got before.\n\n\n\nthe multicollinear estimates\n\nheight_both_mod |&gt; \n  spread_draws(\n    `.*_leg`,\n    regex = T\n  )-&gt;\n  leg_ests\n\n\n\n\nCode\nleg_ests |&gt; \n  ggplot(\n    aes(\n      b_left_leg,\n      b_right_leg\n    )\n  )+\n    geom_point(\n      alpha = 0.1\n    )+\n    coord_fixed()+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\n\nFigure¬†5: Correlated leg estimates\n\n\n\n\nWe can add each parameter together and compare it to the original two models.\n\n\n\nadding together multicollinear estimates\n\nleg_ests |&gt; \n  mutate(\n    .variable = \"b_leg\",\n    .value = b_left_leg + b_right_leg,\n    model = \"both\"\n  ) |&gt; \n  bind_rows(\n    rl_params\n  ) |&gt; \n  mutate(\n    model = fct_relevel(\n      model,\n      \"both\",\n      after = Inf\n    )\n  )-&gt;\n  rl_comp\n\n\n\n\nCode\nrl_comp |&gt; \n  ggplot(\n    aes(\n      .value,\n      .variable\n    )\n  )+\n    stat_halfeye(\n      aes(\n        fill = model\n      )\n    ) +\n  expand_limits(\n    x = 0\n  ) +\n  ylim(\n    \"b_leg\",\n    \"b_right_leg\",\n    \"b_left_leg\"\n  )\n\n\n\n\n\n\n‚ÄúThe model will work fine for prediction.‚Äù\nJust to hammer home the point that the predictive value of the multicollinear model, we can compare its posterior predictive checks to the left and right leg models.\n\n\n\nposterior predictive checks\n\npp_check(height_left_mod)+\n  labs(title = \"model: left\")-&gt;\n  left_pp\n\npp_check(height_right_mod)+\n  labs(title = \"model: right\") -&gt;\n  right_pp\n\npp_check(height_both_mod)+\n  labs(title = \"model: both\") -&gt;\n  both_pp\n\nleft_pp + right_pp + both_pp + plot_layout(guides = \"collect\")\n\n\n\n\n\nFigure¬†6: Posterior predictive checks\n\n\n\n\nWe can also compare their predictions for new leg length.\n\n\n\ngetting predicted values\n\ntibble(\n  left_leg = 32,\n  right_leg = left_leg+0.01\n)-&gt;\n  newleg\n\nlist(\n  left = height_left_mod,\n  right = height_right_mod,\n  both = height_both_mod\n) |&gt; \n  map(\n    ~ predictions(\n      .x, \n      newdata = newleg\n    ) |&gt; \n      posterior_draws()\n  ) |&gt; \n  list_rbind(\n    names_to = \"model\"\n  )-&gt;\n  all_pred\n\n\n\n\nCode\nall_pred |&gt; \n  ggplot(\n    aes(\n      draw,\n      model\n    )\n  )+\n    labs(\n      x = \"predicted height\"\n    )+\n    stat_halfeye()\n\n\n\n\n\nFigure¬†7: Predicted heights\n\n\n\n\n\n\nSo what to do?\nThe upshot of McElreath‚Äôs recommendation for what to do about all this multicollinearity is ‚Äúhave a bad time.‚Äù There‚Äôs no generic answer. Maybe there‚Äôs an acceptable way to specify the model depending on the DAG, but also maybe some questions aren‚Äôt well put, like ‚Äúwhat are the individual contribution of the left leg and the right leg to total height?‚Äù"
  },
  {
    "objectID": "posts/2023-09-07_confounding/index.html",
    "href": "posts/2023-09-07_confounding/index.html",
    "title": "Where I do a lot of work and don‚Äôt understand colliders any better.",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-09-07_confounding/index.html#setup",
    "href": "posts/2023-09-07_confounding/index.html#setup",
    "title": "Where I do a lot of work and don‚Äôt understand colliders any better.",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(ggdensity)\nlibrary(ggforce)\nlibrary(marginaleffects)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(khroma)\n\n\nsource(here::here(\"_defaults.r\"))\n\n\n\nset.seed(2023-9-7)"
  },
  {
    "objectID": "posts/2023-09-07_confounding/index.html#developing-a-sense-for-colliders",
    "href": "posts/2023-09-07_confounding/index.html#developing-a-sense-for-colliders",
    "title": "Where I do a lot of work and don‚Äôt understand colliders any better.",
    "section": "Developing a sense for colliders",
    "text": "Developing a sense for colliders\nFor me to really get a sense of how coliders work, I‚Äôm going to have to simulate a few different datasets, messing around with the parameters, and compare the outcomes. I won‚Äôt do this full Bayesian for the sake of speed. As a reminder, here‚Äôs the DAG. I‚Äôll specifically be messing around with the effect of n on p and c.\n\n\nCode\ndagify(\n  c ~ p + g + n,\n  p ~ g + n\n) |&gt; \n  tidy_dagitty() -&gt;\n  the_dag\n\nthe_dag |&gt; \n  mutate(\n    from_n = ifelse(name == \"n\", ptol_red, \"black\")\n  ) |&gt; \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(\n      aes(\n        shape = name == \"c\"\n      )\n    )+\n    geom_dag_edges(\n      aes(\n        edge_color = from_n\n      )\n    )+\n    geom_dag_text()+\n    scale_color_manual(\n      values = c(\"grey60\", ptol_red)\n    )+\n    guides(\n      shape = \"none\",\n      color = \"none\"\n    )+\n    theme_dag()\n\n\n\n\n\nFigure¬†1: DAG with a collider\n\n\n\n\nI refactored the code from before into a function:\n\n\nsimulation function\nsim_collider &lt;- function(\n    n = 200,\n    # Grandparent on parent\n    b_GP = 1,\n    # Parent on Child\n    b_PC = b_GP,\n    # Grandparent on Child\n    b_GC = 0,\n    # neighborhood\n    b_N = 2){\n  \n  tibble(\n    grandparent = rnorm(n),\n    neighborhood = rbinom(\n      n, \n      size = 1, \n      prob = 0.5\n    ),\n    parent = rnorm(\n      n,\n      mean = b_GP * grandparent + \n        b_N * neighborhood\n    ),\n    child = rnorm(\n      n,\n      mean = b_GC * grandparent +\n        b_PC * parent + \n        b_N  * neighborhood\n    )\n  ) -&gt;\n    haunted_sim\n  \n  return(haunted_sim)\n}\n\n\nAnd I‚Äôll do a grid of 100 values from -11 to 11 for b_N.\n\n\n\nsimulation parameters\n\ntibble(\n  n = 200,\n  b_GP = 1,\n  b_PC = 1,\n  b_GC = 0,\n  b_N = rep(seq(-11, 11, length = 100), 10)\n) -&gt; \n  sim_params\n\n\nNow, with some tidyverse fanciness, I‚Äôll map the simulation function I wrote across each row to get simulation datasets.\n\n\n\nSimulating the data with pmap\n\nsim_params |&gt; \n  rowwise() |&gt; \n  mutate(\n    data = pmap(\n      list(n, b_GP, b_PC, b_GC, b_N),\n      sim_collider\n    )\n  ) |&gt; \n  ungroup()-&gt;\n  sim_data\n\n\nThen, for each data set I‚Äôll fit\nlm(child ~ parent + grandparent)\nand then get the parameters.\n\n\n\nFitting a model for each simulation\n\nsim_data |&gt; \n  mutate(\n    model = map(\n      data, \n      ~lm(child ~ parent + grandparent, data = .x)\n    ),\n    params = map(model, broom::tidy)\n  ) |&gt; \n  unnest(params) |&gt; \n  select(\n    starts_with(\"b_\"),\n    term, \n    estimate\n  ) -&gt;\n  model_params\n\n\n\n\nCode\ntrue_params &lt;- tibble(\n  term = c(\"(Intercept)\", \"grandparent\", \"parent\"),\n  estimate = c(0, 0, 1)\n)\n\nmodel_params |&gt; \n  ggplot(\n    aes(\n      b_N,\n      estimate\n    )\n  )+\n  geom_hline(\n    data = true_params,\n    aes(\n      yintercept = estimate\n    ),\n    color = \"grey40\"\n  )+\n  stat_summary(\n    fun.y = mean,\n    geom = \"line\"\n  )+\n  facet_wrap(~term)+\n  labs(\n    caption = \"b_GC = 0; b_GP = 1; b_PC = 1\"\n  )\n\n\n\n\n\nFigure¬†2: Collider effect\n\n\n\n\nHuh. I guess I wasn‚Äôt expecting an asymptotic relationship for the grandparent and parent effects? It looks like as b_N gets large, the collider confounding reaches some kind of min/max, which for grandparent is -1, and for parent is 1. I don‚Äôt know if this value relates to either the effect of b_GP or b_PC, since both were set to 1? Maybe time for another grid search. I‚Äôll really max outt the b_N effect to get fully into the tail of the asymptote.\n\n\n\nOne big simulation\n\nexpand_grid(\n  n = 200,\n  b_GP = rep(-2:2, 10),\n  b_PC = rep(-2:2, 10),\n  b_GC = 0,\n  b_N = 50\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    data = pmap(\n      list(n, b_GP, b_PC, b_GC, b_N),\n      sim_collider\n    )\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(\n    model = map(\n      data, \n      ~lm(child ~ parent + grandparent, data = .x)\n    ),\n    params = map(model, broom::tidy)\n  ) |&gt; \n  unnest(params) |&gt; \n  select(\n    starts_with(\"b_\"),\n    term, \n    estimate\n  ) -&gt;\n  big_sim\n\n\n\n\nCode\nbig_sim |&gt; \n  filter(\n    term != \"(Intercept)\"\n  ) |&gt; \n  ggplot(\n    aes(\n      b_GP,\n      estimate\n    )\n  )+\n    stat_summary(\n      fun.y = mean,\n      geom = \"line\"\n    )+\n    scale_x_continuous(\n      breaks = c(-2, 0, 2)\n    )+\n    scale_y_continuous(\n      breaks = c(-2, 0, 2)\n    )+\n    facet_grid(term~b_PC, labeller = label_both)+\n    theme(\n      aspect.ratio = 1,\n      strip.text.y  = element_text(size = 8)\n    )+\n    labs(\n      caption = \"b_GC = 0; b_N = 50\"\n    )\n\n\n\n\n\nFigure¬†3: Collider effect\n\n\n\n\nHuh. The associations look straightforward, but I think I need an animation to get it.\n\nThis turned into a whole thing.\n\n\nCode\nlibrary(gganimate)\nnframes = 100\n\n\n\n\nCode\ncolor_value &lt;- function(x, min.v = -2, max.v = 2, scale = color(\"berlin\")(100)){\n  prop = (x - min.v)/(max.v-min.v)\n  closest_idx = round(prop * (length(scale)-1))+1\n  return(scale[closest_idx])\n}\n\n\n\n\nCode\ntibble(\n  name = \"p\",\n  to = \"c\",\n  true = 1,\n  est = true + 1,\n  id = seq(-2, 2, length = nframes),\n  col = color_value(true)\n) |&gt; \n  bind_rows(\n    tibble(\n      name = \"g\",\n      to = \"c\",\n      true = 0,\n      est = seq(2, -2, length = nframes),\n      id = seq(-2, 2, length = nframes),\n      col = color_value(true)\n    )\n  ) |&gt; \n  bind_rows(\n    tibble(\n      name = \"g\",\n      to = \"p\",\n      true = seq(-2, 2, length = nframes),\n      est = NA ,\n      id = seq(-2, 2, length = nframes),\n      col = color_value(true)\n    )\n  ) |&gt; \n  bind_rows(\n    tribble(\n      ~name, ~to, ~true, ~est,\n     \"n\", \"c\",  NA,  NA,\n     \"n\", \"p\",  NA,  NA,\n     \"c\", NA, NA, NA\n    ) |&gt; \n      mutate(\n        across(true:est, as.numeric),\n        col = \"#000000\"\n      ) |&gt; \n      group_by(name, to, true, est, col) |&gt; \n      reframe(\n        id = seq(-2, 2, length = nframes)\n      ) \n  )-&gt;\n  pc_true_anim\n\ntibble(\n  name = \"p\",\n  to = \"c\",\n  true = 1,\n  est = true + 1,\n  id = seq(-2, 2, length = nframes),\n  col = color_value(est)\n) |&gt; \n  bind_rows(\n    tibble(\n      name = \"g\",\n      to = \"c\",\n      true = 0,\n      est = seq(2, -2, length = nframes),\n      id = seq(-2, 2, length = nframes),\n      col = color_value(est)\n    )\n  ) -&gt;\n  pc_est_anim\n\n\n\n\nCode\npc_true_anim |&gt; \n  left_join(the_dag |&gt; as_tibble()) -&gt;\n  true_dag\n\npc_est_anim |&gt; \n  left_join(the_dag |&gt; as_tibble()) -&gt;\n  est_dag\n\n\n\n\nCode\ntrue_dag |&gt; \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(\n      color = \"grey\",\n    )+\n    geom_dag_text()+\n    geom_segment(\n      arrow = arrow(type = \"closed\", length = unit(0.2, \"cm\")),\n      linewidth = 1,\n      aes(\n        color = col\n      )\n    )+\n    geom_segment(\n      data = est_dag,\n      linetype = \"dashed\",\n      aes(\n        x = x+0.1, y = y+0.1, xend = xend+0.1, yend = yend+0.1,\n        color = col\n      ),\n      arrow = arrow(type = \"closed\", length = unit(0.2, \"cm\")),\n      linewidth = 1\n    )+\n    scale_color_identity()+\n    transition_time(\n      id\n    )+\n  labs(\n    title = \"b_GP: {round(frame_time, digits = 2)}\\nest_GC: {round(frame_time*-1, digits = 2)}\"\n  )+\n  theme_dag()-&gt;a\nanimate(a, rewind = T) |&gt; \n  anim_save(\n    filename = \"dag_anim.gif\"\n  )\n\n\n\n\n\nFigure¬†4: Animated DAG\n\n\nSo, as the true effect of Grandparents on parents changes, the estimated direct effect on children is inversely proportional, and, for some reason, the direct effect of parents is just +1?"
  },
  {
    "objectID": "posts/2023-07-13_13-dags3/index.html",
    "href": "posts/2023-07-13_13-dags3/index.html",
    "title": "DAGS part 3",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-07-13_13-dags3/index.html#setup",
    "href": "posts/2023-07-13_13-dags3/index.html#setup",
    "title": "DAGS part 3",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(marginaleffects)\nlibrary(dagitty)\nlibrary(ggdag)\n\nsource(here::here(\"_defaults.r\"))\n\n\n\nset.seed(2023-7-12)"
  },
  {
    "objectID": "posts/2023-07-13_13-dags3/index.html#the-getting-total-expected-effects",
    "href": "posts/2023-07-13_13-dags3/index.html#the-getting-total-expected-effects",
    "title": "DAGS part 3",
    "section": "The Getting Total Expected Effects",
    "text": "The Getting Total Expected Effects\nOk, for the milk data, where the neocortex percentage is correlated with bodymass, how would we get the expected effect of increasing body mass?\n\ndata(milk, package = \"rethinking\")\n\n\nmilk |&gt; \n  drop_na() |&gt; \n  mutate(\n    kcal_z = (kcal.per.g-mean(kcal.per.g))/sd(kcal.per.g),\n    neoc_z = (neocortex.perc-mean(neocortex.perc))/sd(neocortex.perc),\n    log_mass = log(mass),\n    log_mass_z = (log_mass - mean(log_mass))/sd(log_mass)\n  ) -&gt;\n  milk_to_mod\n\nWe‚Äôll have to commit to a DAG, so I‚Äôll go with this:\n\n\n\n\nflowchart LR\n  Mass --&gt; Neocortex\n  Mass --&gt; KCal\n  Neocortex --&gt; KCal\n\n\n\n\n\n\n\n\n\n\n\nThe DAG in Mermaid\n\n\n\n\n\nMaking the DAG in Mermaid was so much nicer!\nflowchart LR\n  Mass --&gt; Neocortex\n  Mass --&gt; KCal\n  Neocortex --&gt; KCal\n\n\n\nSo I‚Äôll use this formula:\n\nmnk_formula &lt;- bf(\n  kcal_z ~ log_mass_z + neoc_z\n) +\nbf(\n  neoc_z ~ log_mass_z\n) +\nset_rescor(FALSE)\n\nI‚Äôm still just superstitiously using set_rescor(), since I don‚Äôt really understand where it‚Äôs supposed to be used or not, but everyone seems to include it in these things.\n\nFitting the model.\n\nbrm(\n  formula = mnk_formula,\n  prior = c(\n    prior(normal(0,0.5), class = b, resp = kcalz),\n    prior(normal(0,0.5), class = b, resp = neocz),    \n    prior(normal(0,0.2), class = Intercept, resp = kcalz),\n    prior(normal(0,0.2), class = Intercept, resp = neocz),    \n    prior(exponential(1), class = sigma, resp = kcalz),\n    prior(exponential(1), class = sigma, resp = neocz) \n  ),\n  data = milk_to_mod,\n  cores = 4,\n  file = \"mnk_model.rds\",\n  backend = \"cmdstanr\"\n)-&gt;\n  mnk_model\n\n\nmnk_model\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: kcal_z ~ log_mass_z + neoc_z \n         neoc_z ~ log_mass_z \n   Data: milk_to_mod (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nkcalz_Intercept      0.00      0.14    -0.27     0.27 1.00     4345     2715\nneocz_Intercept     -0.00      0.13    -0.26     0.25 1.00     4328     2924\nkcalz_log_mass_z    -0.68      0.26    -1.17    -0.12 1.00     2730     2916\nkcalz_neoc_z         0.57      0.26     0.03     1.08 1.00     2662     2873\nneocz_log_mass_z     0.67      0.17     0.32     1.00 1.00     3994     2789\n\nFamily Specific Parameters: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_kcalz     0.81      0.17     0.56     1.21 1.00     3244     2719\nsigma_neocz     0.72      0.14     0.51     1.05 1.00     3922     2591\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nGetting the total effect of bodymass\nI think to work out the total effect of increasing body mass, I need to get the estimated neocortex size for each body mass, and then use that to get the estimated caloric value?\nI also don‚Äôt think the out-of-the-box functions like marginaleffects::predictions() or tidybayes::add_epred_draws() will do the trick either? We‚Äôll have to get analog.\n\nStep 1: Get all of the parameters\n\nmnk_model |&gt; \n  spread_draws(\n    `b_.*`,\n    regex = T\n  )-&gt;\n  mnk_parameters\n\ncolnames(mnk_parameters)\n\n[1] \".chain\"             \".iteration\"         \".draw\"             \n[4] \"b_kcalz_Intercept\"  \"b_neocz_Intercept\"  \"b_kcalz_log_mass_z\"\n[7] \"b_kcalz_neoc_z\"     \"b_neocz_log_mass_z\"\n\n\n\n\nStep 2: Get the predicted neocortex sizes\nOk, if log_mass_z = 0, the expected value of neocortex would just be the Intercept, or b_neocz_Intercept. I‚Äôll create a column called pred_neoc_m0 to mean ‚Äúpredicted neocortex percentage when mass = 0‚Äù. Then, I‚Äôll create a column called pred_neoc_m1 for ‚Äúpredicted neocortex percentage when mass = 1‚Äù\n\nmnk_parameters |&gt; \n  mutate(\n    pred_neoc_m0 = b_neocz_Intercept,\n    pred_neoc_m1 = b_neocz_Intercept + b_neocz_log_mass_z\n  )-&gt;\n  mnk_parameters\n\n\n\nStep 3: Get the predicted kcals\nOk, now to get the predicted kcal when body mass = 0, I‚Äôll have to multiply pred_neoc_mo0 by the neocortex slope for kcal.\n\nmnk_parameters |&gt; \n  mutate(\n    pred_kcal_m0 = b_kcalz_Intercept + (b_kcalz_neoc_z * pred_neoc_m0)\n  ) -&gt;\n  mnk_parameters\n\nNow to get the predicted kcal when mass = 1, I‚Äôll need to multiply the neocortex slope by the predicted neocortex size when mass = 1, and add the body mass slope.\n\nmnk_parameters |&gt; \n  mutate(\n    pred_kcal_m1 = b_kcalz_Intercept + (b_kcalz_neoc_z + pred_neoc_m1) + b_kcalz_log_mass_z\n  )-&gt;\n  mnk_parameters\n\nNow, subtract pred_kcal_m1 from pred_kcal_m0 to get the total effect.\n\nmnk_parameters |&gt; \n  mutate(\n    total_mass_effect = pred_kcal_m1 - pred_kcal_m0\n  ) -&gt;\n  mnk_parameters\n\n\n\nStep 4: Feel confused and pretty certain that you‚Äôve done it wrong\nNow, I‚Äôll compare the total effect vs the direct effect.\n\n\nCode\nmnk_parameters |&gt; \n  select(.draw, b_kcalz_log_mass_z, total_mass_effect) |&gt; \n  pivot_longer(-1) |&gt; \n  mutate(\n    name = case_match(\n      name,\n      \"b_kcalz_log_mass_z\" ~ \"direct effect\",\n      \"total_mass_effect\" ~ \"total effect\"\n    )\n  ) |&gt; \n  ggplot(aes(value, name, fill = name))+\n    stat_halfeye()+\n    labs(\n      fill = \"effect\",\n      y = NULL,\n      x = NULL,\n      title = \"effect of +1 body mass\"\n    )+\n    theme(legend.position = \"none\")\n\n\n\n\n\nWell, I‚Äôm a bit nervous about whether I‚Äôve done this right, especially since the estimated effect of body mass without including neocortex in the model was still on the negative side of 0.\nBut, if we take the DAG seriously, then increasing body mass pretty strongly increases neocortex percentage, and that pretty strongly increases the kcal of the milk‚Ä¶ so maybe this is right!"
  },
  {
    "objectID": "posts/2023-07-13_13-dags3/index.html#categorical-variables",
    "href": "posts/2023-07-13_13-dags3/index.html#categorical-variables",
    "title": "DAGS part 3",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nReturning to the height data to include some categorical predictors:\n\ndata(Howell1, package = \"rethinking\")\n\nHowell1 |&gt; \n  filter(\n    age &gt;= 18\n  ) |&gt; \n  mutate(\n    sex = case_match(\n      male,\n      0 ~ \"female\",\n      1 ~ \"male\",\n    ),\n    weight_z = (weight-mean(weight))/sd(weight)\n  ) -&gt;\n  height_to_model\n\nTo use the ‚Äúindexing‚Äù approach, I think we‚Äôll need to use the -1 formula syntax to remove the intercept.\n\nheight_formula = bf(\n  height ~ -1 + sex + weight\n)\n\nLet‚Äôs look at the default priors.\n\nget_prior(\n  height_formula,\n  data = height_to_model\n)\n\n                prior class      coef group resp dpar nlpar lb ub       source\n               (flat)     b                                            default\n               (flat)     b sexfemale                             (vectorized)\n               (flat)     b   sexmale                             (vectorized)\n               (flat)     b    weight                             (vectorized)\n student_t(3, 0, 8.5) sigma                                  0         default\n\n\nWell, it looks like there‚Äôs not a very easy way to set one prior over the parameters for sex and a different prior for the slope of weight. For now I‚Äôll just leave the priors at their defaults and see what I get.\n\nbrm(\n  height_formula,\n  data = height_to_model,\n  file = \"height_model.rds\",\n  cores = 4\n) -&gt;\n  height_model\n\n\nheight_model\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ -1 + sex + weight \n   Data: height_to_model (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsexfemale   122.74      1.70   119.46   126.01 1.00      835     1208\nsexmale     129.24      1.95   125.47   133.04 1.00      829     1217\nweight        0.64      0.04     0.56     0.72 1.00      820     1264\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.29      0.16     3.98     4.63 1.00     1552     1700\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nGetting Fancier\nI was going to do some more fancier modelling, looking at the effect of sex on weight, but I think if I was going to draw the DAG out, it would be something like this:\n\n\n\n\nflowchart LR\n  sex --&gt; height\n  height --&gt; weight\n  sex --&gt; weight\n\n\n\n\n\nIf we have to draw an arrow of causality between height and weight, it has to go height \\(\\rightarrow\\) weight. Cause taller people necessarily have more mass than shorter people, but people with more mass don‚Äôt necessarily be taller."
  },
  {
    "objectID": "posts/2023-05-09_00-setup/index.html",
    "href": "posts/2023-05-09_00-setup/index.html",
    "title": "Setup",
    "section": "",
    "text": "I‚Äôve set up the blog using the default quarto blog template in RStudio, also initializing a git repo and renv.\n\nrenv::install(c(\"tidyverse\", \"brms\"))\nrenv::install(c(\"coda\", \"mvtnorm\", \"dagitty\"))\n\nThe preface wants to install the book package with devtooks::install_github(), but I‚Äôm pretty sure that‚Äôs been superseded with remotes::install_github(), and renv::install().\n\ninstall.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nrenv::install(\"rmcelreath/rethinking\")"
  },
  {
    "objectID": "posts/2023-09-06_haunted-dag-3/index.html",
    "href": "posts/2023-09-06_haunted-dag-3/index.html",
    "title": "Colliders",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-09-06_haunted-dag-3/index.html#setup",
    "href": "posts/2023-09-06_haunted-dag-3/index.html#setup",
    "title": "Colliders",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(ggdensity)\nlibrary(ggforce)\nlibrary(marginaleffects)\nlibrary(dagitty)\nlibrary(ggdag)\n\n\nsource(here::here(\"_defaults.r\"))\n\n\n\nset.seed(2023-9-6)"
  },
  {
    "objectID": "posts/2023-09-06_haunted-dag-3/index.html#collider-bias",
    "href": "posts/2023-09-06_haunted-dag-3/index.html#collider-bias",
    "title": "Colliders",
    "section": "Collider Bias",
    "text": "Collider Bias\nWhen A has an effect on Z, and B has an effect on Z, then Z is a ‚Äúcollider‚Äù variable between A and B.\n\n\n\n\ngraph LR\n  a[\"A\"]\n  b[\"B\"]\n  z[\"Z\"]\n  \n  a --&gt; z\n  b --&gt; z\n\n\n\n\n\nSo, if I understand things right, fitting a model with\nA ~ B\nwould result in no effect of B on A, which we can check with a DAG.\n\ndagify(\n  Z ~ A,\n  Z ~ B\n) |&gt; \n  impliedConditionalIndependencies()\n\nA _||_ B\n\n\nBut, if we fit a model with\nA ~ B + Z\nnow there would suddenly seem to be an effect of B on A. As a simple demo, we can repeat the simulation of newsworthiness and trustworthiness.\n\n\n\nsimulating selection\n\ntibble(\n  trustworthiness = rnorm(200),\n  newsworthiness = rnorm(200),\n  score = trustworthiness + newsworthiness,\n  score_percentile = ecdf(score)(score),\n  selected = score_percentile &gt;= 0.9\n) -&gt;\n  research\n\n\n\n\nCode\nresearch |&gt; \n  ggplot(\n    aes(\n      newsworthiness,\n      trustworthiness,\n      color = selected\n    )\n  ) +\n    geom_point()+\n    theme(\n      aspect.ratio = NULL\n    )+\n    coord_fixed()\n\n\n\n\n\nFigure¬†1: Selection bias\n\n\n\n\nLast time we just fit a model using the subset of selected values. But here it goes modelling with the whole dataset. First, without the collider.\n\n\n\nno collider model\n\nbrm(\n  trustworthiness ~ newsworthiness,\n  data = research,\n  backend = 'cmdstanr',\n  file = \"research1\"\n)-&gt;\n  research1_mod\n\n\nThen, with the collider.\n\n\n\ncollider model\n\nbrm(\n  trustworthiness ~ newsworthiness + selected,\n  data = research,\n  backend = 'cmdstanr',\n  file = \"research2\"\n)-&gt;\n  research2_mod\n\n\n\n\n\ngetting parameters\n\nlist(\n  `no collider` = research1_mod,\n  collider = research2_mod\n) |&gt; \n  map(\n    ~.x |&gt; \n      gather_draws(\n        `b_.*`,\n        regex = T\n      )\n  ) |&gt; \n  list_rbind(\n    names_to = \"model\"\n  )-&gt;\n  collide_comp\n\n\nJust focusing on the intercept and newsworthiness effects, they went from (correctly) being unrelated in the model without the collider, to having pretty reliable effects by just including the selected variable.\n\n\nCode\ncollide_comp |&gt; \n  filter(\n    str_detect(\n      .variable,\n      \"selected\",\n      negate = T\n    )\n  ) |&gt; \n  ggplot(\n    aes(\n      .value,\n      .variable,\n      color = model\n    )\n  ) +\n    geom_vline(xintercept = 0) +\n    stat_pointinterval(\n      position = position_dodge(width = 0.2)\n    )+\n    theme(legend.position = \"top\")\n\n\n\n\n\nFigure¬†2: Comparing estimates from models with and without the collider"
  },
  {
    "objectID": "posts/2023-09-06_haunted-dag-3/index.html#haunting",
    "href": "posts/2023-09-06_haunted-dag-3/index.html#haunting",
    "title": "Colliders",
    "section": "Haunting!",
    "text": "Haunting!\nHere‚Äôs the scary part. The illustration from the book is about inter-generational effects on education. Grandparents will have an effect on their children (the parent) and parents will have an effect on their children. The question is, is there any direct effect of grandparents on children.\n\n\n\n\ngraph LR\n  g[Grandparent]\n  p[Parent]\n  c[Child]\n  \n  g --&gt; p\n  g -.-&gt;|?| c\n  p --&gt; c\n\n\n\n\n\n\n\n\nfinding the direct effect\n\ndagify(\n  p ~ g,\n  c ~ p,\n  c ~ g\n) |&gt; \n  adjustmentSets(\n    exposure = \"g\", \n    outcome = \"c\",\n    effect = \"direct\"\n  )\n\n\n{ p }\n\n\nOk, but the spooky thing is what if there‚Äôs a variable (like, neighborhood) that‚Äôs shared by the parent and child, but not the grandparent, which we didn‚Äôt record.\n\n\n\n\ngraph LR\n  g[Grandparent]\n  p[Parent]\n  c[Child]\n  n[Neighborhood]\n  \n  g -.-&gt;|?| c\n  g --&gt; p\n  p --&gt; c\n  n --&gt; p\n  n --&gt; c\n  \n  style n stroke-dasharray: 5 5\n\n\n\n\n\nParent has apparently become a collider, but I‚Äôm still trying to noodle through why.\n\nOk, having stepped away for a bit, I think my problem was some confusion about how the ‚Äúpaths‚Äù work in DAGs.\n\n\n\n\n\n\nI realized:\n\n\n\n\nThe connections from one node to another are directed.\nBut when charting a path from a variable to the outcome, you ignore the directedness.\nThen, you add back in the directedness to diagnose confounder, mediator, collider etc.\n\n\n\nSo, ignoring the directedness, we have the following paths from Grandparent to Child.\n\n\n\n\n\n\nUndirected Paths\n\n\n\n\nGrandparent ‚Äî Child\nGrandparent ‚Äî Parent ‚Äî Child\nGrandparent ‚Äî Parent ‚Äî Neighborhood ‚Äî Child\n\n\n\nThen, we can add in the directedness\n\n\n\n\n\n\nDirected Paths\n\n\n\n\nGrandparent ‚Üí Child\nGrandparent ‚Üí Parent ‚Üí Child\nGrandparent ‚Üí Parent ‚Üê Neighborhood ‚Üí Child\n\n\n\nBecause of path 2, (Grandparent ‚Üí Parent ‚Üí Child), in order to get the ‚Äúdirect effect‚Äù of Grandparent, we need to include Parent. But because of path 3 (Grandparent ‚Üí Parent ‚Üê Neighborhood ‚Üí Child), Parent is also a Collider. If we don‚Äôt include Neighborhood in the model (maybe because we didn‚Äôt measure it!) the estimate for Grandparent is going to get all screwy!\nI‚Äôm still developing my intuitions for why and how the estimate will get screwy.\n\n\n\n\n\n\nGetting the paths\n\n\n\n\n\nWith {dagitty} and {ggdag} you‚Äôre supposed to be able to get the paths automatically, but I can‚Äôt get the {ggdag} one to work give me the collider path.\n\n\n\nmaking the dag\n\ndagify(\n  c ~ g + p + n,\n  p ~ g + n\n) -&gt;\n  haunted_dag\n\n\n\n\n\nwith dagitty\n\nhaunted_dag |&gt; \n  paths(\n    from = \"g\",\n    to = \"c\"\n  ) |&gt; \n  pluck(\"paths\")\n\n\n[1] \"g -&gt; c\"           \"g -&gt; p -&gt; c\"      \"g -&gt; p &lt;- n -&gt; c\"\n\n\n\n\n\nwith ggdag\n\nhaunted_dag |&gt; \n  ggdag_paths(\n    from = \"g\",\n    to = \"c\",\n    directed = F\n  )+\n    theme_dag()\n\n\n\n\n\nI think ggdag::ggdag_paths() is calling dagitty::paths() underneath, and just isn‚Äôt passing the directed argument correctly as of\n\nSys.Date()\n\n[1] \"2023-09-06\"\n\n\nI can get it to plot colliders, though.\n\nhaunted_dag |&gt; \n  ggdag_collider()+\n  theme_dag()\n\n\n\n\n\n\n\n\nSimulating the haunted dag\nI‚Äôll mostly just copy the simulation parameters from the book.\n\n\n\nsetting the direct effects\n\n# Grandparent on parent\nb_GP &lt;- 1\n\n# Parent on Child\nb_PC &lt;- b_GP\n\n# Grandparent on Child\nb_GC &lt;- 0\n\n# neighborhood\nb_N &lt;- 2\n\n\n\n\n\nThe simulation\n\nn = 200\n\ntibble(\n  grandparent = rnorm(n),\n  neighborhood = rbinom(\n    n, \n    size = 1, \n    prob = 0.5\n  ),\n  parent = rnorm(\n    n,\n    mean = b_GP * grandparent + \n           b_N * neighborhood\n  ),\n  child = rnorm(\n    n,\n    mean = b_GC * grandparent +\n           b_PC * parent + \n           b_N  * neighborhood\n  )\n) -&gt;\n  haunted_sim\n\n\n\n\nCode\nhaunted_sim |&gt; \n  ggplot(\n    aes(\n      grandparent,\n      child,\n      color = factor(neighborhood)\n    )\n  )+\n  geom_point()+\n  guides(\n    color = \"none\"\n  )+\n  theme(\n    aspect.ratio = 0.8\n  )-&gt;\n  gc_plot\n\nhaunted_sim |&gt; \n  ggplot(\n    aes(\n      parent,\n      child,\n      color = factor(neighborhood)\n    )\n  )+\n    geom_point()+\n  theme_blank_y()+\n  guides(\n    color = \"none\"\n  )+  \n  theme(\n    aspect.ratio = 0.8\n  )-&gt;  \n  pc_plot\n\nhaunted_sim |&gt; \n  ggplot(\n    aes(\n      factor(neighborhood),\n      child,\n      fill = factor(neighborhood),\n      color = factor(neighborhood)\n    )\n  )+\n    geom_dots(\n      dotsize = 1.3,\n      layout = \"hex\",\n      side = \"both\"\n    )+\n  labs(x = \"neighborhood\")+\n  theme_blank_y()+\n  guides(\n    color = \"none\",\n    fill = \"none\"\n  )+  \n  theme(\n    aspect.ratio = 0.8\n  )-&gt;\n  nc_plot\n\ngc_plot + pc_plot + nc_plot\n\n\n\n\n\nFigure¬†3: Simulated ‚Äòhaunted‚Äô data\n\n\n\n\nNow, let‚Äôs fit the model with just grandparent and parent (because, in this example, we don‚Äôt know about the neighborhood).\n\n\n\nthe haunted collider model\n\nbrm(\n  child ~ parent + grandparent,\n  data = haunted_sim,\n  prior = c(\n    prior(normal(0,3), class = b)\n  ),\n  backend = \"cmdstanr\",\n  cores = 4,\n  file = \"haunted1\"\n)-&gt;\n  haunted1_mod\n\n\nNow let‚Äôs get the parameters and compare them to the true values that we created the simulation with.\n\n\n\ntrue values\n\ntribble(\n  ~.variable, ~.value,\n  \"b_Intercept\", 0,\n  \"b_grandparent\", b_GC,\n  \"b_parent\", b_PC\n)-&gt;\n  true_param\n\n\n\n\n\nhaunted params\n\nhaunted1_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T \n  ) -&gt;\n  haunted1_params\n\n\n\n\nCode\nhaunted1_params |&gt; \n  ggplot(\n    aes(\n      .value,\n      .variable\n    )\n  ) +\n    geom_vline(xintercept = 0)+\n    stat_halfeye()+\n    geom_point(\n      data = true_param,\n      aes(color = \"true value\"),\n      size = 3\n    )+\n    scale_color_manual(\n      values = ptol_red,\n      name = NULL\n    )+\n    theme(\n      legend.position = \"top\"\n    )\n\n\n\n\n\nFigure¬†4: Haunted parameters!\n\n\n\n\nAs far as things go, the model will make good predictions, because the statistical associations are correct, but the causal interpretation (‚ÄúGrandparents have a negative effect‚Äù) is wrong.\n\n\n\nposterior predictive check\n\npp_check(haunted1_mod)\n\n\n\n\n\nFigure¬†5: Haunted posterior predictive check\n\n\n\n\nIncluding the variable haunting the DAG ought to improve things, but in reality that assumes we know what it is, and have some measure of it.\n\n\n\nExorcised model\n\nbrm(\n  child ~ parent + grandparent + neighborhood,\n  data = haunted_sim,\n  prior = c(\n    prior(normal(0,3), class = b)\n  ),\n  backend = \"cmdstanr\",\n  cores = 4,\n  file = \"exorcised1\"\n)-&gt;\n  exorcised1_mod\n\n\n\n\n\ntrue params\n\ntribble(\n  ~.variable, ~.value,\n  \"b_Intercept\", 0,\n  \"b_grandparent\", b_GC,\n  \"b_parent\", b_PC,\n  \"b_neighborhood\", b_N\n)-&gt;\n  true_param\n\n\n\n\n\nexorcised params\n\nexorcised1_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T \n  ) -&gt;\n  exorcised1_params\n\n\n\n\nCode\nexorcised1_params |&gt; \n  ggplot(\n    aes(\n      .value,\n      .variable\n    )\n  ) +\n    geom_vline(xintercept = 0)+\n    stat_halfeye()+\n    geom_point(\n      data = true_param,\n      aes(color = \"true value\"),\n      size = 3\n    )+\n    scale_color_manual(\n      values = ptol_red,\n      name = NULL\n    )+\n    theme(\n      legend.position = \"top\"\n    )\n\n\n\n\n\nFigure¬†6: Exoricsed params\n\n\n\n\nI‚Äôm not sure why my posterior estimates are so much further off from McElreath‚Äôs‚Ä¶"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html",
    "href": "posts/2023-06-05_07-linear-models-1/index.html",
    "title": "Linear Models: Part 1",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#loading",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#loading",
    "title": "Linear Models: Part 1",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#simulating-a-galton-board",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#simulating-a-galton-board",
    "title": "Linear Models: Part 1",
    "section": "Simulating a Galton Board",
    "text": "Simulating a Galton Board\n\n‚ÄúSuppose you and a thousand of your closest friends line up in the halfway line of a soccer field.‚Äù\n\nOk, so the N is 1+1,000 (‚Äúyou and 1000 of your closest friends‚Äù). Apparently a soccer field is 360 feet long, and an average stride length is something like 2.3 feet.\n\n(360/2)/2.3\n\n[1] 78.26087\n\n\nWe can get in 78 steps from the halfway line to the end of the field.\n\nset.seed(500)\n\nexpand_grid(\n  person = 1:1001,\n  step = 1:78\n) |&gt; \n  mutate(\n    flip = sample(\n      c(-1, 1), \n      size = n(), \n      replace = T\n    )\n  ) |&gt; \n  mutate(\n    .by = person,\n    position = cumsum(flip)\n  ) -&gt;\n  galton_board\n\n\ngalton_board |&gt; \n  mutate(\n    .by = c(step, position),\n    n = n()\n  ) -&gt;\n  galton_board\n\n\ngalton_board |&gt; \n  ggplot(\n    aes(step, position)\n  )+\n    geom_line(\n      aes(group = person, color = n)\n    ) +\n  scale_x_reverse()+\n  khroma::scale_color_bilbao(\n      guide = \"none\"\n    )+  \n  coord_flip()\n\n\n\n\nIt‚Äôs hard to visualize well with the completely overlapping points. I‚Äôll plot histograms for very 10th step.\n\ngalton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) |&gt; \n  ggplot(aes(position, factor(step)))+\n    stat_histinterval(\n      breaks = breaks_fixed(width = 2),\n      aes(fill = after_stat(pdf))\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    scale_y_discrete(\n      limits = factor(seq(70, 10, by = -10))\n    )"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#infinitesimal-galton-board",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#infinitesimal-galton-board",
    "title": "Linear Models: Part 1",
    "section": "Infinitesimal Galton Board",
    "text": "Infinitesimal Galton Board\nSame as before, but now instead of flipping a coin for -1 and 1, values are sampled from \\(\\mathcal{U}(-1,1)\\).\n\nexpand_grid(\n  person = 1:1001,\n  step = 1:78\n) |&gt; \n  mutate(\n    flip = runif(\n      n(),\n      -1,\n      1\n    )\n  ) |&gt; \n  mutate(\n    .by = person,\n    position = cumsum(flip)\n  ) -&gt;\n  inf_galton_board\n\n\ninf_galton_board |&gt; \n  ggplot(aes(step, position))+\n    geom_line(\n      aes(group = person),\n      alpha = 0.05\n    )+\n  scale_x_reverse()+\n  coord_flip()\n\n\n\n\n\ninf_galton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) |&gt; \n  ggplot(aes(position, factor(step)))+\n    stat_slabinterval(\n      aes(fill = after_stat(pdf)), \n      fill_type = \"gradient\"\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    scale_y_discrete(\n      limits = factor(seq(70, 10, by = -10))\n    )\n\n\n\n\nNice.\nI‚Äôm not 100% sure how to get a normal density estimate superimposed in that same plot. So I‚Äôll fake it instead.\n\ninf_galton_board |&gt; \n  filter(step %in% seq(10, 70, by = 10)) -&gt;\n  ten_steps\n\nten_steps |&gt; \n1  summarise(\n    .by = step,\n    mean = mean(position),\n    sd = sd(position)\n  ) |&gt; \n  nest(\n    .by = step\n  ) |&gt; \n  mutate(\n2    dist = map(\n      data,\n      ~tibble(\n        position = seq(-20, 20, length = 100),\n        dens = dnorm(\n          position, \n          mean = .x$mean, \n          sd = .x$sd\n        )\n      )\n    )\n  ) |&gt; \n  unnest(dist) |&gt; \n3  mutate(\n    dens_norm = dens/max(dens)\n  )-&gt;\n  distributions\n\n\n1\n\nCalculating the distribution parameters for each step grouping.\n\n2\n\nMapping over the distribution parameters to get density values in a tibble.\n\n3\n\nFor plotting over the stat_slab() output, normalizing the density to max out at 1.\n\n\n\n\n\nten_steps |&gt; \n  ggplot(aes(position))+\n    stat_slabinterval(\n      aes(fill = after_stat(pdf)), \n      fill_type = \"gradient\"\n    )+\n    geom_line(\n      data = distributions,\n      aes(y = dens_norm)\n    )+\n    khroma::scale_fill_bilbao(\n      guide = \"none\"\n    )+\n    facet_wrap(\n      ~step, labeller = label_both\n    )+\n    theme_no_y()\n\n\n\n\n\nComparing parameters\nFor my own interest, I wonder how much discrete sampling from -1, 1 vs the uniform distribution affects the \\(\\sigma\\).\n\ngalton_board |&gt; \n  summarise(\n    .by = step,\n    pos_sd = sd(position)\n  ) |&gt; \n  mutate(\n    sampling = \"discrete\"\n  ) -&gt;\n  galton_sd\n\ninf_galton_board |&gt; \n  summarise(\n    .by = step,\n    pos_sd = sd(position)\n  ) |&gt; \n  mutate(\n    sampling = \"uniform\"\n  )-&gt;\n  inf_galton_sd\n\n\nbind_rows(\n  galton_sd, \n  inf_galton_sd\n) |&gt; \n  ggplot(aes(step, pos_sd))+\n    geom_line(\n      aes(color = sampling),\n      linewidth = 1\n    )+\n    expand_limits(y = 0)+\n    labs(\n      y = expression(sigma)\n    )\n\n\n\n\nMessing around with a few obvious values of \\(x\\), in \\(\\mathcal{U}(-x,x)\\), I can‚Äôt tell what would approximate the discrete sampling. 2 is too large, and 1.5 is too small. The answer is probably some horror like \\(\\frac{\\pi}{e}\\).1"
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#model-diagrams",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#model-diagrams",
    "title": "Linear Models: Part 1",
    "section": "Model Diagrams",
    "text": "Model Diagrams\nHere‚Äôs the model described in the text.\n\\[\ny_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\beta x_i\n\\]\n\\[\n\\beta \\sim \\mathcal{N}(0, 10)\n\\]\n\\[\n\\sigma \\sim \\text{Exponential}(1)\n\\]\nHe also defines a sampling distribution over \\(x_1\\), but idk if that‚Äôs right. Here‚Äôs my attempt at converting that into a mermaid diagram.\n\n\n\n\nflowchart RL\n  normal1[\"N(Œº·µ¢, œÉ)\"] --&gt;|\"~\"| y[\"y·µ¢\"]\n  beta[\"Œ≤\"] --&gt; mult1([\"√ó\"])\n  x[x·µ¢] --&gt; mult1\n  mult1 --&gt; mu1[Œº·µ¢]\n  mu1 --&gt; normal1\n  \n  exp1[\"Exp(1)\"] --\"~\"--&gt; sigma1[œÉ]\n  sigma1 --&gt; normal1\n  \n  normal2[\"N(0,10)\"] --\"~\"--&gt; beta\n\n\n\n\n\nIt‚Äôs ok. No quite a Kruschke diagram.\n\nAnother example.\nLet me try to write out the diagram for something like y ~ x + (1|z).\n\\[\ny \\sim(\\mu_i, \\sigma_0)\n\\]\n\\[\n\\mu_i = \\beta_0 + \\beta_1x_i + \\gamma_i\n\\]\n\\[\n\\beta_0 \\sim \\mathcal{N}(0,10)\n\\]\n\\[\n\\beta_2 \\sim \\mathcal{N}(0,2)\n\\]\n\\[\n\\gamma_i = \\Gamma_{z_i}\n\\]\n\\[\n\\Gamma_j \\sim \\mathcal{N}(0,\\sigma_1)\n\\]\n\\[\n\\sigma_0 \\sim \\text{Exponential}(1)\n\\]\n\\[\n\\sigma_1 \\sim \\text{Exponential}(1)\n\\]\n\nGeeze, idk. That double subscript feels rough, and I don‚Äôt know the convention for describing the random effects.\n\n\n\n\nflowchart TD\n  normal1[\"N(Œº·µ¢, œÉ‚ÇÄ)\"] --\"~\"--&gt; y[y·µ¢]\n  beta0[\"Œ≤‚ÇÄ\"] --&gt; plus([\"+\"])\n  beta1[\"Œ≤‚ÇÅ\"] --&gt; plus\n  gamma[\"Œ≥·µ¢\"] --&gt; plus\n  plus --&gt; mu[\"Œº·µ¢\"]\n  mu --&gt; normal1\n  normal2[\"N(0,10)\"] --\"~\"--&gt; beta0\n  normal3[\"N(0,2)\"] --\"~\"--&gt; beta1\n  Gamma[\"Œì[z·µ¢]\"] --&gt; gamma\n  normal4[\"N(0, œÉ‚ÇÅ)\"] --\"~\"--&gt; Gamma\n  exponent0[\"Exp(1)\"] --\"~\"--&gt; sigma0[\"œÉ‚ÇÄ\"]\n  sigma0 --&gt; normal1\n  exponent1[\"Exp(1)\"] --\"~\"--&gt; sigma1[\"œÉ‚ÇÅ\"]\n  sigma1 --&gt; normal4\n  \n\n\n\n\n\nYeah, this is too tall. Will have to think about this. The Krushke style diagram is the most compressed version imo."
  },
  {
    "objectID": "posts/2023-06-05_07-linear-models-1/index.html#footnotes",
    "href": "posts/2023-06-05_07-linear-models-1/index.html#footnotes",
    "title": "Linear Models: Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot literally \\(\\frac{\\pi}{e}\\) though, cause that‚Äôs too small at 1.156‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-07-07_12-dags2/index.html",
    "href": "posts/2023-07-07_12-dags2/index.html",
    "title": "DAGs part 2",
    "section": "",
    "text": "Listening\nFor part 2, I‚Äôm going to try working through this step by step like he does in the book."
  },
  {
    "objectID": "posts/2023-07-07_12-dags2/index.html#setup",
    "href": "posts/2023-07-07_12-dags2/index.html#setup",
    "title": "DAGs part 2",
    "section": "Setup",
    "text": "Setup\n\n\nloading libraries and defaults\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(marginaleffects)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggrepel)\n\nsource(here::here(\"_defaults.R\"))\n\n\n\nset.seed(2023-7-7)"
  },
  {
    "objectID": "posts/2023-07-07_12-dags2/index.html#the-data",
    "href": "posts/2023-07-07_12-dags2/index.html#the-data",
    "title": "DAGs part 2",
    "section": "The data",
    "text": "The data\nWe‚Äôre looking at the rethinking::milk data\n\ndata(milk, package = \"rethinking\")\n\nLet‚Äôs try some summaries. For the categorical data, I‚Äôm going to do my own custom summary, but for the numeric columns I‚Äôll just use gtsummary::tbl_summary().\nAs it turns out, just like usual when I start trying to finesse things, the code got a little intense.\n\n\nCategorical summary\nmilk |&gt; \n  select(\n    where(\n      ~!is.numeric(.x)\n    )\n  ) |&gt; \n  pivot_longer(\n    cols = everything(),\n    names_to = \"var\",\n    values_to = \"value\"\n  ) |&gt; \n  summarise(\n    .by = var,\n    total_groups = n_distinct(value),\n    most_common = fct_count(\n      factor(value),\n      sort = T,\n      prop = T\n      ) |&gt; \n      slice(1)\n  ) |&gt; \n  unnest(most_common) |&gt; \n  gt() |&gt; \n     cols_label(\n       var = \"Variable\",\n       total_groups = \"Total Groups\",\n       f = \"Most common\",\n       n = \"Number of most common\",\n       p = \"Proportion of most common\"\n     ) |&gt; \n  fmt_number(\n    columns = p,\n    decimals = 2\n  )\n\n\n\n\n\n\n\n  \n    \n    \n      Variable\n      Total Groups\n      Most common\n      Number of most common\n      Proportion of most common\n    \n  \n  \n    clade\n4\nApe\n9\n0.31\n    species\n29\nA palliata\n1\n0.03\n  \n  \n  \n\nTable¬†1:  Summary of categorical variables \n\n\n\nComparing the code I wrote for the categorical variables to how straightforward tbl_summary() is kind of illustrates how useful these out-of-the-box tools can be.\n\nmilk |&gt; \n  select(\n    where(is.numeric)\n  ) |&gt; \n  tbl_summary()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 291\n    \n  \n  \n    kcal.per.g\n0.60 (0.49, 0.73)\n    perc.fat\n37 (21, 46)\n    perc.protein\n15.8 (13.0, 20.8)\n    perc.lactose\n49 (38, 60)\n    mass\n3 (2, 11)\n    neocortex.perc\n68.9 (64.5, 71.3)\n    ¬†¬†¬†¬†Unknown\n12\n  \n  \n  \n    \n      1 Median (IQR)\n    \n  \n\nTable¬†2:  Summary of continuous variables"
  },
  {
    "objectID": "posts/2023-07-07_12-dags2/index.html#the-initial-model",
    "href": "posts/2023-07-07_12-dags2/index.html#the-initial-model",
    "title": "DAGs part 2",
    "section": "The initial model",
    "text": "The initial model\nOk, we‚Äôre going to model the kilocalories per gram of milk as the outcome, trying to explore whether or not the neocortex percentage is related.\n\n\nplotting code\nmilk |&gt; \n  drop_na() |&gt; \n  ggplot(\n    aes(\n      neocortex.perc, \n      kcal.per.g,\n      color = clade,\n      fill = clade\n    )\n  )+\n    geom_point(\n      key_glyph = \"rect\"\n    )+\n    geom_text_repel(\n      aes(label = species), \n      size = 3,\n      show.legend = F\n    )+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\n\nFigure¬†1: Neorcortex percentage and kcal per gram of milk\n\n\n\n\n\nPreparing the data\nI won‚Äôt look ahead and drop NAs when I standardize to save some space. Looks like we‚Äôre logging the body mass. I‚Äôll just check that distribution real quick.\n\nmilk |&gt; \n  ggplot(aes(mass))+\n    stat_slab()+\n    geom_rug()+\n    labs(\n      title = \"linear scale\"\n    )+\n    theme_no_y()-&gt;\n  mass_linear\n\nmilk |&gt; \n  ggplot(aes(mass))+\n    stat_slab()+\n    geom_rug()+\n    scale_x_log10()+\n    labs(\n      title = \"log scale\"\n    )+\n    theme_no_y()-&gt;\n  mass_log\n\nmass_linear + mass_log\n\n\n\n\nFigure¬†2: Distribution of mass on a linear vs log scale\n\n\n\n\nYup! Looks like we should log it!\n\nmilk |&gt; \n  drop_na() |&gt; \n  mutate(\n    kcal_z = (kcal.per.g-mean(kcal.per.g))/sd(kcal.per.g),\n    neoc_z = (neocortex.perc-mean(neocortex.perc))/sd(neocortex.perc),\n    log_mass = log(mass),\n    log_mass_z = (log_mass - mean(log_mass))/sd(log_mass)\n  ) -&gt;\n  milk_to_mod\n\n\n\nFitting the model\nI‚Äôll fit models with both the weak priors and the stronger priors. I‚Äôm still needing to check what everything is called with get_prior() before I can confidently set anything.\n\nget_prior(\n  kcal_z ~ neoc_z,\n  data = milk_to_mod\n)\n\n                   prior     class   coef group resp dpar nlpar lb ub\n                  (flat)         b                                   \n                  (flat)         b neoc_z                            \n student_t(3, -0.2, 2.5) Intercept                                   \n    student_t(3, 0, 2.5)     sigma                               0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nOk, should do the trick.\n\nbrm(\n  kcal_z ~ neoc_z,\n  data = milk_to_mod,\n  prior = c(\n    prior(normal(0,1), class = b),\n    prior(normal(0,1), class = Intercept),\n    prior(exponential(1), class = sigma) \n  ),\n  sample_prior = T,\n  file = \"neoc_model_weak.rds\",\n  cores = 4,\n  backend = \"cmdstanr\"\n)-&gt;\n  neoc_model_weak\n\n\nbrm(\n  kcal_z ~ neoc_z,\n  data = milk_to_mod,\n  prior = c(\n    prior(normal(0,0.5), class = b),\n    prior(normal(0,0.2), class = Intercept),\n    prior(exponential(1), class = sigma) \n  ),\n  sample_prior = T,\n  file = \"neoc_model_strong.rds\",\n  cores = 4,\n  backend = \"cmdstanr\"\n)-&gt;\n  neoc_model_strong\n\n\n\nPrior predictive plots\nYou can set an option in brm to only sample the priors for something like this, but instead I just fit whole models to save time. This‚Äôll need to be a bit ‚Äúmanual‚Äù, cause I don‚Äôt think marginaleffects::predictions() has an option to get prior predictions.\n\nprior_draws(neoc_model_weak) |&gt; \n  mutate(\n    draw = row_number(),\n      priors = \"weak\"\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(\n    pred = list(tibble(\n      neoc_z = seq(-2, 2, length = 50),\n      kcal_z = Intercept + (neoc_z * b)\n    ))\n  ) -&gt;\n  weak_prior_predictive\n\nAnd then the same thing for the strong priors model.\n\n\nsame as above\nprior_draws(neoc_model_strong) |&gt; \n  mutate(\n    draw = row_number(),\n      priors = \"strong\"\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(\n    pred = list(tibble(\n      neoc_z = seq(-2, 2, length = 50),\n      kcal_z = Intercept + (neoc_z * b)\n    ))\n  ) -&gt;\n  strong_prior_predictive\n\n\nI‚Äôll just sample 50 fitted lines for each model.\n\nbind_rows(\n  weak_prior_predictive,\n  strong_prior_predictive\n) |&gt; \n  group_by(priors) |&gt; \n  sample_n(50) |&gt; \n  unnest(pred) |&gt; \n  ggplot(aes(neoc_z, kcal_z)) +\n    geom_line(\n      aes(group = draw)\n    )+\n    facet_wrap(~priors)+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\nFigure¬†3: prior predictive distributions\n\n\n\n\nSo, the ‚Äúweaker‚Äù priors are ‚Äúsilly‚Äù (as McElreath puts it) because for some noec_z values, it‚Äôs predicting kcal_z values as extreme as 6. And because I standardized the outcome, that‚Äôs saying some kcal_z values are up to 6 standard deviations from the average. R actually craps out trying to give the cumulative probability!\n\npnorm(6, mean = 0, sd = 1)\n\n[1] 1\n\n\n\n\nThe Posteriors\nLemme compare the posterior parameter estimates for the two models.\n\nneoc_model_weak |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mutate(priors = \"weak\") -&gt;\n  weak_betas\n\nneoc_model_strong |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mutate(priors = \"strong\") -&gt; \n  strong_betas\n\n\nbind_rows(\n  weak_betas,\n  strong_betas\n) |&gt; \n  ggplot(aes(.value, priors))+\n    stat_halfeye()+\n    facet_wrap(~.variable)\n\n\n\n\nFigure¬†4: parameter estimates by model\n\n\n\n\nThe posterior distribution for the Intercept might be notably different, but they‚Äôre still pretty comparable.\nLet‚Äôs see the predicted values.\n\n\nplotting code\npredictions(\n  neoc_model_strong,\n  newdata = datagrid(\n    neoc_z = seq(-2, 2, length = 50)\n  )\n) |&gt; \n  posterior_draws() -&gt;\n  neoc_fit1\n\nneoc_fit1 |&gt; \n  ggplot(aes(neoc_z, draw))+\n    stat_lineribbon(\n      .width = c(\n        0.89,\n        0.7,\n        0.5\n      )\n    )+\n    scale_fill_brewer()+\n    labs(\n      title = \"kcal ~ neocortex\",\n      y = \"kcal_z\"\n    )+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\n\nFigure¬†5: Posterior estimates of kcal_z"
  },
  {
    "objectID": "posts/2023-07-07_12-dags2/index.html#more-models",
    "href": "posts/2023-07-07_12-dags2/index.html#more-models",
    "title": "DAGs part 2",
    "section": "More Models",
    "text": "More Models\nOk, we‚Äôre also going to fit modes for\n\nkcal_z ~ log_mass_z\nkcal_z ~ neoc_z + log_mass_z\n\n\nbrm(\n  kcal_z ~ log_mass_z,\n  data = milk_to_mod,\n  prior = c(\n    prior(normal(0,0.5), class = b),\n    prior(normal(0,0.2), class = Intercept),\n    prior(exponential(1), class = sigma) \n  ),\n  sample_prior = T,\n  file = \"mass_model.rds\",\n  cores = 4,\n  backend = \"cmdstanr\"\n)-&gt;\n  mass_model\n\n\nbrm(\n  kcal_z ~ neoc_z + log_mass_z,\n  data = milk_to_mod,\n  prior = c(\n    prior(normal(0,0.5), class = b),\n    prior(normal(0,0.2), class = Intercept),\n    prior(exponential(1), class = sigma) \n  ),\n  sample_prior = T,\n  file = \"neoc_mass_model.rds\",\n  cores = 4,\n  backend = \"cmdstanr\"\n)-&gt;\n  neoc_mass_model\n\nWe can compare the parameters from each with some reused code from above!\n\n\nposterior getting\nmass_model |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mutate(model = \"~mass\") -&gt; \n  mass_betas\n\nneoc_mass_model |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mutate(model = \"~neoc+mass\") -&gt; \n  neoc_mass_betas\n\nstrong_betas |&gt; \n  mutate(model = \"~neoc\") -&gt;\n  neoc_betas\n\n\n\n\nplotting code\nbind_rows(\n  neoc_betas,\n  mass_betas,\n  neoc_mass_betas\n) |&gt; \n  ggplot(aes(.value, model))+\n    stat_halfeye(\n      aes(fill = after_stat(x &gt;= 0)),\n      show.legend = F\n    )+\n    facet_wrap(~.variable)+\n    theme(\n      aspect.ratio = 0.75\n    )\n\n\n\n\n\nFigure¬†6: Comparison of parameters across models\n\n\n\n\nSo, including both predictors in the model amplified the effect for both of them. We can get the fitted values now\n\n\nCode\npredictions(\n  neoc_mass_model,\n  newdata = datagrid(\n    neoc_z = seq(-2, 2, length = 50),\n    log_mass_z = 0\n  )\n) |&gt; \n  posterior_draws() |&gt;  \n  mutate(model = \"~neoc + mass\")-&gt; \n  neoc_fit2\n\nneoc_fit1 |&gt; \n  mutate(model = \"~neoc\")-&gt;\n  neoc_fit1\n\nbind_rows(\n  neoc_fit1,\n  neoc_fit2\n) |&gt; \n  ggplot(aes(neoc_z, draw))+\n    stat_lineribbon(\n      .width = c(0.89,0.7, 0.5),\n      color = NA\n    )  +\n    scale_fill_brewer()+\n   labs(\n     y = \"kcal_z\",\n     subtitle = \"log_mass_z = 0\"\n   )+\n   facet_wrap(~model)+\n   theme(\n     aspect.ratio = 1\n   )\n\n\n\n\n\nFigure¬†7: comparison of predicted values across neocortex percentage\n\n\n\n\n\n\nCode\npredictions(\n  mass_model,\n  newdata = datagrid(\n    log_mass_z = seq(-2, 2, length = 50),\n    neoc_z = 0\n  )\n) |&gt; \n  posterior_draws() |&gt; \n  mutate(model = \"~mass\")-&gt;\n  mass_fit1\n\npredictions(\n  neoc_mass_model,\n  newdata = datagrid(\n    log_mass_z = seq(-2, 2, length = 50),\n    neoc_z = 0\n  )\n) |&gt; \n  posterior_draws() |&gt; \n  mutate(model = \"~neoc + mass\")-&gt;\n  mass_fit2\n\nbind_rows(\n  mass_fit1,\n  mass_fit2\n) |&gt; \n  ggplot(aes(log_mass_z, draw))+\n    stat_lineribbon(\n      .width = c(0.89,0.7, 0.5),\n      color = NA\n    )  +\n    scale_fill_brewer()+\n   labs(\n     y = \"kcal_z\",\n     subtitle = \"neoc_z = 0\"\n   )+\n   facet_wrap(~model)+\n   theme(\n     aspect.ratio = 1\n   )\n\n\n\n\n\nFigure¬†8: comparison of predicted values across bodymass"
  },
  {
    "objectID": "posts/2023-07-07_12-dags2/index.html#why",
    "href": "posts/2023-07-07_12-dags2/index.html#why",
    "title": "DAGs part 2",
    "section": "Why?",
    "text": "Why?\nEach predictor is correlated with the outcome, and also (strongly) correlated with each other.\n\n\nCode\nmilk_to_mod |&gt; \n  ggplot(aes(log_mass_z, kcal_z))+\n    geom_point()+\n    stat_smooth(\n      method = 'lm',\n      se = F,\n      color = ptol_blue\n    )+\n    scale_x_continuous(position = \"top\")-&gt;\n  mass_kcal\n\nmilk_to_mod |&gt; \n  ggplot(aes(neoc_z, kcal_z))+\n    geom_point()+\n    stat_smooth(\n      method = 'lm',\n      se = F,\n      color = ptol_blue\n    )+\n    scale_x_continuous(position = \"top\")+\n    scale_y_continuous(position = \"right\")+\n  theme(\n    aspect.ratio = 1\n  )-&gt;\n  neoc_kcal\n\nmilk_to_mod |&gt; \n  ggplot(aes(log_mass_z, neoc_z))+\n    geom_point()+\n    stat_smooth(\n      method = 'lm',\n      se = F,\n      color = ptol_blue\n    )+\n  theme(\n    aspect.ratio = 1\n  )-&gt;\n  mass_neoc\n\nlayout &lt;- \"\nAB\nC#\n\"\n\nmass_kcal + neoc_kcal + mass_neoc + plot_layout(design = layout)\n\n\n\n\n\nFigure¬†9: Relationship between the three variables\n\n\n\n\n\nIsn‚Äôt this collinearity?\nSo, on this point, I‚Äôm not completely sure how I should feel about the model with both body mass and neocortex percentage, since it looks like ‚Äúcollinearity‚Äù which is supposed to be üëª spooky üëª. In the book, he gives three possible DAGs, so I‚Äôll see what the ‚Äúadjustment sets‚Äù are like for each.\n\ndagify(\n  kcal ~ mass,\n  kcal ~ neoc,\n  neoc ~ mass\n) |&gt; \n  adjustmentSets(\n    outcome = \"kcal\",\n    exposure = \"neoc\",\n    effect = \"direct\"\n    )\n\n{ mass }\n\n\n\ndagify(\n  kcal ~ mass,\n  kcal ~ neoc,\n  # flipping this\n  mass ~ neoc\n) |&gt; \n  adjustmentSets(\n    outcome = \"kcal\",\n    exposure = \"neoc\",\n    effect = \"direct\"\n  )\n\n{ mass }\n\n\n\ndagify(\n  kcal ~ mass,\n  kcal ~ neoc,\n  mass ~ UNK,\n  neoc ~ UNK,\n  latent = \"UNK\"\n) |&gt; \n  adjustmentSets(\n    outcome = \"kcal\",\n    exposure = \"neoc\",\n    effect = \"direct\"\n  ) \n\n{ mass }\n\n\nWell, they all say to get the direct effect of neocortex percentage on kcal per gram, you need to include mass‚Ä¶ Which I can be cool with, I just need to figure out how we‚Äôre thinking about collinearity now! Maybe the paper Collinearity isn‚Äôt a disease that needs curing is a place to start!"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html",
    "title": "Garden of Forking paths part 2",
    "section": "",
    "text": "Listening\nrenv::install(\"khroma\")\nlibrary(tidyverse)\nlibrary(khroma)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#a-nicer-table-version.",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#a-nicer-table-version.",
    "title": "Garden of Forking paths part 2",
    "section": "A nicer table version.",
    "text": "A nicer table version.\nI‚Äôd like to re-represent the Bayesian Update in a nicer GT table. Some options are\n\nPlotting extensions from {gtExtras}\nEmojis\n\n\nrenv::install(\"gtExtras\")\nrenv::install(\"svglite\")\nrenv::install(\"emoji\")\n\n\nlibrary(gtExtras)\nlibrary(emoji)\n\nFirst, trying the ‚Äúwin/losses‚Äù column plot from {gtExtra} to illustrate the blue vs white marbles.\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(1, blue_marbs), \n                     rep(0, white_marbs)))\n  ) -&gt; \n  marbles_wl\n\nThe cell background will have to be off-white for the white ticks to show\n\nmarbles_wl |&gt; \n  gt() |&gt; \n  gt_plt_winloss(marbles, palette = c(\"blue\", \"white\", \"grey\")) |&gt; \n  tab_style(style = cell_fill(color = \"antiquewhite\"), \n            locations = cells_body())\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\n          \n    1\n3\n          \n    2\n2\n          \n    3\n1\n          \n    4\n0\n          \n  \n  \n  \n\nTable¬†1:  Representing marble compositions with ‚Äòwin-loss‚Äô plots \n\n\n\nI‚Äôm not overwhelmed by the result. I‚Äôll try emojis instead.\n\nblue_marb &lt;- emojis |&gt; \n  filter(str_detect(name, \"blue\"), \n         str_detect(name, \"circle\")) |&gt; \n  pull(emoji)\n\nblue_marb\n\n[1] \"üîµ\"\n\n\n\nwhite_marb &lt;- emojis |&gt; \n  filter(str_detect(name, \"white\"), \n         str_detect(name, \"circle\")) |&gt; \n  pull(emoji)\n\nwhite_marb\n\n[1] \"‚ö™\"\n\n\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(blue_marb, blue_marbs), \n                     rep(white_marb, white_marbs)))\n  ) -&gt; \n  marbles_emoji\n\n\nmarbles_emoji |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\n‚ö™, ‚ö™, ‚ö™, ‚ö™\n    1\n3\nüîµ, ‚ö™, ‚ö™, ‚ö™\n    2\n2\nüîµ, üîµ, ‚ö™, ‚ö™\n    3\n1\nüîµ, üîµ, üîµ, ‚ö™\n    4\n0\nüîµ, üîµ, üîµ, üîµ\n  \n  \n  \n\nTable¬†2:  Representing marble compositions with emoji \n\n\n\nUpdate: As it turns out, getting tables with emoji and the plots with gt_plt_*() do not play nice with LaTeX. For now, I‚Äôm saving the tables to png with gtsave() just for the pdf output.\nYes, this is it.\n\nRerunning the sampling\nI‚Äôll re-run the sampling from the previous post.\n\nsampling_df &lt;- function(marbles, \n                        n = 1000, \n                        size = 3, \n                        pattern = c(blue_marb, white_marb, blue_marb)){\n  sampling_tibble &lt;- tibble(samp = 1:n)   \n  sampling_tibble |&gt; \n    mutate(\n      chosen = map(samp, \n                   ~sample(marbles, \n                           size = 3, \n                           replace = T)),\n      match = map_lgl(chosen, \n                      ~all(.x == pattern))                 \n    ) |&gt; \n    summarise(prop_match = mean(match))-&gt;                         \n    sampling_tibble\n  return(sampling_tibble)\n}\n\n\nmarbles_emoji |&gt; \n ungroup() |&gt; \n  mutate(\n    prob = map(marbles, ~sampling_df(.x, n = 10000))\n  ) |&gt; \n  unnest(prob) |&gt; \n  mutate(norm_probs = prop_match/sum(prop_match))-&gt;\n  marble_probs\n\nI want to label the column of probabilities with the key sequence.\n\nkey_seq &lt;- str_glue(\"{blue_marb}, {white_marb}, {blue_marb}\")\n\ngtExtras::gt_plt_bar_pct() will plot a bar chart within the table.\n\nmarble_probs |&gt; \n  select(marbles, norm_probs) |&gt; \n  mutate(norm_probs = norm_probs * 100) |&gt; \n  gt() |&gt; \n  cols_label(\n    norm_probs = str_glue(\"p(marbles | {key_seq})\")\n  ) |&gt; \n  gt_plt_bar_pct(norm_probs, \n                 scaled = T, \n                 fill = \"steelblue\")\n\n\n\n\n\n\n  \n    \n    \n      marbles\n      p(marbles | üîµ, ‚ö™, üîµ)\n    \n  \n  \n    ‚ö™, ‚ö™, ‚ö™, ‚ö™\n\n    üîµ, ‚ö™, ‚ö™, ‚ö™\n\n    üîµ, üîµ, ‚ö™, ‚ö™\n\n    üîµ, üîµ, üîµ, ‚ö™\n\n    üîµ, üîµ, üîµ, üîµ\n\n  \n  \n  \n\nTable¬†3:  Probability of each marble composition given (üîµ, ‚ö™Ô∏è, üîµ) samples\nwith replacement. \n\n\n\nThere we go!\n\n\nWith the Bayesian Update\n\nmarble_probs |&gt; \n  mutate(new_prob = blue_marbs/sum(blue_marbs),\n         multiplied = norm_probs * new_prob,\n         norm_new = multiplied/sum(multiplied)) |&gt; \n  select(marbles, norm_probs, norm_new) |&gt; \n  mutate(norm_probs = norm_probs * 100,\n         norm_new = norm_new * 100) |&gt; \n  gt() |&gt; \n  cols_label(\n    norm_probs = str_glue(\"p(marbles | {key_seq})\"),\n    norm_new = str_glue(\"after {blue_marb}\")\n  ) |&gt; \n  gt_plt_bar_pct(norm_probs, \n                 scaled = T, \n                 fill = \"steelblue\") |&gt; \n  gt_plt_bar_pct(norm_new, \n                 scaled = T, \n                 fill = \"steelblue\") |&gt; \n  cols_width(2 ~ px(200),\n             3 ~ px(200))\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n    \n      marbles\n      p(marbles | üîµ, ‚ö™, üîµ)\n      after üîµ\n    \n  \n  \n    ‚ö™, ‚ö™, ‚ö™, ‚ö™\n\n\n    üîµ, ‚ö™, ‚ö™, ‚ö™\n\n\n    üîµ, üîµ, ‚ö™, ‚ö™\n\n\n    üîµ, üîµ, üîµ, ‚ö™\n\n\n    üîµ, üîµ, üîµ, üîµ\n\n\n  \n  \n  \n\nTable¬†4:  Probability of each marble composition given an additional (üîµ)\nsample"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#bayesian-updating",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#bayesian-updating",
    "title": "Garden of Forking paths part 2",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\nI‚Äôll try to illustrate Baysian updating with an animated plotly plot.\n\nrenv::install(\"plotly\")\nrenv::install(\"slider\")\n\n\nlibrary(plotly)\nlibrary(slider)\n\nI know enough to know that the distribution that‚Äôs being updated is the beta. (Apparently is the binomial. Still a but lost on their distinction!) So I‚Äôll get the density for each update.\n\nplot(\n  seq(0,1, length =100),\n  dbeta(seq(0,1, length =100), 1, 1),\n  type = 'l'\n)\n\n\n\n\n\nwater_land_sequence &lt;- c(\"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\n\nI‚Äôll use slider::slide() to generate a data frame of sample updates. I‚Äôll need a function that takes a sequence of W and L and converts them into counts.\n\nw_l_count &lt;- function(x){\n  tibble(\n    water = sum(x == \"W\"),\n    land = sum(x == \"L\")\n  )\n}\n\n\nslide(water_land_sequence, \n      .f = w_l_count, \n      .before = Inf,\n      .after = 0) |&gt; \n  bind_rows() |&gt; \n  mutate(seq = row_number()) |&gt; \n  bind_rows(\n    tibble(\n      water = 0,\n      land = 0, \n      seq = 0\n    )\n  ) |&gt; \n  arrange(seq) -&gt;\n  sequence_counts\n\n\nsequence_counts |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      water\n      land\n      seq\n    \n  \n  \n    0\n0\n0\n    1\n0\n1\n    1\n1\n2\n    2\n1\n3\n    3\n1\n4\n    3\n2\n5\n    4\n2\n6\n    4\n3\n7\n    5\n3\n8\n  \n  \n  \n\nTable¬†5:  Table of water, land count updates \n\n\n\nNow to get the densities.\n\nsequence_counts |&gt; \n  rowwise() |&gt; \n  mutate(\n    density = map2(\n      water, land, ~tibble(\n        prop = seq(0.0001, 0.9999, length = 100),\n        density_unstd = dbinom(water, size = water + land, prob = prop),\n        density = density_unstd/sum(density_unstd)\n        )\n    )\n  ) |&gt; \n  unnest(density)-&gt;\n  density_updates\n\n\ndensity_updates |&gt; \n  ggplot(aes(prop, density))+\n    geom_line(aes(group = seq, color = seq))\n\n\n\n\nFigure¬†1: beta distribution updates\n\n\n\n\nGood first step.\nI had to turn to the plotly book to get the animated lines correct https://plotly-r.com/animating-views.html.\n\nsequence_counts |&gt; \n  mutate(\n    annotation = str_glue(\"W:{water}, L:{land}\")\n  ) -&gt; \n  wl_annotate\n\n\ndensity_updates |&gt; \n  plot_ly() |&gt; \n  add_lines(\n    x = ~prop,\n    y = ~density,\n    frame = ~seq,\n    line = list(simplify = F, width = 3)\n  ) |&gt; \n  add_text(\n    data = wl_annotate,\n    text = ~annotation,\n    frame = ~seq,\n    x = 0.1,\n    y = 0.025,\n    textfont = list(size = 20)\n  ) |&gt;\n  layout(\n    showlegend = F\n  )\n\n\n\n\nFigure¬†2: Animated Bayesian Updating\n\n\n\nUpdate: I started making this just for the pdf, which can‚Äôt have the animation, but it‚Äôs actually kind of nice enough to include in the html.\n\ndensity_updates |&gt; \n  group_by(prop) |&gt; \n  arrange(seq) |&gt; \n  mutate(\n    prev_density = lag(density),\n    facet_lab = str_glue(\"W:{water}, L{land}\")\n  ) |&gt; \n  ggplot(aes(prop, density))+\n    geom_area(aes(y = prev_density), linetype = 2, alpha = 0.2)+\n    geom_area(alpha = 0.6, fill = \"steelblue\")+\n    scale_y_continuous(expand = expansion(mult = c(0,0)))+\n    facet_wrap(~facet_lab)+\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 8),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nFigure¬†3: Static figure of Bayesian Updating"
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#on-priors",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#on-priors",
    "title": "Garden of Forking paths part 2",
    "section": "On priors",
    "text": "On priors\n\nThe fact that statistical inference uses mathematics does not imply that there is only one reasonable or useful way to conduct an analysis."
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#grid-approximation",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#grid-approximation",
    "title": "Garden of Forking paths part 2",
    "section": "Grid approximation",
    "text": "Grid approximation\nOk, I‚Äôll do one grid approximation for the hell of it.\n‚Ä¶\nGot distracted and went down a rabbit hole on the beta vs binomial distributions.\n\ntibble(\n  prob = seq(0.0001, 0.9999, length = 50), \n  prior_unstd = case_when(\n    prob &lt; 0.5 ~ 0,\n    .default = 1\n  ),\n  prior_std = prior_unstd/sum(prior_unstd),\n  likelihood_binom = dbinom(6, size = 9, prob = prob),\n  l_binom_std = likelihood_binom/sum(likelihood_binom),\n  likelihood_beta = dbeta(prob, 6, 9-6),\n  l_beta_std = likelihood_beta/sum(likelihood_beta)\n) |&gt; \n  ggplot(aes(prob))+\n    geom_point(aes(y = l_binom_std, color = \"binom\"))+\n    geom_point(aes(y = l_beta_std, color = \"beta\"))+\n    scale_color_bright()+\n    labs(\n      x = \"probability\",\n      color = \"distribution\",\n      y = NULL\n    )\n\n\n\n\nFigure¬†4: Comparing normalized densities for beta(6,3) and binom(6, 9, p)\n\n\n\n\nGlad I did this. I guess\n\ndbinom \\(\\propto P(O, S | p)\\)\ndbeta \\(\\propto P(p|O,S)\\)\n\nI guess I‚Äôd want to see dbinom plotted out with O on the x axis?\n\ntibble(\n  probability = seq(0.0001, 0.9999, length = 10)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    densities = map(\n      probability,\n      ~tibble(obs = 0:9, \n              density = dbinom(obs, size = 9, prob = .x))\n    )\n  ) |&gt; \n  unnest(densities) -&gt;\n  binomial_densities \n\nbinomial_densities |&gt; \n  ggplot(aes(obs, density, color = probability))+\n    geom_point()+\n    geom_line(aes(group = probability)) +\n    geom_rect(\n      color = \"red\",\n      fill = NA,\n      xmin = 5.5,\n      xmax = 6.5,\n      ymin = 0,\n      ymax = 1\n    )+\n    scale_color_batlow()\n\n\n\n\nFigure¬†5: Binomial distributions of various successes out of 9 trials, for various p\n\n\n\n\nWhat we‚Äôre plotting out is what‚Äôs in the red box, flipped on its side.\n\nbinomial_densities |&gt; \n  filter(obs == 6) |&gt; \n  ggplot(aes(probability, density))+\n    geom_line()+\n    geom_point(aes(color = probability))+\n    scale_color_batlow()\n\n\n\n\nFigure¬†6: Normalized binomial density for 6 successes out of 9 trials for various p."
  },
  {
    "objectID": "posts/2023-05-10_03-forking-paths-2/index.html#update",
    "href": "posts/2023-05-10_03-forking-paths-2/index.html#update",
    "title": "Garden of Forking paths part 2",
    "section": "Update!",
    "text": "Update!\nThanks TJ!\n\n\ni forgot how i know this but they are the same if you plug in likelihood_beta = dbeta(prob, 1 + 6, 1 + 9-6),\n\n‚Äî tj mahr üççüçï (@tjmahr) May 10, 2023\n\n\n\ntibble(\n  prob = seq(0.0001, 0.9999, length = 50), \n  prior_unstd = case_when(\n    prob &lt; 0.5 ~ 0,\n    .default = 1\n  ),\n  prior_std = prior_unstd/sum(prior_unstd),\n  likelihood_binom = dbinom(6, size = 9, prob = prob),\n  l_binom_std = likelihood_binom/sum(likelihood_binom),\n  likelihood_beta = dbeta(prob, (6+1), (9-6)+1),\n  l_beta_std = likelihood_beta/sum(likelihood_beta)\n) |&gt; \n  ggplot(aes(prob))+\n    geom_point(aes(y = l_binom_std, color = \"binom\", size = \"binom\"))+\n    geom_point(aes(y = l_beta_std, color = \"beta\",size = \"beta\"))+\n    scale_color_bright()+\n    labs(\n      x = \"probability\",\n      color = \"distribution\",\n      size = \"distribution\",\n      y = NULL\n    )\n\n\n\n\nFigure¬†7: Comparing normalized densities for beta(6+1,3+1) and binom(6, 9, p)"
  },
  {
    "objectID": "posts/2023-09-05_haunted-dag-2/index.html",
    "href": "posts/2023-09-05_haunted-dag-2/index.html",
    "title": "Multicollinearity & Post-treatment bias",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-09-05_haunted-dag-2/index.html#setup",
    "href": "posts/2023-09-05_haunted-dag-2/index.html#setup",
    "title": "Multicollinearity & Post-treatment bias",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\nlibrary(ggdensity)\nlibrary(ggforce)\nlibrary(marginaleffects)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(GGally)\n\n\nsource(here::here(\"_defaults.r\"))\n\n\n\nset.seed(2023-9-5)"
  },
  {
    "objectID": "posts/2023-09-05_haunted-dag-2/index.html#what-to-not-do",
    "href": "posts/2023-09-05_haunted-dag-2/index.html#what-to-not-do",
    "title": "Multicollinearity & Post-treatment bias",
    "section": "What to (not) do",
    "text": "What to (not) do\nHere‚Äôs what McElreath says about multicollinearity\n\nSome fields actually teach students to inspect pairwise correlations before fitting a model, to identify and drop highly correlated predictors. This is a mistake. Pairwise correlations are not the problem. It is the conditional associations‚Äînot correlations‚Äîthat matter. (emphasis added)"
  },
  {
    "objectID": "posts/2023-09-05_haunted-dag-2/index.html#a-real-multicollinear-example",
    "href": "posts/2023-09-05_haunted-dag-2/index.html#a-real-multicollinear-example",
    "title": "Multicollinearity & Post-treatment bias",
    "section": "A real multicollinear example",
    "text": "A real multicollinear example\nA real multicollinear example involves the percent fat and lactose in primate‚Äôs milk when used to predict the kcal.\n\ndata(milk, package = \"rethinking\")\n\n\n\n\npreparing data\n\nmilk |&gt; \n  drop_na(\n    kcal.per.g, \n    perc.fat, \n    perc.lactose\n  ) |&gt; \n  mutate(\n    kcal_z = (kcal.per.g-mean(kcal.per.g))/sd(kcal.per.g),\n    fat_z = (perc.fat - mean(perc.fat))/sd(perc.fat),\n    lactose_z = (perc.lactose - mean(perc.lactose))/sd(perc.lactose)\n  )-&gt;\n  milk_to_mod\n\n\nI wanted to make a pairs plot with GGally::pairs(), but something is busted with the axes. I‚Äôll have to do it a little by hand.\n\n\n\nGGally::pairs() attempt\n\nmilk_to_mod |&gt; \n  select(\n    ends_with(\"_z\")\n  ) |&gt; \n  GGally::ggpairs()\n\n\n\n\n\nFigure¬†1: pairs plot of the primate milk data\n\n\n\n\n\n\nCode\nmilk_to_mod |&gt; \n  ggplot(aes(lactose_z, kcal_z)) +\n    geom_point()-&gt;\n  a\n\nmilk_to_mod |&gt; \n  ggplot(aes(fat_z, kcal_z))+\n    geom_point()+\n    theme_blank_y()-&gt;\n  b\n\nmilk_to_mod |&gt; \n  ggplot(aes(lactose_z, fat_z))+\n    geom_point()+\n    theme_blank_x()-&gt;\n  c\n\nthis_layout &lt;- \"\nC#\nAB\n\"\n\na+b+c + plot_layout(design = this_layout)\n\n\n\n\n\nFigure¬†2: Pairs plot\n\n\n\n\nOk, now I‚Äôll fit models for\nkcal_z ~ lactose_z\nkcal_z ~ fat_z\nkcal_z ~ lactose_z + fat_z\nwith the same priors from the book.\n\n\n\nmodel priors\n\nmilk_prior &lt;- c(\n  prior(normal(0, 0.2), class = Intercept),\n  prior(normal(0, 0.5), class = b)\n)\n\n\n\n\n\nlactose model\n\nbrm(\n  kcal_z ~ lactose_z,\n  prior = milk_prior,\n  data = milk_to_mod,\n  backend = \"cmdstanr\",\n  cores = 4,\n  file = \"kcal_lact_mod\" \n)-&gt;\n  kcal_lact_mod\n\n\n\n\n\nfat model\n\nbrm(\n  kcal_z ~ fat_z,\n  prior = milk_prior,\n  data = milk_to_mod,\n  backend = \"cmdstanr\",\n  cores = 4,\n  file = \"kcal_fat_mod\" \n)-&gt;\n  kcal_fat_mod\n\n\n\n\n\nlactose and fat model\n\nbrm(\n  kcal_z ~ lactose_z + fat_z,\n  prior = milk_prior,\n  data = milk_to_mod,\n  backend = \"cmdstanr\",\n  cores = 4,\n  file = \"kcal_lact_fat_mod\" \n)-&gt;\n  kcal_lact_fat_mod\n\n\nNow to compare the estimates\n\n\n\n\n\n\na function\n\n\n\n\n\nI should really refactor the code chunk below into its own function, but the nonstandard evaluation of gather_draws() is intimidating to me.\n\n\n\n\n\n\ngetting all betas\n\nlist(\n  lact = kcal_lact_mod,\n  fat = kcal_fat_mod,\n  lact_fat = kcal_lact_fat_mod\n) |&gt; \n  map(\n    ~ .x |&gt; \n      gather_draws(\n        `b_.*`,\n        regex = T\n      )\n  ) |&gt; \n  list_rbind(\n    names_to = \"model\"\n  ) -&gt;\n  all_milk_params\n\n\nWe only want to look at the non-intercept parameters.\n\n\n\ndropping intercepts\n\nall_milk_params |&gt; \n  filter(\n    str_detect(\n      .variable, \n      \"Intercept\", \n      negate = T\n    )\n  ) -&gt;\n  milk_betas\n\n\n\n\nCode\nmilk_betas |&gt; \n  mutate(\n    model = str_c(\n      \"~\", \n      model\n    ) |&gt; \n      str_replace(\n        \"_\",\n        \"+\"\n      )\n  ) |&gt; \n  ggplot(\n    aes(\n      .value,\n      .variable,\n      color = model\n    )\n  )+\n    geom_vline(\n      xintercept = 0\n    )+\n    stat_pointinterval(\n      position = position_dodge(width = 0.2)\n    )\n\n\n\n\n\nFigure¬†3: Parameter comparison\n\n\n\n\nSo, in each separate model, lactose and fat have larger magnitudes than in the model with both.\nLets grab the correlation of the parameters in the full model.\n\n\n\ngetting parameter correlation.\n\nall_milk_params |&gt; \n  filter(\n    model == \"lact_fat\"\n  ) |&gt; \n  pivot_wider(\n    names_from = .variable,\n    values_from = .value\n  ) |&gt; \n  select(\n    starts_with(\"b_\")\n  ) |&gt; \n  cor() -&gt;\n  milk_param_cor\n\nmilk_param_cor\n\n\n            b_Intercept b_lactose_z    b_fat_z\nb_Intercept  1.00000000  0.01948817 0.02332534\nb_lactose_z  0.01948817  1.00000000 0.92006316\nb_fat_z      0.02332534  0.92006316 1.00000000\n\n\nFor fun, let‚Äôs make this cleaner for gt\n\n\nCode\nmilk_param_cor[\n  upper.tri(milk_param_cor, diag = T)\n] &lt;- NA\n\nmilk_param_cor |&gt; \n  as_tibble(rownames = \"param\") |&gt; \n  slice(-1) |&gt; \n  select(-b_fat_z) |&gt; \n  gt() |&gt; \n  sub_missing() |&gt; \n  fmt_number() |&gt; \n  cols_label(\n    param = \"\"\n  ) \n\n\n\n\n\n\n\n  \n    \n    \n      \n      b_Intercept\n      b_lactose_z\n    \n  \n  \n    b_lactose_z\n0.02\n‚Äî\n    b_fat_z\n0.02\n0.92\n  \n  \n  \n\nTable¬†1:  Parameter posterior correlation \n\n\n\nNotably, the correlation of the b_fat_z and the b_lactose_z parameters ‚â† the correlation of the data.\n\n(milk_to_mod |&gt; \n  select(\n    lactose_z,\n    fat_z\n  ) |&gt; \n  cor())[1,2]\n\n[1] -0.9416373\n\n\nHere‚Äôs a visual comparison of the original data versus the posterior estimates for the effect of the variables.\n\n\nCode\nmilk_to_mod |&gt; \n  ggplot(\n    aes(\n      lactose_z,\n      fat_z\n    )\n  )+\n    geom_point()+\n    labs(\n      title = \"data\"\n    )-&gt;\n  data_cor\n\nall_milk_params |&gt; \n  filter(\n    model == \"lact_fat\"\n  ) |&gt; \n  pivot_wider(\n    names_from = .variable,\n    values_from = .value\n  ) |&gt; \n  ggplot(\n    aes(\n      b_lactose_z, \n      b_fat_z\n    )\n  )+\n    stat_hdr_points()+\n    guides(\n      color = \"none\"\n    ) +\n    labs(title = \"posterior\")-&gt;\n  posterior_cor\n\ndata_cor + posterior_cor\n\n\n\n\n\nFigure¬†4: Data vs Posterior parameters\n\n\n\n\nMcElreath says one thing to do is compare the posterior to the prior. Very similar posteriors and priors could indicate identifiability problems.\n\n\nCode\nall_milk_params |&gt; \n  filter(\n    model == \"lact_fat\",\n    .variable %in% c(\"b_lactose_z\", \"b_fat_z\")\n  ) |&gt; \n  ggplot(\n    aes(\n      .value\n    )\n  )+\n  stat_density(\n    aes(\n      color=\"posterior\"\n    ),\n    geom = \"line\"\n  )+\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = 0,\n      sd = 0.5\n    ),\n    aes(\n      color = \"prior\"\n    )\n  ) +\n  facet_wrap(\n    ~.variable\n  )+\n  xlim(\n    0.5 * -3,\n    0.5 * 3\n  )+\n  labs(\n    color = NULL,\n    x = NULL\n  )+\n  theme_no_y()+\n  theme(\n    aspect.ratio = 0.8\n  )\n\n\n\n\n\nFigure¬†5: Prior/Posterior comparison"
  },
  {
    "objectID": "posts/2023-09-05_haunted-dag-2/index.html#post-treatment-bias",
    "href": "posts/2023-09-05_haunted-dag-2/index.html#post-treatment-bias",
    "title": "Multicollinearity & Post-treatment bias",
    "section": "Post-treatment bias",
    "text": "Post-treatment bias\nMaking this work is going to involve both wrapping my mind around a post-treatment bias, and figuring out how to set a lognormal prior or family in brms.\nThe hypothetical situation: You‚Äôre testing different antifungal soils on plant growth, and you‚Äôre measuring their height, and the presence/absence of fungus. The chronological process is something like:\n\n\n\n\nflowchart LR\n  a[measure sprouts]\n  b(treat soil)\n  a --&gt; b\n  c[measure plants]\n  d[record fungus]\n  b --&gt; c\n  b --&gt; d\n\n\n\n\n\nThe causal process might be something like\n\n\n\n\ngraph LR\n  h0[initial height]\n  h1[second height]\n  f[fungus]\n  t[treatment]\n  \n  h0 --&gt; h1\n  f --&gt; h1\n  t --&gt; f\n\n\n\n\n\nThis makes it much clearer now! ‚ÄúPost treatment‚Äù meaning ‚Äúa variable that sits between the treatment and the outcome.‚Äù\n\n\n\nfungus simulation\n\nn = 100\ntibble(\n  plant_id = 1:n,\n  treatment = plant_id %% 2,\n  h0 = rnorm(n, 10, 2),\n  fungus = rbinom(\n    100,\n    size = 1,\n    prob = 0.5 - treatment * 0.4\n  ),\n  h1 = h0 + \n    rnorm(\n      n, \n      mean = 5 - 3 * fungus\n    )\n)-&gt;\n  fungus_sim\n\n\n\n\nCode\nfungus_sim |&gt; \n  ggplot(\n    aes(\n      h0,\n      h1,\n      color = factor(treatment)\n    )\n  )+\n    geom_point()+\n    geom_abline(color = \"grey60\")+\n    labs(\n      color = \"treatment\"\n    )+\n  theme(\n    legend.position = \"top\",\n    aspect.ratio = NULL\n  )+\n  coord_fixed()-&gt;\n  fungus1\n\nfungus_sim |&gt; \n  ggplot(\n    aes(\n      h0,\n      h1,\n      color = factor(fungus)\n    )\n  )+\n    geom_point()+\n    geom_abline(color = \"grey60\")+\n    labs(\n      color = \"fungus\"\n    )+\n    scale_color_brewer(\n      palette = \"Dark2\"\n    )+\n  theme(\n    legend.position = \"top\",\n    aspect.ratio = NULL\n  )+\n  coord_fixed()-&gt;\n  fungus2\n\nfungus1 + fungus2\n\n\n\n\n\n\n\nFigure¬†6: plant hight, comparing treatment vs fungus effects\n\n\n\n\n\n\n\nFitting the model\nThe way the book fits the model is to use a multiplier on h0. To get this to work in brm() , I think I need to use its non-linear modelling capacity.\nFirst, we fit just an across-the-board model, without including treatment or fungus\n\n\n\ngrowth only model\n\nbrm(\n  bf(\n    h1 ~ h0 * p,\n    p ~ 1,\n    nl = T\n  ),\n  prior = c(\n    prior(lognormal(0, 0.25), coef = Intercept, nlpar = p)\n  ),\n  data = fungus_sim,\n  backend = \"cmdstanr\",\n  file = \"fungus1\"\n)-&gt;\n  fungus1_mod\n\n\n\n\n\ngetting growth estimate\n\nfungus1_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  ) -&gt; fungus1_params\n\n\n\n\nCode\nfungus1_params |&gt; \n  ggplot(\n    aes(\n      .value, \n      .variable\n    )\n  )+\n    stat_halfeye()\n\n\n\n\n\nFigure¬†7: Estimated growth-only model\n\n\n\n\nThis is, thankfully, very similar to what the posterior from the book was! So maybe I did it right. Let‚Äôs grab the maximum likelihood estimate from the simulated data.\n\n\n\ndata summary stats\n\nfungus_sim |&gt; \n  mutate(\n    p = h1/h0\n  ) |&gt; \n  reframe(\n    stat = c(\"median\", \"mean\", \"logmean\"),\n    value = c(\n      median(p),\n      mean(p),\n      exp(mean(log(p)))\n    )\n  ) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n\n\n  \n    \n    \n      stat\n      value\n    \n  \n  \n    median\n1.45\n    mean\n1.43\n    logmean\n1.42\n  \n  \n  \n\nTable¬†2:  Summary stats of the growth data. \n\n\n\n\n\nIncluding both predictors\nNow we‚Äôll do the ‚Äúbad‚Äù thing and include both predictors. The book keeps the lognormal prior on the intercept of the multiplier, but just a normal prior on the treatment and fungus effects.\n\n\n\nfungus + treatment model\n\nbrm(\n  bf(\n    h1 ~ h0 * p,\n    p ~ treatment + fungus,\n    nl = T\n  ),\n  prior = c(\n    prior(lognormal(0, 0.2), coef = Intercept, nlpar = p),\n    prior(normal(0, 0.5), coef = treatment, nlpar = p),\n    prior(normal(0, 0.5), coef = fungus, nlpar = p)\n  ),\n  data = fungus_sim,\n  backend = \"cmdstanr\",\n  file = \"fungus2\"\n)-&gt;\n  fungus2_mod\n\n\n\n\n\ngetting parameters\n\nfungus2_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  )-&gt;\n  fungus2_param\n\n\n\n\nCode\nfungus2_param |&gt; \n  mutate(\n    .variable = .variable |&gt; \n      as.factor() |&gt; \n      fct_relevel(\n      \"b_p_Intercept\",\n      after = Inf\n    )\n  ) |&gt; \n  ggplot(\n    aes(\n      .value,\n      .variable\n    )\n  )+\n    geom_vline(xintercept = 0)+\n    stat_halfeye()\n\n\n\n\n\nOk, so just like the book\n\nThe multiplier intercept got bigger (since it‚Äôs the growth for treatment=0, fungus=0).\nWe‚Äôve got a negative effect of fungus.\nWe‚Äôve got a weak or 0 effect of treatment.\n\nThe non-effect of treatment makes sense, since the effect of treatment is conditional on the effect of the fungus, and the presence/absence of fungus is itself an outcome of the treatment.\nBut, this doesn‚Äôt mean the treatment didn‚Äôt work. There are a lot more plants without fungus in the treatment condition than the non-treatment.\n\n\n\ntreatment by fungus\n\nfungus_sim |&gt; \n  count(\n    treatment, fungus\n  ) |&gt; \n  pivot_wider(\n    names_from = fungus,\n    values_from = n\n  ) |&gt; \n  gt() |&gt; \n  tab_spanner(\n    columns = 2:3,\n    label = \"fungus\"\n  )\n\n\n\n\n\n\n\n  \n    \n    \n      treatment\n      \n        fungus\n      \n    \n    \n      0\n      1\n    \n  \n  \n    0\n29\n21\n    1\n47\n3\n  \n  \n  \n\nTable¬†3:  Treatment by Fungus \n\n\n\n\n\nTreatment only\nLet‚Äôs fit one more model, leaving out fungus.\n\n\n\ntreatment only model\n\nbrm(\n  bf(\n    h1 ~ h0 * p,\n    p ~ treatment,\n    nl = T\n  ),\n  prior = c(\n    prior(lognormal(0, 0.2), coef = Intercept, nlpar = p),\n    prior(normal(0, 0.5), coef = treatment, nlpar = p)\n  ),\n  data = fungus_sim,\n  backend = \"cmdstanr\",\n  file = \"fungus3\"\n)-&gt;\n  fungus3_mod\n\n\n\n\n\ngetting treatment only params\n\nfungus3_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    regex = T\n  ) -&gt;\n  fungus3_param\n\n\n\n\nCode\nfungus3_param |&gt; \n  ggplot(\n    aes(\n      .value, \n      .variable\n    )\n  )+\n    geom_vline(\n      xintercept = 0\n    ) +\n    stat_halfeye()\n\n\n\n\n\nFigure¬†8: Estimates from treatment only model.\n\n\n\n\nNow we get a reliable positive effect of treatment.\n\n\nLooking at it in a DAG\nI‚Äôll use the {ggdag} and {dagitty} packages to build a directed acyclic graph, and then get the ‚Äúconditional independencies‚Äù from it.\nThe ggdag::dagify() function takes a sequence of formulas that translate back and forth between the dags like so:\n# dag\nh0 -&gt; h1\n\n# formula\nh1 ~ h0\n\n\n\nmaking the dag\n\n# from {ggdag}\ndagify(\n  h1 ~ h0,\n  h1 ~ fungus,\n  fungus ~ treatment\n)-&gt;\n  fungus_dag\n\n\n\n\n\ngetting the independencies\n\nimpliedConditionalIndependencies(\n  fungus_dag\n)\n\n\nfngs _||_ h0\nh0 _||_ trtm\nh1 _||_ trtm | fngs\n\n\nSo, getting these conditional independence statements to look nice is a whole thing, apparently. There‚Äôs a unicode character, ‚´´, but in LaTeX the best option is apparently \\perp\\!\\!\\!\\perp, \\(\\perp\\!\\!\\!\\perp\\).\nAnyway, the important statement in there is\n\\[\\text{h}1 \\perp\\!\\!\\!\\perp \\text{treatment}~ |~ \\text{fungus}\\]\nThis means that if fungus is included, then h1 (our outcome) is independent from treatment, i.e.¬†including the post-treatment effect in the model will make it seem like there‚Äôs no effect of the treatment."
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html",
    "title": "02 Small Worlds and Large Worlds",
    "section": "",
    "text": "listening\nIn the analogy, models are ‚ÄúSmall‚Äù, self-contained worlds."
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html#garden-of-forking-paths.",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html#garden-of-forking-paths.",
    "title": "02 Small Worlds and Large Worlds",
    "section": "Garden of forking paths.",
    "text": "Garden of forking paths.\nI was thinking of working out the probabilities by doing random sampling‚Ä¶\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)\nsource(here(\"_defaults.R\"))\n\nGenerating the marble dataframe\n\ntibble(\n  blue_marbs = 0:4,\n  white_marbs = 4 - blue_marbs\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    marbles = list(c(rep(\"blue\", blue_marbs), rep(\"white\", white_marbs)))\n  ) -&gt; \n  marbles\n\n\nmarbles |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      blue_marbs\n      white_marbs\n      marbles\n    \n  \n  \n    0\n4\nwhite, white, white, white\n    1\n3\nblue, white, white, white\n    2\n2\nblue, blue, white, white\n    3\n1\nblue, blue, blue, white\n    4\n0\nblue, blue, blue, blue\n  \n  \n  \n\nTable¬†1:  The marble sampling distributions \n\n\n\nIn retrospect, I‚Äôm glad I did this, because I thought we were sampling without replacement.\nHere‚Äôs a function that will repeatedly sample from a set of marbles, and compare the result to a reference group.\n\nsampling_df &lt;- function(marbles, n = 1000, size = 3, pattern = c(\"blue\", \"white\", \"blue\")){\n1  sampling_tibble &lt;- tibble(samp = 1:n)\n  sampling_tibble |&gt; \n    mutate(\n2      chosen = map(samp, ~sample(marbles, size = 3, replace = T)),\n3      match = map_lgl(chosen, ~all(.x == pattern))\n    ) |&gt; \n4    summarise(prop_match = mean(match))-&gt;\n    sampling_tibble\n  return(sampling_tibble)\n}\n\n\n1\n\nI‚Äôll capture everything within a tibble.\n\n2\n\nRowwise, sample from marbles with replacement.\n\n3\n\nReturn T or F if the sequence matches the pattern exactly.\n\n4\n\nThe mean() of the T, F column to get the proportion that match.\n\n\n\n\n\nsampling_df(\n  marbles = marbles$marbles[[4]],\n  n = 5000\n) \n\n# A tibble: 1 √ó 1\n  prop_match\n       &lt;dbl&gt;\n1      0.140\n\n\n\nmarbles |&gt; \n ungroup() |&gt; \n  mutate(\n    prob = map(marbles, ~sampling_df(.x, n = 10000))\n  ) |&gt; \n  unnest(prob) |&gt; \n  mutate(norm_probs = prop_match/sum(prop_match))-&gt;\n  marble_probs\n\n\nmarble_probs |&gt; \n  ggplot(aes(blue_marbs, norm_probs))+\n    geom_col(fill = \"steelblue4\")+\n    labs(\n      title = \"blue, white, blue\",\n      x = \"# of blue marbles\",\n      y = \"probability\"\n    ) + \n  ylim(0,1)-&gt;probs1\nprobs1\n\n\n\n\nFigure¬†1: Probability of each composition of marbles"
  },
  {
    "objectID": "posts/2023-05-09_02-small-large-worlds/index.html#updating-probabilities",
    "href": "posts/2023-05-09_02-small-large-worlds/index.html#updating-probabilities",
    "title": "02 Small Worlds and Large Worlds",
    "section": "Updating probabilities",
    "text": "Updating probabilities\nWhat if we draw one more blue\n\nmarble_probs |&gt; \n  mutate(new_obs_prob = blue_marbs / sum(blue_marbs),\n         posterior_prob = norm_probs * new_obs_prob,\n         posterior_norm = posterior_prob/sum(posterior_prob))-&gt;\n  marble_probs\n\n\nmarble_probs |&gt; \n  ggplot(aes(blue_marbs, posterior_norm))+\n    geom_col(fill = \"steelblue4\")+\n    ylim(0,1)+\n      labs(\n      title = \"probability update after blue\",\n      x = \"# of blue marbles\",\n      y = \"probability\"\n    ) -&gt;\n  probs2\n\nprobs1 | probs2\n\n\n\n\nFigure¬†2: Bayesian update"
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html",
    "href": "posts/2023-06-15_10-reporting2/index.html",
    "title": "Reporting linear model parameters",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html#loading",
    "href": "posts/2023-06-15_10-reporting2/index.html#loading",
    "title": "Reporting linear model parameters",
    "section": "Loading",
    "text": "Loading\n\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(gt)\nlibrary(gtsummary)\n\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(broom.mixed)\n\nsource(here::here(\"_defaults.R\"))\n\nTable libraries\n\nlibrary(gtsummary)\nlibrary(stargazer)\n\n\nheight_weight_mod &lt;- read_rds(here::here(\"posts\", \"2023-06-14_09-reporting\", \"height_weight_mod.rds\"))"
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html#the-goal",
    "href": "posts/2023-06-15_10-reporting2/index.html#the-goal",
    "title": "Reporting linear model parameters",
    "section": "The goal",
    "text": "The goal\nTry to get estimates from this brms model into a table, or some other kind of format, that I like:\n\nheight_weight_mod\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight0 \n   Data: height_to_mod (Number of observations: 251) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.59      0.32   153.93   155.22 1.00     4061     2653\nweight0       0.91      0.05     0.81     1.01 1.00     3991     2705\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.16      0.24     4.72     5.64 1.00     4328     3175\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html#off-the-shelf-table-methods",
    "href": "posts/2023-06-15_10-reporting2/index.html#off-the-shelf-table-methods",
    "title": "Reporting linear model parameters",
    "section": "Off-the-shelf table methods",
    "text": "Off-the-shelf table methods\n{gtsummary} has a tbl_regression() function.\n\nheight_weight_mod |&gt; \n  gtsummary::tbl_regression(\n    intercept = T\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n    \n  \n  \n    (Intercept)\n155\n154, 155\n    weight0\n0.91\n0.81, 1.0\n  \n  \n  \n    \n      1 CI = Credible Interval\n    \n  \n\n\n\n\nIt‚Äôs ok.\nI know {stargazer} is a package people use for reporting models.\n\ndata(penguins, package = \"palmerpenguins\")\n\npenguin_mod &lt;- lm(bill_length_mm ~ body_mass_g + species, penguins)\n\nstargazer::stargazer(penguin_mod, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nbill_length_mm\n\n\n\n\n\n\n\n\nbody_mass_g\n\n\n0.004***\n\n\n\n\n\n\n(0.0003)\n\n\n\n\n\n\n\n\n\n\nspeciesChinstrap\n\n\n9.921***\n\n\n\n\n\n\n(0.351)\n\n\n\n\n\n\n\n\n\n\nspeciesGentoo\n\n\n3.558***\n\n\n\n\n\n\n(0.486)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n24.919***\n\n\n\n\n\n\n(1.063)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n342\n\n\n\n\nR2\n\n\n0.808\n\n\n\n\nAdjusted R2\n\n\n0.806\n\n\n\n\nResidual Std. Error\n\n\n2.403 (df = 338)\n\n\n\n\nF Statistic\n\n\n474.006*** (df = 3; 338)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nTo be quite honest, I‚Äôm not a big fan of this giant table of everything & asterisks, and it looks like it‚Äôs not implemented for brms models anyway.\n\nstargazer::stargazer(height_weight_mod)\n\n\n% Error: Unrecognized object type.\n\n\nOh well."
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html#graphical-summary-methods",
    "href": "posts/2023-06-15_10-reporting2/index.html#graphical-summary-methods",
    "title": "Reporting linear model parameters",
    "section": "Graphical summary methods",
    "text": "Graphical summary methods\nThere‚Äôs a few challenges for visualizing this particular model given that the intercept is on such a different scale from the rest of the parameters.\n\nmcmc_plot(height_weight_mod)\n\n\n\n\n\nheight_weight_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    sigma,\n    regex = TRUE\n  ) |&gt; \n  ggplot(aes(.value))+\n    stat_slabinterval(\n      normalize = \"panels\",\n      point_interval = \"mean_hdi\"\n    )+\n    facet_wrap(~.variable, scales = \"free\", ncol = 1)+\n    theme_no_y()"
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html#custom-table",
    "href": "posts/2023-06-15_10-reporting2/index.html#custom-table",
    "title": "Reporting linear model parameters",
    "section": "Custom table",
    "text": "Custom table\nI want a custom {gt} table that captures the distributional nature of the posteriors. That‚Äôll involve calculating things like the credible intervals ‚Äúby-hand‚Äù (really, using tidybayes::mean_hdci() and doing some reshaping before the monster {gt} code.\n\nGetting parameter draws\nI‚Äôll grab the parameter draws with gather_draws()\n\nheight_weight_mod |&gt; \n  gather_draws(\n    `b_.*`,\n    sigma,\n    regex = T \n  ) -&gt; parameter_draws\n\n\n\nGetting the parameter summaries\nI‚Äôll get the posterior mean and 95% & 50% hdci right now, which results in a long table.\n\nparameter_draws |&gt; \n  group_by(.variable) |&gt; \n  mean_hdci(.value, .width = c(0.95, 0.5)) -&gt;\n  initial_table_long\n\ninitial_table_long\n\n# A tibble: 6 √ó 7\n  .variable    .value  .lower  .upper .width .point .interval\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_Intercept 155.    154.    155.      0.95 mean   hdci     \n2 b_weight0     0.911   0.812   1.00    0.95 mean   hdci     \n3 sigma         5.16    4.70    5.62    0.95 mean   hdci     \n4 b_Intercept 155.    154.    155.      0.5  mean   hdci     \n5 b_weight0     0.911   0.881   0.944   0.5  mean   hdci     \n6 sigma         5.16    4.98    5.29    0.5  mean   hdci     \n\n\n\n\nLong to wide\nWhat I have right now is a table like\n| .variable | .lower | .upper | .width |\n| intercept | ...... | ...... |   0.5  |\n| intercept | ...... | ...... |   0.95 | \nWhat for my final table will be something like\n| .variable | lower_95 | lower_50 | upper_50 | upper_95 |\n| intercept | ........ | ........ | ........ | ........ |\nSo, getting from here to there will involve some pivoting\n\ninitial_table_long |&gt; \n  pivot_longer(\n    .lower:.upper,\n    names_to = \"side\",\n    values_to = \"side_value\"\n  ) |&gt; \n  unite(\n    col = \"side\",\n    c(side, .width)\n  ) |&gt; \n  pivot_wider(\n    names_from = side,\n    values_from = side_value\n  ) |&gt; \n  relocate(\n    ends_with(\"_0.5\"),\n    .after = .lower_0.95\n  ) |&gt; \n  select(-.point, -.interval) -&gt;\n  initial_table_wide\n\ninitial_table_wide\n\n# A tibble: 3 √ó 6\n  .variable    .value .lower_0.95 .lower_0.5 .upper_0.5 .upper_0.95\n  &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 b_Intercept 155.        154.       154.       155.         155.  \n2 b_weight0     0.911       0.812      0.881      0.944        1.00\n3 sigma         5.16        4.70       4.98       5.29         5.62\n\n\n\n\nPosterior sd\nI also want to grab the posterior standard deviation, maybe just cause I‚Äôm old fashioned, I‚Äôll have to calculate that back with the original posterior draws and then join it onto the table I‚Äôm making.\n\nparameter_draws |&gt; \n  group_by(.variable) |&gt; \n  summarise(\n    posterior_sd = sd(.value)\n  ) -&gt;\n  posterior_sd\n\nposterior_sd\n\n# A tibble: 3 √ó 2\n  .variable   posterior_sd\n  &lt;chr&gt;              &lt;dbl&gt;\n1 b_Intercept       0.324 \n2 b_weight0         0.0488\n3 sigma             0.237 \n\n\n\ninitial_table_wide |&gt; \n  left_join(posterior_sd) |&gt; \n  relocate(\n    posterior_sd,\n    .after = .value\n  ) -&gt;\n  final_report_table\n\nfinal_report_table\n\n# A tibble: 3 √ó 7\n  .variable    .value posterior_sd .lower_0.95 .lower_0.5 .upper_0.5 .upper_0.95\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 b_Intercept 155.          0.324      154.       154.       155.         155.  \n2 b_weight0     0.911       0.0488       0.812      0.881      0.944        1.00\n3 sigma         5.16        0.237        4.70       4.98       5.29         5.62\n\n\n\n\nThe {gt} code\nOne last bit of clean up will be to the variable names, and creating a group column so I can separate the intercept and weight parameters from the variance parameter.\n\nfinal_report_table |&gt; \n  ungroup() |&gt; \n  mutate(\n    group = case_when(\n      str_detect(.variable, \"b_\") ~ \"Œ≤\",\n      .default = \"variance\"\n    ),\n    .variable = str_remove(.variable, \"b_\")\n  ) |&gt; \n  group_by(group) -&gt;\n  for_gt\n\nfor_gt |&gt; \n  gt()-&gt;\n  gt_interim_01\n\ngt_interim_01\n\n\n\n\n\n  \n    \n    \n      .variable\n      .value\n      posterior_sd\n      .lower_0.95\n      .lower_0.5\n      .upper_0.5\n      .upper_0.95\n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.5857634\n0.32423855\n153.9159103\n154.3976375\n154.8290184\n155.196791\n    weight0\n0.9112191\n0.04884185\n0.8124778\n0.8805526\n0.9442994\n1.004583\n    \n      variance\n    \n    sigma\n5.1583113\n0.23732630\n4.7040727\n4.9795864\n5.2942085\n5.619339\n  \n  \n  \n\n\n\n\nThat‚Äôs too many decimal places.\n\ngt_interim_01 |&gt; \n  fmt_number(decimals = 2) -&gt;\n  gt_interim_02\n\ngt_interim_02\n\n\n\n\n\n  \n    \n    \n      .variable\n      .value\n      posterior_sd\n      .lower_0.95\n      .lower_0.5\n      .upper_0.5\n      .upper_0.95\n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.59\n0.32\n153.92\n154.40\n154.83\n155.20\n    weight0\n0.91\n0.05\n0.81\n0.88\n0.94\n1.00\n    \n      variance\n    \n    sigma\n5.16\n0.24\n4.70\n4.98\n5.29\n5.62\n  \n  \n  \n\n\n\n\nNext, I‚Äôll add spanners for the credible intervals, and point estimates.\n\ngt_interim_02 |&gt; \n   tab_spanner(\n    label = \"50% HDI\",\n    columns = .lower_0.5:.upper_0.5\n  ) |&gt; \n  tab_spanner(\n    label = \"95% HDI\",\n    columns = .lower_0.95:.upper_0.95\n  ) |&gt; \n  tab_spanner(\n    label = \"posterior estimates\",\n    columns = c(.value, posterior_sd)\n  ) -&gt;\n  gt_interim_03\n\ngt_interim_03\n\n\n\n\n\n  \n    \n    \n      \n      \n        95% HDI\n      \n    \n    \n      .variable\n      \n        posterior estimates\n      \n      .lower_0.95\n      \n        50% HDI\n      \n      .upper_0.95\n    \n    \n      .value\n      posterior_sd\n      .lower_0.5\n      .upper_0.5\n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.59\n0.32\n153.92\n154.40\n154.83\n155.20\n    weight0\n0.91\n0.05\n0.81\n0.88\n0.94\n1.00\n    \n      variance\n    \n    sigma\n5.16\n0.24\n4.70\n4.98\n5.29\n5.62\n  \n  \n  \n\n\n\n\nWith the spanners in place, we can adjust the actual column labels.\n\ngt_interim_03 |&gt; \n   cols_label(\n    .variable = md(\"**parameter**\"),\n    .value = md(\"**mean**\"),\n    posterior_sd = md(\"**sd**\"),\n    .lower_0.95:.upper_0.95 ~ \"\"\n  )-&gt;\n  gt_interim_04\n\ngt_interim_04\n\n\n\n\n\n  \n    \n    \n      \n      \n        95% HDI\n      \n    \n    \n      parameter\n      \n        posterior estimates\n      \n      \n      \n        50% HDI\n      \n      \n    \n    \n      mean\n      sd\n      \n      \n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.59\n0.32\n153.92\n154.40\n154.83\n155.20\n    weight0\n0.91\n0.05\n0.81\n0.88\n0.94\n1.00\n    \n      variance\n    \n    sigma\n5.16\n0.24\n4.70\n4.98\n5.29\n5.62\n  \n  \n  \n\n\n\n\nNow, to add some visual clarity to the credible intervals. I‚Äôll fill with ptol_blue, (hex code #4477aa).\n\ngt_interim_04 |&gt; \n  tab_style(\n    style = cell_fill(\n      color = ptol_blue, \n      alpha = 0.5\n      ),\n    locations = cells_body(\n      columns = ends_with(\"_0.5\")\n    )\n   ) |&gt; \n  tab_style(\n    style = cell_fill(\n      color = ptol_blue, \n      alpha = 0.2\n      ),\n    locations = cells_body(\n      columns = ends_with(\"_0.95\")\n    )\n   )-&gt;\n  gt_interim_05\n\ngt_interim_05\n\n\n\n\n\n  \n    \n    \n      \n      \n        95% HDI\n      \n    \n    \n      parameter\n      \n        posterior estimates\n      \n      \n      \n        50% HDI\n      \n      \n    \n    \n      mean\n      sd\n      \n      \n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.59\n0.32\n153.92\n154.40\n154.83\n155.20\n    weight0\n0.91\n0.05\n0.81\n0.88\n0.94\n1.00\n    \n      variance\n    \n    sigma\n5.16\n0.24\n4.70\n4.98\n5.29\n5.62\n  \n  \n  \n\n\n\n\nFor me, the group labels are too visually similar to the names of parameter values, so I‚Äôll style those too.\n\ngt_interim_05 |&gt; \n  tab_style(\n    style = cell_text(\n      style = \"italic\", \n      size = \"small\"\n    ),\n    locations = cells_row_groups()\n  ) -&gt;\n  gt_interim_06\n\ngt_interim_06\n\n\n\n\n\n  \n    \n    \n      \n      \n        95% HDI\n      \n    \n    \n      parameter\n      \n        posterior estimates\n      \n      \n      \n        50% HDI\n      \n      \n    \n    \n      mean\n      sd\n      \n      \n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.59\n0.32\n153.92\n154.40\n154.83\n155.20\n    weight0\n0.91\n0.05\n0.81\n0.88\n0.94\n1.00\n    \n      variance\n    \n    sigma\n5.16\n0.24\n4.70\n4.98\n5.29\n5.62\n  \n  \n  \n\n\n\n\nFinal steps, just adding some header and footer information\n\ngt_interim_06 |&gt; \n  tab_header(\n    title = \"height ~ weight0\"\n  ) |&gt; \n  tab_source_note(\n    source_note = md(\"**priors:**&lt;br&gt;intercept &tilde; N(178, 20)&lt;br&gt;Œ≤ &tilde; N(0,10)&lt;br&gt;œÉ &tilde; U(0,50)\")\n  )-&gt;\n  final_gt_table\n\nfinal_gt_table\n\n\n\n\n\n  \n    \n      height ~ weight0\n    \n    \n    \n      \n      \n        95% HDI\n      \n    \n    \n      parameter\n      \n        posterior estimates\n      \n      \n      \n        50% HDI\n      \n      \n    \n    \n      mean\n      sd\n      \n      \n    \n  \n  \n    \n      Œ≤\n    \n    Intercept\n154.59\n0.32\n153.92\n154.40\n154.83\n155.20\n    weight0\n0.91\n0.05\n0.81\n0.88\n0.94\n1.00\n    \n      variance\n    \n    sigma\n5.16\n0.24\n4.70\n4.98\n5.29\n5.62\n  \n  \n    \n      priors:intercept Àú N(178, 20)Œ≤ Àú N(0,10)œÉ Àú U(0,50)\n    \n  \n  \n\n\n\n\nI‚Äôll just save this table as a png to be the post image.\n\ngtsave(final_gt_table, filename = \"final_gt_table.png\")"
  },
  {
    "objectID": "posts/2023-06-15_10-reporting2/index.html#was-it-worth-it",
    "href": "posts/2023-06-15_10-reporting2/index.html#was-it-worth-it",
    "title": "Reporting linear model parameters",
    "section": "Was it worth it?",
    "text": "Was it worth it?\nThat was a lot of fuss, but a lot of those steps I might roll into my own functions for an actual paper."
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html",
    "href": "posts/2023-06-02_05-sampling2/index.html",
    "title": "Sampling Summaries",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#loading",
    "href": "posts/2023-06-02_05-sampling2/index.html#loading",
    "title": "Sampling Summaries",
    "section": "Loading",
    "text": "Loading\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(here)\n\nsource(here(\"_defaults.R\"))"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#setting-up-the-grid-samples",
    "href": "posts/2023-06-02_05-sampling2/index.html#setting-up-the-grid-samples",
    "title": "Sampling Summaries",
    "section": "Setting up the grid samples",
    "text": "Setting up the grid samples\n\ntibble(\n  p = seq(0, 1, length = 1000),\n  dens = dbeta(p, 6+1, 3+1),\n  posterior = dens/sum(dens)\n) -&gt;\n  posterior_grid\n\n\nposterior_grid |&gt; \n  ggplot(aes(p, posterior))+\n    geom_area(fill = ptol_blue, color = \"black\")\n\n\n\n\n\nSampling from the posterior\n\nposterior_grid |&gt; \n  sample_n(\n    size = 1e4,\n    replace = T,\n    weight = posterior\n  )-&gt;\n  posterior_samples\n\nThis isn‚Äôt MCMC sampling, but I‚Äôll plot it as a line just for consistency for how MCMC chains look.\n\nposterior_samples |&gt; \n  mutate(\n    sample = row_number()\n  ) |&gt; \n  ggplot(aes(sample, p))+\n    geom_line()\n\n\n\n\nComparing to the sampling to the original density function.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_density(\n      fill = ptol_blue\n      ) +\n    geom_line(\n      data = posterior_grid,\n      aes(y = dens),\n      color = ptol_red,\n      linewidth = 1\n    )"
  },
  {
    "objectID": "posts/2023-06-02_05-sampling2/index.html#quantiles",
    "href": "posts/2023-06-02_05-sampling2/index.html#quantiles",
    "title": "Sampling Summaries",
    "section": "Quantiles",
    "text": "Quantiles\nFirst manually\n\nposterior_samples |&gt; \n  reframe(\n    lowhi = quantile(p, probs = c(0.25, 0.75))\n  ) |&gt; \n  pull(lowhi)-&gt;\n  fifty_quantile\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    ggdist::stat_slab(\n      color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n    labs(\n      fill = \"fifty\",\n      y = NULL\n    )+\n    scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n    theme(\n      axis.text.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n\n\n\n\nI think I‚Äôll create a shortcut theme for having no y axis.\n\ntheme_no_y &lt;- function(){\n  theme(\n      axis.text.y = element_blank(),\n      axis.title.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n}\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_slab() +\n    theme_no_y()\n\n\n\n\n\nTidybayes functions\nI‚Äôm not 100% sure how all of the tidybayes functions work.\n\nposterior_samples |&gt; \n  summarise(\n    median_qi(p, .width = 0.5)\n  )\n\n# A tibble: 1 √ó 6\n      y  ymin  ymax .width .point .interval\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.644 0.542 0.738    0.5 median qi       \n\n\n\nposterior_samples |&gt; \n  reframe(\n    quantile  = quantile(p, probs = c(0.25, 0.5, 0.75))\n  )\n\n# A tibble: 3 √ó 1\n  quantile\n     &lt;dbl&gt;\n1    0.542\n2    0.644\n3    0.738\n\n\nOk, *_qi() returns the quantile interval.\nI‚Äôd like to make the plot according to the statistics calculated by stat_halfeye(), but can‚Äôt seem to get it to work.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      aes(\n        fill = after_stat(x &gt; xmin)\n      )\n    )\n\n\n\n\nI‚Äôll just do the same filling I did before.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_qi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()-&gt;\n  fifty_qi\nfifty_qi\n\n\n\n\n\n\nHPDI\nLemme try hpdi now.\n\nposterior_samples |&gt; \n  summarise(\n    mean_hdi(p, .width = 0.5)\n  )-&gt;\n  posterior_hdi\n\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_hdi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n          x &gt;= posterior_hdi$ymin & \n            x &lt;=  posterior_hdi$ymax\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()-&gt;\n  fifty_hdi\nfifty_hdi\n\n\n\n\n\nfifty_qi/fifty_hdi\n\n\n\n\nThey‚Äôre very similar, but if I mix the qi fill and the hdi interval, they‚Äôre different.\n\nposterior_samples |&gt; \n  ggplot(aes(p))+\n    stat_halfeye(\n      .width = 0.5,\n      point_interval = median_hdi,\n      slab_color = \"black\",\n      aes(\n        fill = after_stat(\n         x &gt;= fifty_quantile[1] & x &lt;= fifty_quantile[2]\n        )\n      )\n    )+\n  scale_fill_manual(\n      values = c(\"grey90\", ptol_blue)\n    )+\n  labs(fill = \"fifty\")+\n  theme_no_y()"
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html",
    "href": "posts/2023-07-05_11-dags1/index.html",
    "title": "DAGs part 1",
    "section": "",
    "text": "Listening\nSo, as a linguist, the only Directed Acyclic Graphs I‚Äôve ever worked with are syntax trees. I don‚Äôt know if it‚Äôs embarrassing that I‚Äôve never really utilized them in my statistical analysis, but I‚Äôll start to now!"
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html#setup",
    "href": "posts/2023-07-05_11-dags1/index.html#setup",
    "title": "DAGs part 1",
    "section": "Setup",
    "text": "Setup\n\n\nsetup\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(patchwork)\nlibrary(ggblend)\n\nlibrary(brms)\nlibrary(marginaleffects)\n\nsource(here::here(\"_defaults.R\"))\nknitr::opts_chunk$set(dev = \"png\", dev.args = list(type = \"cairo-png\"))\n\n\nNew packages for today.\n\nlibrary(dagitty)\nlibrary(ggdag)\ndata(WaffleDivorce, package = \"rethinking\")"
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html#the-waffledivorce-data",
    "href": "posts/2023-07-05_11-dags1/index.html#the-waffledivorce-data",
    "title": "DAGs part 1",
    "section": "The WaffleDivorce data",
    "text": "The WaffleDivorce data\nI‚Äôll leave the Location and Loc columns out of the overall summary.\n\nWaffleDivorce |&gt; \n  select(\n    -Location, -Loc\n  ) |&gt; \n  gtsummary::tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 501\n    \n  \n  \n    Population\n4.4 (1.6, 6.7)\n    MedianAgeMarriage\n25.90 (25.33, 26.75)\n    Marriage\n19.7 (17.1, 22.1)\n    Marriage.SE\n1.19 (0.81, 1.77)\n    Divorce\n9.75 (8.30, 10.90)\n    Divorce.SE\n0.80 (0.57, 1.26)\n    WaffleHouses\n1 (0, 40)\n    South\n14 (28%)\n    Slaves1860\n0 (0, 80,828)\n    Population1860\n407,722 (43,321, 920,977)\n    PropSlaves1860\n0.00 (0.00, 0.09)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nI‚Äôm not sure why everyone likes assigning the full named data frame to a new variable called d. It‚Äôs annoying to type out W a f f l e D i v o r c e , but aren‚Äôt we all using IDEs with tab completion?\nLet‚Äôs look at the variables discussed in the chapter.\n\n\nplotting code\nWaffleDivorce |&gt; \n  ggplot(aes(WaffleHouses, Divorce))+\n    geom_point() +\n    stat_smooth(\n      method = lm,\n      color = ptol_blue\n      )+\n    theme(aspect.ratio = 1)-&gt;\n  waffledivorce_p\n\nWaffleDivorce |&gt; \n  ggplot(aes(Marriage, Divorce))+\n    geom_point() +\n    stat_smooth(\n      method = lm,\n      color = ptol_blue\n      )+\n    theme(aspect.ratio = 1) -&gt;\n  marriagedivorce_p\n\nWaffleDivorce |&gt; \n  ggplot(aes(MedianAgeMarriage, Divorce))+\n    geom_point() +\n    stat_smooth(\n      method = lm,\n      color = ptol_blue\n      )+\n    theme(aspect.ratio = 1) -&gt;\n  agedivorce_p\n\nwaffledivorce_p + marriagedivorce_p + agedivorce_p\n\n\n\n\n\nFigure¬†1: The relationship between three variables and divorce rate."
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html#the-dag",
    "href": "posts/2023-07-05_11-dags1/index.html#the-dag",
    "title": "DAGs part 1",
    "section": "The DAG",
    "text": "The DAG\nThe book mostly focuses on the effect of median age at marriage, the marriage rate, and divorce rate, which you can represent as DAG like so:\n\ndagify(\n  divorce ~ age,\n  divorce ~ marriage,\n  marriage ~ age\n) |&gt; \n  ggdag(\n    text_col = ptol_red\n  )+\n    theme_void()+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\nFigure¬†2: DAG attempt 1\n\n\n\n\nNot quite happy with this first attempt. Looks like I‚Äôll really have to use these single character labels, which I‚Äôm not the biggest fan of, to make them fit inside the nodes. Looks like I might also need to do more by-hand adjustment of both the coordinates of each node, and also the aesthetics of the plot.\n\ndagify(\n  D ~ A,\n  D ~ M,\n  M ~ A,\n  outcome = \"D\",\n  exposure = \"A\",\n  coords = \n    tribble(\n      ~name, ~x, ~y,\n      \"D\", 0, 0,\n      \"A\", -1, 0,\n      ## UGH\n      \"M\", -0.5, -sqrt(1-(0.5^2))\n    )\n) -&gt;\n  dam_dag\n\ndam_dag |&gt; \n  tidy_dagitty() |&gt; \n  ggplot(aes(x =x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(\n     color = \"grey\"\n    )+\n    geom_dag_text(\n      color = ptol_blue\n    )+\n    geom_dag_edges()+\n    theme_dag()+\n    coord_fixed()\n\n\n\n\nFigure¬†3: DAG attempt 2\n\n\n\n\nWell, I‚Äôm a little annoyed at how manual getting the layout to be exactly like I wanted was, but OK.\n\nAdding in Waffle Houses\nLet‚Äôs figure out how to get the number of Waffle Houses into the DAG. I‚Äôll say there‚Äôs a latent variable R for Region\n\ndagify(\n  D ~ A,\n  D ~ M,\n  M ~ A,\n  W ~ R,\n  A ~ R,\n  M ~ R,\n  outcome = \"D\",\n  exposure = c(\"M\", \"A\"),\n  latent = \"R\",\n  coords = \n    tribble(\n      ~name, ~x, ~y,\n      \"D\", 0, 0,\n      \"A\", -1, 0,\n      ## UGH\n      \"M\", -0.5, -sqrt(1-(0.5^2)),\n      \"R\", -1.5, -sqrt(1-(0.5^2)),\n      \"W\", -2, 0\n    )\n) -&gt;\n  wrdam_dag\n\nwrdam_dag |&gt; \n tidy_dagitty() |&gt; \n  ggplot(aes(x =x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(\n     aes(\n       color = name == \"R\"\n     )\n    )+\n    geom_dag_text(\n      #color = ptol_blue\n    )+\n    geom_dag_edges() +\n    coord_fixed() +\n    theme_dag()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\nOk, well, we‚Äôll see how intense I ever get about making these DAG figures."
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html#doing-the-full-luxury-bayes",
    "href": "posts/2023-07-05_11-dags1/index.html#doing-the-full-luxury-bayes",
    "title": "DAGs part 1",
    "section": "Doing the Full Luxury Bayes",
    "text": "Doing the Full Luxury Bayes\nFirst, prepping for modelling by standardizing all of the variables.\n\nWaffleDivorce |&gt; \n  mutate(\n    divorce_z = (Divorce - mean(Divorce))/sd(Divorce),\n    age_z = (MedianAgeMarriage-mean(MedianAgeMarriage))/sd(MedianAgeMarriage),\n    marriage_z = (Marriage - mean(Marriage))/sd(Marriage)\n  )-&gt;\n  waffle_to_model\n\nTo figure out the model we need to get the ‚Äúdirect effect‚Äù of marriage rate on divorce rate, we can use dagitty::adjustmentSets().\n\ndam_dag |&gt; \n  adjustmentSets(\n    outcome = \"D\",\n    exposure = \"M\"\n  )\n\n{ A }\n\n\nSo, we need to include median marriage age in the model.\nFor the ‚Äúfull luxury Bayes‚Äù approach, I‚Äôll combine brms formulas to model both the divorce rate and the marriage rate in one go.\n\nwaffle_formula &lt;-   bf(\n    divorce_z ~ age_z + marriage_z\n  )+\n  bf(\n    marriage_z ~ age_z\n  )+\n  # not 100% sure this is right\n  set_rescor(F)\n\nLet‚Äôs look at the default priors. I‚Äôm, trying out some more stuff with {gt} here to get a table I like, but it takes up a lot of space so I‚Äôm collapsing it. I also need to figure out what kind of behavior makes sense to me for table captions created by quarto and table titles created by {gt}.\n\n\ntable code\nget_prior(\n  waffle_formula,\n  data = waffle_to_model\n) |&gt; \n  as_tibble() |&gt; \n  select(\n    prior,\n    class,\n    coef,\n    resp\n  ) |&gt; \n  group_by(class) |&gt; \n  filter(\n    str_length(resp) &gt; 0\n  ) |&gt; \n  filter(\n    !(class == \"b\" & coef == \"\")\n  ) |&gt; \n  gt(\n    rowname_col = \"prior\"\n  ) |&gt; \n    sub_values(\n      columns = prior,\n      values = \"\",\n      replacement = \"flat\"\n    ) |&gt; \n    tab_stub_indent(\n      rows = everything(),\n      indent = 2\n    ) |&gt; \n  tab_header(\n    title = md(\"Default `brms` priors\")\n  )\n\n\n\n\n\n\n\n  \n    \n      Default brms priors\n    \n    \n    \n      \n      coef\n      resp\n    \n  \n  \n    \n      b\n    \n    flat\nage_z\ndivorcez\n    flat\nmarriage_z\ndivorcez\n    flat\nage_z\nmarriagez\n    \n      Intercept\n    \n    student_t(3, 0, 2.5)\n\ndivorcez\n    student_t(3, -0.1, 2.5)\n\nmarriagez\n    \n      sigma\n    \n    student_t(3, 0, 2.5)\n\ndivorcez\n    student_t(3, 0, 2.5)\n\nmarriagez\n  \n  \n  \n\nTable¬†1:  Default priors \n\n\n\nSo, a thing that hadn‚Äôt really clicked with me until I was teaching from Bodo Winter‚Äôs textbook is that if you z-score both the outcome and the predictors in a model, the resulting slopes are Pearson‚Äôs r, which is always going to be \\(-1 \\le \\rho \\le 1\\). Not that we really have to stress it with this particular data and model, efficiencywise, but we can set a prior on these slopes with a relatively narrow scale, and it‚Äôll be pretty reasonable. Here‚Äôs a normal(0, 0.5) and a student_t(3, 0, 0.5) for comparison.\n\n\nplotting code\ntibble(\n  x = seq(-1.5, 1.5, length = 500),\n  dens = dnorm(x, sd = 0.5),\n  prior = \"normal(0, 0.5)\"\n) |&gt; \n  bind_rows(\n    tibble(\n      x = seq(-1.5, 1.5, length = 500),\n      dens = dstudent_t(x, df = 3, sigma = 0.5),\n      prior = \"student_t(3, 0, 0.5)\"\n    )\n  ) |&gt; \n  ggplot(\n    aes(x = x, y = dens)\n  )+\n  list(\n    geom_area(\n      aes(fill = prior),\n      position = \"identity\",\n      #alpha = 0.6,\n      color = \"black\"\n    ) |&gt;  blend(\"multiply\"),\n    geom_vline(\n      xintercept = c(-1, 1),\n      linewidth = 1,\n      color = \"grey40\"\n    )) |&gt; \n    blend(\"screen\")+ \n    khroma::scale_fill_bright(\n      limits = c( \n        \"student_t(3, 0, 0.5)\",\n         \"normal(0, 0.5)\"\n      )\n    )+\n    labs(\n      x = NULL\n    ) +\n    scale_y_continuous(\n      expand = expansion(mult = 0.01)\n    ) +\n    theme_no_y()\n\n\n\n\n\nFigure¬†4: Comparison of a normal and t distribution\n\n\n\n\nI‚Äôll use the slightly broader t distribution for the slope priors.\n\nslope_priors &lt;- prior(\n  student_t(3, 0, 0.5),\n  class = b\n)\n\nNow for fitting the whole thing.\n\nbrm(\n  formula = waffle_formula,\n  prior = slope_priors,\n  data = waffle_to_model,\n  backend = \"cmdstanr\",\n  file = \"dam.rds\",\n  cores = 4\n)-&gt;\n  full_model\n\n\nfull_model\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: divorce_z ~ age_z + marriage_z \n         marriage_z ~ age_z \n   Data: waffle_to_model (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndivorcez_Intercept      0.00      0.12    -0.23     0.24 1.00     6550     3104\nmarriagez_Intercept    -0.00      0.10    -0.20     0.20 1.00     6593     2946\ndivorcez_age_z         -0.61      0.17    -0.94    -0.28 1.00     3753     3193\ndivorcez_marriage_z    -0.06      0.16    -0.38     0.24 1.00     3876     3235\nmarriagez_age_z        -0.70      0.10    -0.90    -0.48 1.00     5942     2618\n\nFamily Specific Parameters: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_divorcez      0.84      0.09     0.69     1.03 1.00     5746     2990\nsigma_marriagez     0.72      0.08     0.59     0.89 1.00     6880     2852\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html#marginalizing",
    "href": "posts/2023-07-05_11-dags1/index.html#marginalizing",
    "title": "DAGs part 1",
    "section": "Marginalizing",
    "text": "Marginalizing\nSo, to ‚Äúmarginalize‚Äù over age, to get the direct effect of the marriage rate, I‚Äôd like to use the marginaleffects::slopes() function, but I think we‚Äôve got a slight issue.\n\nslopes(\n  full_model\n) |&gt; \n  as_tibble() |&gt; \n  filter(group == \"divorcez\") |&gt; \n  count(term)\n\n# A tibble: 1 √ó 2\n  term      n\n  &lt;chr&gt; &lt;int&gt;\n1 age_z    50\n\n\nBecause marriage_z is also an outcome variable, it doesn‚Äôt want to give me its marginal slopes in the divorce_z outcome model. So much for full luxury bayes! But I can work around with predictions. I think what I want to use is grid_typ=\"counterfactual\" in datagrid().\n\ndatagrid(\n  model = full_model,\n  marriage_z = c(0,1),\n  grid_type = \"counterfactual\"\n) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\npredictions(\n  full_model,\n  newdata = datagrid(\n    marriage_z = c(0,1),\n    grid_type = \"counterfactual\"\n  )\n) |&gt; \n  posterior_draws() |&gt; \n  filter(group == \"divorcez\") -&gt;\n  divorce_pred\n\nnrow(divorce_pred)\n\n[1] 400000\n\n\nOk, this gives us 40,000 values, which is 20,000 for marriage_z == 0 and 20,000 for marriage_z == 1. And given that the original data had 50 rows, that‚Äôs back to the 4,000 posterior samples we got from the model.\n\nhead(divorce_pred) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe draw column has the posterior draw, so what I want to do is pivot wider so there‚Äôs a column for marriage_z==0 and marriage_z==1, then subtract one from the other. I had some issues figuring out which columns need to get dropped for that to happen cleanly, but the answer is rowid and, I think, everything from estimate through conf.high\n\ndivorce_pred |&gt; \n  select(\n    -rowid,\n    -(estimate:conf.high)\n    ) |&gt; \n  pivot_wider(\n    names_from = marriage_z,\n    values_from = draw\n  ) |&gt; \n  mutate(marriage_effect = `1`-`0`) |&gt; \n  group_by(drawid) |&gt; \n  summarise(\n    avg_marriage_effect = mean(marriage_effect)\n  ) -&gt;\n  avg_marriage_effect\n\nAs it turns out, every estimate of marriage_effect was the same within each draw, but this might not‚Äôve been the case for a model with interactions, say.\n\n\nplotting code\navg_marriage_effect |&gt; \n  ggplot(aes(avg_marriage_effect)) +\n  list(\n    stat_halfeye(\n      point_interval = \"mean_hdci\",\n      fill = ptol_blue,\n      slab_color = \"black\"\n    ),\n    geom_vline(\n      xintercept = 0,\n      color = \"grey40\",\n      linewidth = 1\n    )) |&gt; blend(\"screen\")+\n    scale_y_continuous(\n      expand = expansion(mult = 0.02)\n    )+\n    labs(x = \"marriage direct effect\")+\n    theme_no_y()\n\n\n\n\n\nFigure¬†5: Marriage rate direct effect on divorce rate\n\n\n\n\nI have a sneaking suspicion that for this case, this is identical to the estimate of the slope.\n\n\nplotting code\nfull_model |&gt; \n  spread_draws(\n    b_divorcez_marriage_z\n  ) |&gt; \n  ggplot(aes(b_divorcez_marriage_z)) +\n  list(\n    stat_halfeye(\n      point_interval = \"mean_hdci\",\n      fill = ptol_blue,\n      slab_color = \"black\"\n    ),\n    geom_vline(\n      xintercept = 0,\n      color = \"grey40\",\n      linewidth = 1\n    )) |&gt; blend(\"screen\")+\n    scale_y_continuous(\n      expand = expansion(mult = 0.02)\n    )+\n    theme_no_y()\n\n\n\n\n\nFigure¬†6: Posterior slope of marriage rate on divorce rate\n\n\n\n\nLol, well."
  },
  {
    "objectID": "posts/2023-07-05_11-dags1/index.html#one-big-plot",
    "href": "posts/2023-07-05_11-dags1/index.html#one-big-plot",
    "title": "DAGs part 1",
    "section": "One big plot",
    "text": "One big plot\nLet‚Äôs make one big plot of all the estimated effects. Not all of the parameters from the model are ones we‚Äôll want\n\nfull_model |&gt; \n  get_variables()\n\n [1] \"b_divorcez_Intercept\"  \"b_marriagez_Intercept\" \"b_divorcez_age_z\"     \n [4] \"b_divorcez_marriage_z\" \"b_marriagez_age_z\"     \"sigma_divorcez\"       \n [7] \"sigma_marriagez\"       \"lprior\"                \"lp__\"                 \n[10] \"accept_stat__\"         \"treedepth__\"           \"stepsize__\"           \n[13] \"divergent__\"           \"n_leapfrog__\"          \"energy__\"             \n\n\nI‚Äôll grab all the betas and the sigmas.\n\nfull_model |&gt; \n  gather_draws(\n    `b_.*`,\n    `sigma_.*`,\n    regex = T\n  )-&gt;\n  all_param_draws\n\nI‚Äôll want to facet the plots by whether we‚Äôre looking at draws for the marriage rate outcome or for the divorce rate outcome, so I‚Äôll create some new columns.\n\nall_param_draws |&gt; \n  mutate(\n    outcome = case_when(\n      str_detect(.variable, \"marriagez\")~\"marriage rate~\",\n      str_detect(.variable, \"divorcez\")~\"divorce rate~\"\n    ),\n    class = case_when(\n      str_detect(.variable, \"b_\") ~ \"betas\",\n      str_detect(.variable, \"sigma\") ~ \"sigmas\"\n    )\n  ) -&gt; \n  all_param_draws\n\nAnd now I‚Äôll want a new cleaned up variable name for plotting.\n\nall_param_draws |&gt; \n  mutate(\n    param = .variable |&gt; \n      str_remove(\"b\") |&gt; \n      str_remove(\"_divorcez\") |&gt; \n      str_remove(\"_marriagez\") |&gt; \n      str_remove(\"^_\")\n  )-&gt;\n  all_param_draws\n\n\n\nplotting code\nall_param_draws |&gt; \n  mutate(\n    param = factor(\n      param,\n      levels = rev(c(\n        \"Intercept\", \n        \"age_z\", \n        \"marriage_z\", \n        \"sigma\"\n        ))\n    )\n  ) |&gt; \n  ggplot(aes(.value, param))+\n    stat_halfeye(\n      aes(\n        fill = after_stat(x &lt; 0)\n      ),\n      point_interval = \"mean_hdci\"\n    )+\n    scale_x_continuous(\n      breaks = c(-1, 0, 1)\n    )+\n    labs(y = NULL,\n         x = NULL)+\n    facet_grid(\n      class ~ outcome, \n      space = \"free\",\n      scales = \"free\"\n    )+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\nFigure¬†7: Posterior estimates of model parameters\n\n\n\n\nOne thing that‚Äôs maybe less than ideal is that the sigma parameters really aren‚Äôt on the same kind of scale here. Maybe they should be in a completely different plot, and put together with patchwork?\n\n\nplotting code\nall_param_draws |&gt; \n  mutate(\n    param = factor(\n      param,\n      levels = rev(c(\n        \"Intercept\", \n        \"age_z\", \n        \"marriage_z\", \n        \"sigma\"\n        ))\n    )\n  ) -&gt;\n  param_to_plot\n\nparam_to_plot |&gt; \n  filter(class == \"betas\") |&gt; \n  ggplot(aes(.value, param))+\n  list(\n    stat_halfeye(\n      aes(\n        fill = after_stat(x &lt; 0)\n      ),\n      point_interval = \"mean_hdci\"\n    ),\n    geom_vline(\n      xintercept = 0,\n      color = \"grey40\",\n      linewidth = 1\n    )\n  ) |&gt; \n  blend(\"screen\")+\n    #scale_x_continuous(\n    #  breaks = c(-1, 0, 1)\n    #)+\n    labs(y = NULL,\n         x = NULL)+\n    facet_grid(\n      class ~ outcome\n      #space = \"free\",\n      #scales = \"free\"\n    )+\n    theme(\n      legend.position = \"none\"\n    )-&gt;\n  betas\n\nparam_to_plot |&gt; \n  filter(class == \"sigmas\") |&gt; \n  ggplot(aes(.value, param))+\n    stat_halfeye(\n      aes(\n        fill = after_stat(x &lt; 0)\n      ),\n      point_interval = \"mean_hdci\"\n    )+\n    geom_vline(\n      xintercept = 0,\n      color = \"black\",\n      linewidth = 1\n    )+\n    #scale_x_continuous(\n    #  breaks = c(-1, 0, 1)\n    #)+\n    labs(y = NULL,\n         x = NULL)+\n    facet_grid(\n      class ~ outcome, \n      #space = \"free\",\n      #scales = \"free\"\n    )+\n    expand_limits(x = 0)+\n    theme(\n      legend.position = \"none\",\n      strip.text.x = element_blank()\n    ) -&gt; \n  sigmas\n\n\nlayout &lt;- \"\nA\nA\nA\nB\n\"\n\nbetas + sigmas + plot_layout(design = layout)\n\n\n\n\n\nFigure¬†8: Posterior estimates of model parameters\n\n\n\n\nHm, idk."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Workthrough Blog",
    "section": "",
    "text": "Where I do a lot of work and don‚Äôt understand colliders any better.\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nColliders\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nMulticollinearity & Post-treatment bias\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nHaunted DAGSs\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDAGS part 3\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDAGs part 2\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDAGs part 1\n\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nReporting linear model parameters\n\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nReporting a linear model\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear models, part 2\n\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nPredictive Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models: Part 1\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nSampling Summaries\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting Sampling\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nGarden of Forking paths part 2\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\n01 Golem of Prague Chapter\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nSetup\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\n02 Small Worlds and Large Worlds\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog to document my progress in working through Richard McElreath‚Äôs Statistical Rethinking: A Bayesian Course with Examples in R and Stan, which I‚Äôll be supplementing with Solomon Kurz‚Äô bookdown project, Statistical Rethinking with brms, ggplot2, and the tidyverse."
  }
]